{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Value-to-Value Check (Hash Comparison)\n",
    "\n",
    "This notebook:\n",
    "- Loads configuration with column mappings\n",
    "- Generates normalized hash of comparable columns\n",
    "- Includes unique ID columns for backtracking\n",
    "- Allows filtering by partition and columns for performance\n",
    "- **PCDS**: Generates SAS code to compute row hashes\n",
    "- **AWS**: Generates Athena SQL to compute row hashes\n",
    "- Compares hashes to find mismatched rows\n",
    "\n",
    "**SQL Format**: `SELECT MD5(normalized_col1 || '|' || normalized_col2 || ...) AS _hash, unique_id1, unique_id2, ... FROM table`\n",
    "\n",
    "**Input**: `data/config.json`  \n",
    "**Output**: SAS file, Athena SQL, mismatch report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Import normalization utilities\n",
    "from utils import (\n",
    "    normalize_oracle_column,\n",
    "    normalize_athena_column,\n",
    "    build_oracle_hash_expr,\n",
    "    build_athena_hash_expr\n",
    ")\n",
    "\n",
    "print(\"Step 5: Value-to-Value Check (Hash Comparison)\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_JSON = \"data/config.json\"\n",
    "\n",
    "with open(CONFIG_JSON, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(f\"Loaded configuration: {config['run_name']}\")\n",
    "print(f\"Total tables: {config['metadata']['total_tables']}\")\n",
    "print(f\"Step 4 completed: {config['status']['step4_completed']}\")\n",
    "\n",
    "# Output paths\n",
    "SAS_HASH_FILE = f\"output/pcds_hash_{config['run_name']}.sas\"\n",
    "ATHENA_HASH_FILE = f\"output/aws_hash_{config['run_name']}.sql\"\n",
    "\n",
    "Path(\"output\").mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Configuration - Select Tables, Columns, Partitions\n",
    "\n",
    "Optionally filter to specific tables/columns/partitions for performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter options (None = all)\n",
    "FILTER_TABLE_INDICES = None  # e.g., [0, 1, 2] for first 3 tables\n",
    "FILTER_VINTAGES = None       # e.g., ['2024-01-01', '2024-02-01']\n",
    "FILTER_COLUMNS = None        # e.g., ['CUSTOMER_ID', 'AMOUNT'] - only these comparable columns\n",
    "MAX_ROWS_PER_VINTAGE = None  # e.g., 100000 for testing\n",
    "\n",
    "print(\"Hash check configuration:\")\n",
    "print(f\"  Table filter: {FILTER_TABLE_INDICES or 'All tables'}\")\n",
    "print(f\"  Vintage filter: {FILTER_VINTAGES or 'All vintages'}\")\n",
    "print(f\"  Column filter: {FILTER_COLUMNS or 'All comparable columns'}\")\n",
    "print(f\"  Max rows per vintage: {MAX_ROWS_PER_VINTAGE or 'Unlimited'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Generate SAS Code for PCDS Row Hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sas_hash_code(table_pair, column_mapping, vintage='whole', filter_cols=None, max_rows=None):\n",
    "    \"\"\"\n",
    "    Generate SAS code to compute row hashes\n",
    "    \n",
    "    Args:\n",
    "        table_pair: Dict with table info\n",
    "        column_mapping: Dict with column mappings\n",
    "        vintage: Date partition or 'whole'\n",
    "        filter_cols: List of PCDS columns to include (None = all)\n",
    "        max_rows: Maximum rows to process (None = all)\n",
    "    \n",
    "    Returns:\n",
    "        str: SAS code\n",
    "    \"\"\"\n",
    "    pcds_tbl = table_pair['pcds_tbl']\n",
    "    \n",
    "    # Get comparable columns\n",
    "    comparable_cols = [\n",
    "        c for c in column_mapping['comparable_columns']\n",
    "        if not filter_cols or c['pcds_col'] in filter_cols\n",
    "    ]\n",
    "    \n",
    "    # Get unique ID columns\n",
    "    unique_id_cols = []\n",
    "    if table_pair.get('unique_id_cols'):\n",
    "        unique_id_cols = [c.strip() for c in table_pair['unique_id_cols'].split(',')]\n",
    "    \n",
    "    if not unique_id_cols:\n",
    "        return f\"/* Skipping {pcds_tbl} - No unique ID columns specified */\\n\"\n",
    "    \n",
    "    # Build WHERE clause\n",
    "    where_parts = []\n",
    "    if vintage != 'whole' and table_pair.get('pcds_dt'):\n",
    "        where_parts.append(f\"{table_pair['pcds_dt']} = '{vintage}'\")\n",
    "    if table_pair.get('pcds_where'):\n",
    "        where_parts.append(table_pair['pcds_where'])\n",
    "    \n",
    "    where_clause = \" AND \".join(where_parts) if where_parts else \"1=1\"\n",
    "    \n",
    "    # Build normalized concatenation for hash\n",
    "    normalized_exprs = []\n",
    "    for col_info in comparable_cols:\n",
    "        pcds_col = col_info['pcds_col']\n",
    "        pcds_type = col_info['pcds_type']\n",
    "        \n",
    "        # Normalize based on type\n",
    "        norm_expr = normalize_oracle_column(pcds_col, pcds_type)\n",
    "        normalized_exprs.append(norm_expr)\n",
    "    \n",
    "    # Concatenate with separator\n",
    "    concat_expr = \" || '|' || \".join(normalized_exprs)\n",
    "    \n",
    "    # Build unique ID select\n",
    "    unique_id_select = \",\\n    \".join(unique_id_cols)\n",
    "    \n",
    "    # Build SAS code\n",
    "    limit_clause = f\"(OBS={max_rows})\" if max_rows else \"\"\n",
    "    \n",
    "    sas_code = f\"\"\"\n",
    "/* Hash computation for {pcds_tbl} - Vintage: {vintage} */\n",
    "DATA work.hash_{pcds_tbl.replace('.', '_')}_{vintage.replace('-', '')};\n",
    "  SET {pcds_tbl}{limit_clause};\n",
    "  WHERE {where_clause};\n",
    "  \n",
    "  /* Compute normalized hash */\n",
    "  _concat_str = {concat_expr};\n",
    "  _hash = MD5(_concat_str);\n",
    "  \n",
    "  /* Keep only hash and unique IDs */\n",
    "  KEEP _hash {' '.join(unique_id_cols)};\n",
    "RUN;\n",
    "\n",
    "PROC EXPORT DATA=work.hash_{pcds_tbl.replace('.', '_')}_{vintage.replace('-', '')}\n",
    "  OUTFILE='/path/to/output/pcds_hash_{pcds_tbl.replace('.', '_')}_{vintage.replace('-', '')}.csv'\n",
    "  DBMS=CSV REPLACE;\n",
    "RUN;\n",
    "\"\"\"\n",
    "    \n",
    "    return sas_code\n",
    "\n",
    "\n",
    "print(\"SAS hash code generation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate SAS hash code for all tables\n",
    "sas_hash_parts = []\n",
    "sas_hash_parts.append(\"/* PCDS Row Hash Computation */\")\n",
    "sas_hash_parts.append(\"/* Generated: \" + datetime.now().isoformat() + \" */\\n\")\n",
    "\n",
    "for idx, (table_pair, col_mapping) in enumerate(zip(config['table_pairs'], config['column_mappings'])):\n",
    "    # Apply table filter\n",
    "    if FILTER_TABLE_INDICES and idx not in FILTER_TABLE_INDICES:\n",
    "        continue\n",
    "    \n",
    "    # Get vintages\n",
    "    if 'row_meta' in config:\n",
    "        vintages = list(set([\n",
    "            r['vintage'] for r in config['row_meta'] \n",
    "            if r['pcds_tbl'] == table_pair['pcds_tbl']\n",
    "        ]))\n",
    "    else:\n",
    "        vintages = ['whole']\n",
    "    \n",
    "    # Apply vintage filter\n",
    "    if FILTER_VINTAGES:\n",
    "        vintages = [v for v in vintages if v in FILTER_VINTAGES]\n",
    "    \n",
    "    for vintage in vintages:\n",
    "        sas_code = generate_sas_hash_code(\n",
    "            table_pair, \n",
    "            col_mapping, \n",
    "            vintage, \n",
    "            FILTER_COLUMNS,\n",
    "            MAX_ROWS_PER_VINTAGE\n",
    "        )\n",
    "        sas_hash_parts.append(sas_code)\n",
    "\n",
    "full_sas_hash = \"\\n\".join(sas_hash_parts)\n",
    "\n",
    "# Save SAS hash file\n",
    "with open(SAS_HASH_FILE, 'w') as f:\n",
    "    f.write(full_sas_hash)\n",
    "\n",
    "print(f\"✓ SAS hash code generated: {SAS_HASH_FILE}\")\n",
    "print(f\"  Total lines: {len(full_sas_hash.splitlines())}\")\n",
    "print(f\"\\n⚠ ACTION REQUIRED:\")\n",
    "print(f\"  1. Copy {SAS_HASH_FILE} to your SAS server\")\n",
    "print(f\"  2. Run the SAS program\")\n",
    "print(f\"  3. Collect the generated CSV files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Generate Athena SQL for AWS Row Hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_athena_hash_sql(table_pair, column_mapping, vintage='whole', filter_cols=None, max_rows=None):\n",
    "    \"\"\"\n",
    "    Generate Athena SQL to compute row hashes\n",
    "    \n",
    "    Args:\n",
    "        table_pair: Dict with table info\n",
    "        column_mapping: Dict with column mappings\n",
    "        vintage: Date partition or 'whole'\n",
    "        filter_cols: List of AWS columns to include (None = all)\n",
    "        max_rows: Maximum rows to process (None = all)\n",
    "    \n",
    "    Returns:\n",
    "        str: Athena SQL\n",
    "    \"\"\"\n",
    "    aws_tbl = table_pair['aws_tbl']\n",
    "    \n",
    "    # Get comparable columns (map to AWS column names)\n",
    "    comparable_cols = column_mapping['comparable_columns']\n",
    "    \n",
    "    if filter_cols:\n",
    "        # Filter by PCDS column names, then map to AWS\n",
    "        pcds_to_aws = column_mapping['pcds_to_aws']\n",
    "        filter_aws_cols = [pcds_to_aws.get(c) for c in filter_cols if c in pcds_to_aws]\n",
    "        comparable_cols = [c for c in comparable_cols if c['aws_col'] in filter_aws_cols]\n",
    "    \n",
    "    # Get unique ID columns (map to AWS)\n",
    "    unique_id_cols = []\n",
    "    if table_pair.get('unique_id_cols'):\n",
    "        pcds_ids = [c.strip() for c in table_pair['unique_id_cols'].split(',')]\n",
    "        pcds_to_aws = column_mapping['pcds_to_aws']\n",
    "        unique_id_cols = [pcds_to_aws.get(pid, pid.lower()) for pid in pcds_ids]\n",
    "    \n",
    "    if not unique_id_cols:\n",
    "        return f\"-- Skipping {aws_tbl} - No unique ID columns specified\\n\"\n",
    "    \n",
    "    # Build WHERE clause\n",
    "    where_parts = []\n",
    "    if vintage != 'whole' and table_pair.get('aws_dt'):\n",
    "        where_parts.append(f\"{table_pair['aws_dt']} = '{vintage}'\")\n",
    "    if table_pair.get('aws_where'):\n",
    "        where_parts.append(table_pair['aws_where'])\n",
    "    \n",
    "    where_clause = \" AND \".join(where_parts) if where_parts else \"1=1\"\n",
    "    \n",
    "    # Build normalized concatenation for hash\n",
    "    normalized_exprs = []\n",
    "    for col_info in comparable_cols:\n",
    "        aws_col = col_info['aws_col']\n",
    "        aws_type = col_info['aws_type']\n",
    "        \n",
    "        # Normalize based on type\n",
    "        norm_expr = normalize_athena_column(aws_col, aws_type)\n",
    "        normalized_exprs.append(norm_expr)\n",
    "    \n",
    "    # Concatenate with separator\n",
    "    concat_expr = \" || '|' || \".join(normalized_exprs)\n",
    "    \n",
    "    # Build unique ID select\n",
    "    unique_id_select = \",\\n  \".join(unique_id_cols)\n",
    "    \n",
    "    # Build SQL\n",
    "    limit_clause = f\"LIMIT {max_rows}\" if max_rows else \"\"\n",
    "    \n",
    "    sql = f\"\"\"\n",
    "-- Hash computation for {aws_tbl} - Vintage: {vintage}\n",
    "SELECT\n",
    "  MD5(TO_UTF8({concat_expr})) AS _hash,\n",
    "  {unique_id_select}\n",
    "FROM {aws_tbl}\n",
    "WHERE {where_clause}\n",
    "{limit_clause};\n",
    "\"\"\"\n",
    "    \n",
    "    return sql\n",
    "\n",
    "\n",
    "print(\"Athena hash SQL generation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Athena hash SQL for all tables\n",
    "athena_hash_parts = []\n",
    "athena_hash_parts.append(\"-- AWS Athena Row Hash Computation\")\n",
    "athena_hash_parts.append(\"-- Generated: \" + datetime.now().isoformat())\n",
    "athena_hash_parts.append(\"\")\n",
    "\n",
    "all_hash_queries = []\n",
    "\n",
    "for idx, (table_pair, col_mapping) in enumerate(zip(config['table_pairs'], config['column_mappings'])):\n",
    "    # Apply table filter\n",
    "    if FILTER_TABLE_INDICES and idx not in FILTER_TABLE_INDICES:\n",
    "        continue\n",
    "    \n",
    "    # Get vintages\n",
    "    if 'row_meta' in config:\n",
    "        vintages = list(set([\n",
    "            r['vintage'] for r in config['row_meta'] \n",
    "            if r['aws_tbl'] == table_pair['aws_tbl']\n",
    "        ]))\n",
    "    else:\n",
    "        vintages = ['whole']\n",
    "    \n",
    "    # Apply vintage filter\n",
    "    if FILTER_VINTAGES:\n",
    "        vintages = [v for v in vintages if v in FILTER_VINTAGES]\n",
    "    \n",
    "    for vintage in vintages:\n",
    "        sql = generate_athena_hash_sql(\n",
    "            table_pair, \n",
    "            col_mapping, \n",
    "            vintage,\n",
    "            FILTER_COLUMNS,\n",
    "            MAX_ROWS_PER_VINTAGE\n",
    "        )\n",
    "        athena_hash_parts.append(sql)\n",
    "        athena_hash_parts.append(\"\")\n",
    "        \n",
    "        all_hash_queries.append({\n",
    "            'table': table_pair['aws_tbl'],\n",
    "            'vintage': vintage,\n",
    "            'sql': sql\n",
    "        })\n",
    "\n",
    "full_athena_hash = \"\\n\".join(athena_hash_parts)\n",
    "\n",
    "# Save Athena hash SQL file\n",
    "with open(ATHENA_HASH_FILE, 'w') as f:\n",
    "    f.write(full_athena_hash)\n",
    "\n",
    "print(f\"✓ Athena hash SQL generated: {ATHENA_HASH_FILE}\")\n",
    "print(f\"  Total queries: {len(all_hash_queries)}\")\n",
    "print(f\"  Total lines: {len(full_athena_hash.splitlines())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Execute Athena Hash Queries (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for Athena execution\n",
    "# Similar to Step 4, implement execute_athena_query or use AWS console\n",
    "\n",
    "print(\"\\n⚠ Athena hash queries can be executed:\")\n",
    "print(\"  1. Programmatically (implement boto3 integration)\")\n",
    "print(\"  2. Copy/paste from SQL file to Athena console\")\n",
    "print(\"  3. Review and approve SQL file first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Load and Compare Hash Results\n",
    "\n",
    "After collecting PCDS and AWS hash CSVs, compare to find mismatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_hashes(pcds_csv, aws_csv, unique_id_cols):\n",
    "    \"\"\"\n",
    "    Compare PCDS and AWS hash CSVs to find mismatches\n",
    "    \n",
    "    Args:\n",
    "        pcds_csv: Path to PCDS hash CSV\n",
    "        aws_csv: Path to AWS hash CSV\n",
    "        unique_id_cols: List of unique ID column names\n",
    "    \n",
    "    Returns:\n",
    "        dict: Comparison results\n",
    "    \"\"\"\n",
    "    df_pcds = pd.read_csv(pcds_csv)\n",
    "    df_aws = pd.read_csv(aws_csv)\n",
    "    \n",
    "    print(f\"  PCDS rows: {len(df_pcds)}\")\n",
    "    print(f\"  AWS rows: {len(df_aws)}\")\n",
    "    \n",
    "    # Merge on unique IDs\n",
    "    df_merged = df_pcds.merge(\n",
    "        df_aws,\n",
    "        on=unique_id_cols,\n",
    "        how='outer',\n",
    "        suffixes=('_pcds', '_aws'),\n",
    "        indicator=True\n",
    "    )\n",
    "    \n",
    "    # Analyze results\n",
    "    both = df_merged[df_merged['_merge'] == 'both']\n",
    "    pcds_only = df_merged[df_merged['_merge'] == 'left_only']\n",
    "    aws_only = df_merged[df_merged['_merge'] == 'right_only']\n",
    "    \n",
    "    # Among rows in both, find hash mismatches\n",
    "    hash_match = both['_hash_pcds'] == both['_hash_aws']\n",
    "    hash_mismatch = both[~hash_match]\n",
    "    \n",
    "    results = {\n",
    "        'total_pcds': len(df_pcds),\n",
    "        'total_aws': len(df_aws),\n",
    "        'in_both': len(both),\n",
    "        'pcds_only': len(pcds_only),\n",
    "        'aws_only': len(aws_only),\n",
    "        'hash_match': hash_match.sum(),\n",
    "        'hash_mismatch': len(hash_mismatch),\n",
    "        'mismatch_rows': hash_mismatch[unique_id_cols + ['_hash_pcds', '_hash_aws']].to_dict('records')\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage (uncomment when CSVs are ready)\n",
    "# results = compare_hashes(\n",
    "#     'output/pcds_hash_table1_whole.csv',\n",
    "#     'output/aws_hash_table1_whole.csv',\n",
    "#     ['customer_id', 'transaction_id']\n",
    "# )\n",
    "# print(f\"Hash matches: {results['hash_match']}\")\n",
    "# print(f\"Hash mismatches: {results['hash_mismatch']}\")\n",
    "\n",
    "print(\"Hash comparison function defined\")\n",
    "print(\"\\nLoad CSV files after SAS and Athena complete to compare hashes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 Update Configuration JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update config\n",
    "config['row_hash_check'] = {\n",
    "    'sas_file': SAS_HASH_FILE,\n",
    "    'athena_sql_file': ATHENA_HASH_FILE,\n",
    "    'filter_tables': FILTER_TABLE_INDICES,\n",
    "    'filter_vintages': FILTER_VINTAGES,\n",
    "    'filter_columns': FILTER_COLUMNS,\n",
    "    'max_rows_per_vintage': MAX_ROWS_PER_VINTAGE,\n",
    "    'status': 'code_generated_awaiting_execution'\n",
    "}\n",
    "config['status']['step5_completed'] = True\n",
    "config['status']['step5_completed_at'] = datetime.now().isoformat()\n",
    "\n",
    "# Save updated config\n",
    "with open(CONFIG_JSON, 'w') as f:\n",
    "    json.dump(config, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\n✓ Configuration updated and saved to: {CONFIG_JSON}\")\n",
    "print(f\"\\n✓ Step 5 Complete - All validation steps finished!\")\n",
    "print(f\"\\n⚠ Next steps:\")\n",
    "print(f\"  1. Execute SAS hash code on SAS server\")\n",
    "print(f\"  2. Execute Athena hash queries\")\n",
    "print(f\"  3. Collect CSV results\")\n",
    "print(f\"  4. Run hash comparison using compare_hashes() function\")\n",
    "print(f\"  5. Investigate any mismatched rows using unique IDs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
