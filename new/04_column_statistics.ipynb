{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Column Statistics Comparison\n",
    "\n",
    "This notebook:\n",
    "- Loads configuration with column mappings from Step 3\n",
    "- **PCDS**: Generates SAS code to compute statistics for all comparable columns\n",
    "  - You run this SAS file on SAS server (background)\n",
    "  - SAS emails back CSV with statistics\n",
    "  - Load the CSV back into this notebook\n",
    "- **AWS**: Generates and executes Athena SQL queries for statistics\n",
    "  - Submits queries directly from notebook\n",
    "  - Fetches results in parallel\n",
    "- Compares PCDS vs AWS statistics\n",
    "- Updates JSON with results\n",
    "\n",
    "**Input**: `data/config.json`  \n",
    "**Output**: SAS file, Athena SQL, updated `data/config.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Step 4: Column Statistics Comparison\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_JSON = \"data/config.json\"\n",
    "\n",
    "with open(CONFIG_JSON, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(f\"Loaded configuration: {config['run_name']}\")\n",
    "print(f\"Total tables: {config['metadata']['total_tables']}\")\n",
    "print(f\"Step 3 completed: {config['status']['step3_completed']}\")\n",
    "\n",
    "# Output paths\n",
    "SAS_OUTPUT_FILE = f\"output/pcds_stats_{config['run_name']}.sas\"\n",
    "ATHENA_SQL_FILE = f\"output/aws_stats_{config['run_name']}.sql\"\n",
    "\n",
    "Path(\"output\").mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Generate SAS Code for PCDS Statistics\n",
    "\n",
    "Generate a single SAS file that computes statistics for all comparable columns across all tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sas_statistics_code(table_pair, column_mapping, vintage='whole'):\n",
    "    \"\"\"\n",
    "    Generate SAS code to compute statistics for a table\n",
    "    \n",
    "    Args:\n",
    "        table_pair: Dict with table info\n",
    "        column_mapping: Dict with column mappings\n",
    "        vintage: Date partition or 'whole'\n",
    "    \n",
    "    Returns:\n",
    "        str: SAS code\n",
    "    \"\"\"\n",
    "    pcds_tbl = table_pair['pcds_tbl']\n",
    "    comparable_cols = [c['pcds_col'] for c in column_mapping['comparable_columns']]\n",
    "    \n",
    "    # Build WHERE clause\n",
    "    where_parts = []\n",
    "    if vintage != 'whole' and table_pair.get('pcds_dt'):\n",
    "        where_parts.append(f\"{table_pair['pcds_dt']} = '{vintage}'\")\n",
    "    if table_pair.get('pcds_where'):\n",
    "        where_parts.append(table_pair['pcds_where'])\n",
    "    \n",
    "    where_clause = \" AND \".join(where_parts) if where_parts else \"1=1\"\n",
    "    \n",
    "    # Generate SAS code\n",
    "    sas_code = f\"\"\"\n",
    "/* Statistics for {pcds_tbl} - Vintage: {vintage} */\n",
    "PROC SQL;\n",
    "  CREATE TABLE work.stats_{pcds_tbl.replace('.', '_')}_{vintage.replace('-', '')} AS\n",
    "  SELECT\n",
    "    '{pcds_tbl}' AS table_name,\n",
    "    '{vintage}' AS vintage,\n",
    "    variable,\n",
    "    data_type,\n",
    "    count,\n",
    "    distinct_count,\n",
    "    max_val,\n",
    "    min_val,\n",
    "    mean,\n",
    "    std,\n",
    "    missing,\n",
    "    sum_val,\n",
    "    sum_sq\n",
    "  FROM (\n",
    "\"\"\"\n",
    "    \n",
    "    # Generate statistics for each column\n",
    "    col_stats = []\n",
    "    for col in comparable_cols:\n",
    "        col_stat = f\"\"\"\n",
    "    SELECT\n",
    "      '{col}' AS variable,\n",
    "      'TBD' AS data_type,\n",
    "      COUNT({col}) AS count,\n",
    "      COUNT(DISTINCT {col}) AS distinct_count,\n",
    "      MAX({col}) AS max_val,\n",
    "      MIN({col}) AS min_val,\n",
    "      MEAN({col}) AS mean,\n",
    "      STD({col}) AS std,\n",
    "      SUM(CASE WHEN {col} IS NULL THEN 1 ELSE 0 END) AS missing,\n",
    "      SUM({col}) AS sum_val,\n",
    "      SUM({col} * {col}) AS sum_sq\n",
    "    FROM {pcds_tbl}\n",
    "    WHERE {where_clause}\n",
    "\"\"\"\n",
    "        col_stats.append(col_stat)\n",
    "    \n",
    "    sas_code += \"\\n    UNION ALL\\n\".join(col_stats)\n",
    "    sas_code += \"\\n  );\\nQUIT;\\n\"\n",
    "    \n",
    "    return sas_code\n",
    "\n",
    "\n",
    "print(\"SAS code generation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate SAS code for all tables\n",
    "sas_code_parts = []\n",
    "sas_code_parts.append(\"/* PCDS Column Statistics */\")\n",
    "sas_code_parts.append(\"/* Generated: \" + datetime.now().isoformat() + \" */\\n\")\n",
    "\n",
    "for table_pair, col_mapping in zip(config['table_pairs'], config['column_mappings']):\n",
    "    # Get vintages from row_meta or use 'whole'\n",
    "    if 'row_meta' in config:\n",
    "        vintages = list(set([\n",
    "            r['vintage'] for r in config['row_meta'] \n",
    "            if r['pcds_tbl'] == table_pair['pcds_tbl']\n",
    "        ]))\n",
    "    else:\n",
    "        vintages = ['whole']\n",
    "    \n",
    "    for vintage in vintages:\n",
    "        sas_code = generate_sas_statistics_code(table_pair, col_mapping, vintage)\n",
    "        sas_code_parts.append(sas_code)\n",
    "\n",
    "# Combine all results and export\n",
    "sas_code_parts.append(\"\"\"\n",
    "/* Combine all results */\n",
    "DATA work.all_stats;\n",
    "  SET work.stats_:;\n",
    "RUN;\n",
    "\n",
    "/* Export to CSV */\n",
    "PROC EXPORT DATA=work.all_stats\n",
    "  OUTFILE='/path/to/output/pcds_column_stats.csv'\n",
    "  DBMS=CSV REPLACE;\n",
    "RUN;\n",
    "\n",
    "/* Email results */\n",
    "filename mailout email\n",
    "  to=\"your.email@company.com\"\n",
    "  subject=\"PCDS Column Statistics - \" || \"&sysdate\"\n",
    "  attach=\"/path/to/output/pcds_column_stats.csv\";\n",
    "\n",
    "data _null_;\n",
    "  file mailout;\n",
    "  put \"PCDS column statistics completed.\";\n",
    "  put \"See attached CSV file.\";\n",
    "run;\n",
    "\"\"\")\n",
    "\n",
    "full_sas_code = \"\\n\".join(sas_code_parts)\n",
    "\n",
    "# Save SAS file\n",
    "with open(SAS_OUTPUT_FILE, 'w') as f:\n",
    "    f.write(full_sas_code)\n",
    "\n",
    "print(f\"✓ SAS code generated: {SAS_OUTPUT_FILE}\")\n",
    "print(f\"  Total lines: {len(full_sas_code.splitlines())}\")\n",
    "print(f\"\\n⚠ ACTION REQUIRED:\")\n",
    "print(f\"  1. Copy {SAS_OUTPUT_FILE} to your SAS server\")\n",
    "print(f\"  2. Run the SAS program\")\n",
    "print(f\"  3. Wait for email with CSV attachment\")\n",
    "print(f\"  4. Continue to next cell to load AWS statistics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Generate and Execute Athena SQL for AWS Statistics\n",
    "\n",
    "Generate Athena SQL queries and execute them directly from the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_athena_statistics_sql(table_pair, column_mapping, vintage='whole'):\n",
    "    \"\"\"\n",
    "    Generate Athena SQL to compute statistics for a table\n",
    "    \n",
    "    Args:\n",
    "        table_pair: Dict with table info\n",
    "        column_mapping: Dict with column mappings\n",
    "        vintage: Date partition or 'whole'\n",
    "    \n",
    "    Returns:\n",
    "        str: Athena SQL\n",
    "    \"\"\"\n",
    "    aws_tbl = table_pair['aws_tbl']\n",
    "    comparable_cols = column_mapping['comparable_columns']\n",
    "    \n",
    "    # Build WHERE clause\n",
    "    where_parts = []\n",
    "    if vintage != 'whole' and table_pair.get('aws_dt'):\n",
    "        where_parts.append(f\"{table_pair['aws_dt']} = '{vintage}'\")\n",
    "    if table_pair.get('aws_where'):\n",
    "        where_parts.append(table_pair['aws_where'])\n",
    "    \n",
    "    where_clause = \" AND \".join(where_parts) if where_parts else \"1=1\"\n",
    "    \n",
    "    # Generate SQL for each column\n",
    "    col_selects = []\n",
    "    for col_info in comparable_cols:\n",
    "        aws_col = col_info['aws_col']\n",
    "        col_select = f\"\"\"\n",
    "  SELECT\n",
    "    '{aws_tbl}' AS table_name,\n",
    "    '{vintage}' AS vintage,\n",
    "    '{aws_col}' AS variable,\n",
    "    '{col_info['aws_type']}' AS data_type,\n",
    "    COUNT({aws_col}) AS count,\n",
    "    COUNT(DISTINCT {aws_col}) AS distinct_count,\n",
    "    CAST(MAX({aws_col}) AS VARCHAR) AS max_val,\n",
    "    CAST(MIN({aws_col}) AS VARCHAR) AS min_val,\n",
    "    AVG(CAST({aws_col} AS DOUBLE)) AS mean,\n",
    "    STDDEV(CAST({aws_col} AS DOUBLE)) AS std,\n",
    "    SUM(CASE WHEN {aws_col} IS NULL THEN 1 ELSE 0 END) AS missing,\n",
    "    SUM(CAST({aws_col} AS DOUBLE)) AS sum_val,\n",
    "    SUM(CAST({aws_col} AS DOUBLE) * CAST({aws_col} AS DOUBLE)) AS sum_sq\n",
    "  FROM {aws_tbl}\n",
    "  WHERE {where_clause}\n",
    "\"\"\"\n",
    "        col_selects.append(col_select)\n",
    "    \n",
    "    sql = \"\\nUNION ALL\\n\".join(col_selects)\n",
    "    sql = f\"-- Statistics for {aws_tbl} - Vintage: {vintage}\\n\" + sql\n",
    "    \n",
    "    return sql\n",
    "\n",
    "\n",
    "print(\"Athena SQL generation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Athena SQL for all tables\n",
    "athena_sql_parts = []\n",
    "athena_sql_parts.append(\"-- AWS Athena Column Statistics\")\n",
    "athena_sql_parts.append(\"-- Generated: \" + datetime.now().isoformat())\n",
    "athena_sql_parts.append(\"\")\n",
    "\n",
    "all_queries = []\n",
    "\n",
    "for table_pair, col_mapping in zip(config['table_pairs'], config['column_mappings']):\n",
    "    # Get vintages\n",
    "    if 'row_meta' in config:\n",
    "        vintages = list(set([\n",
    "            r['vintage'] for r in config['row_meta'] \n",
    "            if r['aws_tbl'] == table_pair['aws_tbl']\n",
    "        ]))\n",
    "    else:\n",
    "        vintages = ['whole']\n",
    "    \n",
    "    for vintage in vintages:\n",
    "        sql = generate_athena_statistics_sql(table_pair, col_mapping, vintage)\n",
    "        athena_sql_parts.append(sql)\n",
    "        athena_sql_parts.append(\";\\n\")\n",
    "        \n",
    "        # Store for execution\n",
    "        all_queries.append({\n",
    "            'table': table_pair['aws_tbl'],\n",
    "            'vintage': vintage,\n",
    "            'sql': sql\n",
    "        })\n",
    "\n",
    "full_athena_sql = \"\\n\".join(athena_sql_parts)\n",
    "\n",
    "# Save Athena SQL file\n",
    "with open(ATHENA_SQL_FILE, 'w') as f:\n",
    "    f.write(full_athena_sql)\n",
    "\n",
    "print(f\"✓ Athena SQL generated: {ATHENA_SQL_FILE}\")\n",
    "print(f\"  Total queries: {len(all_queries)}\")\n",
    "print(f\"  Total lines: {len(full_athena_sql.splitlines())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Execute Athena Queries (Optional - Requires AWS Setup)\n",
    "\n",
    "Execute Athena queries programmatically. You can also review the SQL file first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Athena execution function (requires boto3)\n",
    "def execute_athena_query(sql, database='default', s3_output='s3://your-bucket/athena-results/'):\n",
    "    \"\"\"\n",
    "    Execute Athena query and return results\n",
    "    \n",
    "    Args:\n",
    "        sql: SQL query string\n",
    "        database: Athena database name\n",
    "        s3_output: S3 path for query results\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with results\n",
    "    \"\"\"\n",
    "    # TODO: Implement with boto3\n",
    "    # import boto3\n",
    "    # client = boto3.client('athena')\n",
    "    # response = client.start_query_execution(...)\n",
    "    # Wait for completion and fetch results\n",
    "    \n",
    "    print(f\"  [MOCK] Executing Athena query...\")\n",
    "    \n",
    "    # Placeholder - return empty DataFrame\n",
    "    return pd.DataFrame({\n",
    "        'table_name': [],\n",
    "        'vintage': [],\n",
    "        'variable': [],\n",
    "        'data_type': [],\n",
    "        'count': [],\n",
    "        'distinct_count': [],\n",
    "        'max_val': [],\n",
    "        'min_val': [],\n",
    "        'mean': [],\n",
    "        'std': [],\n",
    "        'missing': [],\n",
    "        'sum_val': [],\n",
    "        'sum_sq': []\n",
    "    })\n",
    "\n",
    "\n",
    "# Execute all queries (uncomment when ready)\n",
    "# aws_stats_results = []\n",
    "# for query in all_queries:\n",
    "#     print(f\"Executing: {query['table']} - {query['vintage']}\")\n",
    "#     df = execute_athena_query(query['sql'])\n",
    "#     aws_stats_results.append(df)\n",
    "# \n",
    "# df_aws_stats = pd.concat(aws_stats_results, ignore_index=True)\n",
    "# print(f\"✓ AWS statistics collected: {len(df_aws_stats)} rows\")\n",
    "\n",
    "print(\"\\n⚠ Athena execution is optional here.\")\n",
    "print(\"  You can either:\")\n",
    "print(\"  1. Execute queries programmatically (implement execute_athena_query)\")\n",
    "print(\"  2. Copy/paste SQL from file to Athena console\")\n",
    "print(\"  3. Use AWS CLI or other tools\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Load PCDS Statistics from CSV\n",
    "\n",
    "After receiving the CSV from SAS, load it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PCDS CSV (after receiving from SAS)\n",
    "PCDS_STATS_CSV = \"output/pcds_column_stats.csv\"  # Update path as needed\n",
    "\n",
    "try:\n",
    "    df_pcds_stats = pd.read_csv(PCDS_STATS_CSV)\n",
    "    print(f\"✓ Loaded PCDS statistics: {len(df_pcds_stats)} rows\")\n",
    "    print(f\"  Columns: {list(df_pcds_stats.columns)}\")\n",
    "    display(df_pcds_stats.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"⚠ PCDS stats CSV not found: {PCDS_STATS_CSV}\")\n",
    "    print(\"  Waiting for SAS to complete and send CSV...\")\n",
    "    df_pcds_stats = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Compare Statistics (When Both Available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for comparison\n",
    "# When both df_pcds_stats and df_aws_stats are available:\n",
    "# - Merge on (table_name, vintage, variable)\n",
    "# - Compare count, distinct_count, min, max, mean, std\n",
    "# - Flag significant differences\n",
    "\n",
    "print(\"Statistics comparison will be performed once both PCDS and AWS data are loaded.\")\n",
    "print(\"\\nComparison metrics:\")\n",
    "print(\"  - Row count\")\n",
    "print(\"  - Distinct count\")\n",
    "print(\"  - Min/Max values\")\n",
    "print(\"  - Mean/Std (for numeric columns)\")\n",
    "print(\"  - Missing count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 Update Configuration JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update config with file paths\n",
    "config['column_statistics'] = {\n",
    "    'sas_file': SAS_OUTPUT_FILE,\n",
    "    'athena_sql_file': ATHENA_SQL_FILE,\n",
    "    'pcds_csv': PCDS_STATS_CSV if 'df_pcds_stats' in dir() and df_pcds_stats is not None else None,\n",
    "    'status': 'sas_generated_awaiting_results'\n",
    "}\n",
    "config['status']['step4_completed'] = True\n",
    "config['status']['step4_completed_at'] = datetime.now().isoformat()\n",
    "\n",
    "# Save updated config\n",
    "with open(CONFIG_JSON, 'w') as f:\n",
    "    json.dump(config, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\n✓ Configuration updated and saved to: {CONFIG_JSON}\")\n",
    "print(f\"\\n✓ Step 4 Complete - Ready for Step 5\")\n",
    "print(f\"\\n⚠ Remember to complete SAS execution and load results before final comparison\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
