{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Analysis Step 3: Comparison and Aggregation\n",
    "\n",
    "This notebook compares PCDS and AWS metadata and generates final reports.\n",
    "\n",
    "**Purpose:**\n",
    "- Load results from Step 1 (PCDS) and Step 2 (AWS)\n",
    "- Compare column mappings and data types\n",
    "- Identify mismatches in row counts and date ranges\n",
    "- Generate comprehensive CSV report\n",
    "- Create JSON output for column_statistics step\n",
    "- Upload results to S3 (optional)\n",
    "\n",
    "**Inputs:**\n",
    "- `pcds_meta_results.pkl` - PCDS metadata from Step 1\n",
    "- `aws_meta_results.pkl` - AWS metadata from Step 2\n",
    "\n",
    "**Outputs:**\n",
    "- CSV report with comparison results\n",
    "- JSON file for next pipeline step\n",
    "- S3 upload (optional)\n",
    "\n",
    "**Note:** This step only performs comparisons and does not connect to any database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import re\nimport os\nimport csv\nimport json\nimport shutil\nimport pickle\nimport argparse\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport awswrangler as aws\nimport boto3\n\nfrom upath import UPath\nfrom loguru import logger\nfrom tqdm import tqdm\nfrom datetime import datetime, timedelta, timezone\nfrom dataclasses import dataclass, field\nfrom configparser import ConfigParser\nfrom confection import Config\nfrom unittest import mock\nfrom enum import Enum\nfrom typing import Literal, Dict, List\nfrom collections import defaultdict, abc\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning, message='.*pandas only supports SQLAlchemy connectable.*')\n\n# Note: This notebook uses Parquet format for cross-platform compatibility\n# Install pyarrow if needed: pip install pyarrow or conda install -c conda-forge pyarrow\nimport pyarrow"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Constants and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Global Constants ---\n",
    "SEP = '; '\n",
    "WIDTH = 80\n",
    "NO_DATE = 'no_date_provided'\n",
    "SESSION = None\n",
    "\n",
    "class PullStatus(Enum):\n",
    "    \"\"\"Enumeration for data pull status codes\"\"\"\n",
    "    NONEXIST_PCDS = 'Nonexisting PCDS Table'\n",
    "    NONEXIST_AWS = 'Nonexisting AWS Table'\n",
    "    NONDATE_PCDS = 'Nonexisting Date Variable in PCDS'\n",
    "    NONDATE_AWS = 'Nonexisting Date Variable in AWS'\n",
    "    EMPTY_PCDS = 'Empty PCDS Table'\n",
    "    EMPTY_AWS = 'Empty AWS Table'\n",
    "    NO_MAPPING = 'Column Mapping Not Provided'\n",
    "    SUCCESS = 'Successful Data Access'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Data Types and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions ---\n",
    "def read_str_lst(lst_str, sep='\\n'):\n",
    "    \"\"\"Parse newline-separated string into list\"\"\"\n",
    "    return [x for x in lst_str.strip().split(sep) if x]\n",
    "\n",
    "def read_dstr_lst(dct_str, sep='='):\n",
    "    \"\"\"Parse key=value pairs into dictionary\"\"\"\n",
    "    d = dict(line.split(sep, 1) for line in read_str_lst(dct_str))\n",
    "    return {k.strip(): v.strip() for k, v in d.items()}\n",
    "\n",
    "@dataclass\n",
    "class MetaMerge:\n",
    "    \"\"\"Results from merging PCDS and AWS column metadata\"\"\"\n",
    "    unique_pcds: list\n",
    "    unique_aws: list\n",
    "    col_mapping: pd.DataFrame\n",
    "    mismatches: str\n",
    "    uncaptured: str\n",
    "\n",
    "@dataclass\n",
    "class MetaMatch:\n",
    "    \"\"\"Column matching configuration\"\"\"\n",
    "    candidates: str\n",
    "    drop_cols: dict\n",
    "    add_cols: dict\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.candidates = read_str_lst(self.candidates)\n",
    "        self.drop_cols = list(self.drop_cols)\n",
    "        self.add_cols = list(self.add_cols)\n",
    "\n",
    "#--- Start logging session with separator ---#\n",
    "def start_run():\n",
    "    logger.info('\\n\\n' + '=' * WIDTH)\n",
    "\n",
    "#--- End logging session with separator ---#\n",
    "def end_run():\n",
    "    logger.info('\\n\\n' + '=' * WIDTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Utility Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class IO:\n    \"\"\"File I/O utility class - uses Parquet/JSON for cross-platform compatibility\"\"\"\n\n    @staticmethod\n    def write_dataframe(file, df):\n        \"\"\"Save DataFrame in portable Parquet format\"\"\"\n        file = UPath(file)\n        df.to_parquet(file, index=True, engine='pyarrow', compression='snappy')\n\n    @staticmethod\n    def read_dataframe(file):\n        \"\"\"Load DataFrame from Parquet format\"\"\"\n        file = UPath(file)\n        return pd.read_parquet(file, engine='pyarrow')\n\n    @staticmethod\n    def write_json(file, data, cls=None):\n        \"\"\"Save to JSON with proper serialization\"\"\"\n        import numpy as np\n        import pandas as pd\n        import datetime as dt\n\n        def convert(obj):\n            if isinstance(obj, (np.integer, np.floating)):\n                return obj.item()\n            elif isinstance(obj, np.ndarray):\n                return obj.tolist()\n            elif pd.isna(obj):\n                return None\n            elif isinstance(obj, (dt.datetime, dt.date)):\n                return obj.isoformat()\n            elif isinstance(obj, set):\n                return list(obj)\n            raise TypeError(f\"Object of type {type(obj)} is not JSON serializable\")\n\n        with open(file, 'w') as f:\n            json.dump(data, f, indent=2, default=convert, cls=cls)\n\n    @staticmethod\n    def read_json(file):\n        \"\"\"Read JSON file into dictionary\"\"\"\n        with open(file, 'r') as fp:\n            data = json.load(fp)\n        return data\n\n    @staticmethod\n    def write_pickle(file, data):\n        \"\"\"Deprecated: Use write_dataframe or write_json instead\"\"\"\n        with open(file, 'wb') as f:\n            pickle.dump(data, f)\n\n    @staticmethod\n    def read_pickle(file):\n        \"\"\"Deprecated: Use read_dataframe or read_json instead\"\"\"\n        with open(file, 'rb') as fp:\n            data = pickle.load(fp)\n        return data\n\n    @staticmethod\n    def delete_file(file):\n        \"\"\"Delete file if it exists\"\"\"\n        if (filepath := UPath(file)).exists():\n            filepath.unlink()\n\nclass UDict(dict):\n    \"\"\"Case-insensitive dictionary for flexible key matching\"\"\"\n\n    def __getitem__(self, key):\n        return super().__getitem__(self._match(key))\n\n    def __contains__(self, key):\n        try:\n            self._match(key)\n            return True\n        except KeyError:\n            return False\n\n    def _match(self, key):\n        \"\"\"Find matching key regardless of case\"\"\"\n        for k in self:\n            if k.lower() == key.lower():\n                return k\n        raise KeyError(key)\n\n    def update(self, other=None, **kwargs):\n        if other is not None:\n            for k, v in other.items() if isinstance(other, abc.Mapping) else other:\n                self[k] = v\n        for k, v in kwargs.items():\n            assert self._match(k)\n            self[k] = v\n\n    def get(self, key, default_value=None):\n        try:\n            return self[key]\n        except KeyError:\n            return default_value\n\nclass Misc:\n    \"\"\"Miscellaneous utility functions\"\"\"\n\n    @staticmethod\n    def remove_items(input_str, delete_lst):\n        \"\"\"Remove specific items from semicolon-separated string\"\"\"\n        pattern = '|'.join(r'\\b%s\\b;?\\s?' % x for x in delete_lst)\n        return re.sub(pattern, '', input_str).rstrip('; ')\n\n    @staticmethod\n    def prefix(a, b):\n        \"\"\"Check if either string is prefix of the other\"\"\"\n        return a.startswith(b) or b.startswith(a)\n\n    @staticmethod\n    def common(a, b, use_prefix=False):\n        \"\"\"Find common items between two lists with optional prefix matching\"\"\"\n        def prefix_cmp(a, b):\n            return a.startswith(b) or b.startswith(a)\n\n        result, visited = {}, set()\n        prefix_d = defaultdict(list)\n\n        #>>> Build prefix matching dictionary <<<#\n        for x, y in [(x, y) for x in a for y in b]:\n            if prefix_cmp(x, y):\n                prefix_d[x].append(y)\n\n        #>>> Prioritize exact matches <<<#\n        for x in a:\n            if x in b and x not in visited:\n                result[x] = x\n                visited.add(x)\n\n        #>>> Handle prefix matches for remaining items <<<#\n        for x in a:\n            if x in result and (not use_prefix):\n                continue\n            for y in prefix_d[x]:\n                if y not in visited:\n                    result[x] = y\n                    visited.add(y)\n        return result\n\n    @staticmethod\n    def convert2int(a):\n        \"\"\"Safely convert value to integer\"\"\"\n        try:\n            return int(a)\n        except (TypeError, ValueError):\n            return None\n\n    @staticmethod\n    def convert2datestr(a):\n        \"\"\"Convert datetime to string format\"\"\"\n        if isinstance(a, datetime):\n            return a.strftime('%Y-%m-%d')\n        return a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: S3 Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S3:\n",
    "    \"\"\"AWS S3 utility functions\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def upload_multiple(s3_url, folder, prefix=''):\n",
    "        \"\"\"Upload multiple files from folder to S3\"\"\"\n",
    "        folder, s3_url = UPath(folder), UPath(s3_url)\n",
    "        for file in folder.glob('%s.*' % prefix):\n",
    "            aws.s3.upload(\n",
    "                local_file=file.as_posix(),\n",
    "                path=s3_url.joinpath(file.name).as_posix(),\n",
    "                boto3_session=SESSION\n",
    "            )\n",
    "            logger.info(f\"Uploading {file.name} to {s3_url} [finished]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Data Type Mapping Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Check if PCDS and AWS data types are compatible ---#\n",
    "def map_pcds_aws(row):\n",
    "    aws_dtype = row.data_type_aws\n",
    "    match (pcds_dtype := row.data_type_pcds):\n",
    "        case 'NUMBER':\n",
    "            ok_1 = aws_dtype == 'double'\n",
    "            return ok_1\n",
    "        case _ if pcds_dtype.startswith('NUMBER'):\n",
    "            y1 = re.match(r'NUMBER\\(\\d*,(\\d+)\\)', pcds_dtype).group(1)\n",
    "            match = re.match(r'decimal\\(\\d*,(\\d+)\\)', aws_dtype)\n",
    "            return bool(match and match.group(1) == y1)\n",
    "        case _ if pcds_dtype.startswith('VARCHAR2'):\n",
    "            return pcds_dtype.replace('VARCHAR2', 'varchar') == aws_dtype\n",
    "        case _ if pcds_dtype.startswith('CHAR'):\n",
    "            n = re.match(r'CHAR\\((\\d+)\\)', pcds_dtype).group(1)\n",
    "            return not (aws_dtype.startswith('VARCHAR') and n != 1)\n",
    "        case 'DATE':\n",
    "            ok_1 = aws_dtype == 'date'\n",
    "            ok_2 = aws_dtype.startswith('timestamp')\n",
    "            return ok_1 | ok_2\n",
    "        case _ if pcds_dtype.startswith('TIMESTAMP'):\n",
    "            return aws_dtype.startswith('timestamp')\n",
    "        case _:\n",
    "            s = \">>> Mismatched type on {}\\n\\tPCDS ({}) ==> AWS ({})\"\n",
    "            logger.info(s.format(row.column_name_aws, pcds_dtype, aws_dtype))\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Metadata Comparison Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Merge and compare PCDS and AWS column metadata ---#\n",
    "def process_merge(pcds: pd.DataFrame, aws: pd.DataFrame, tokenised_cols: list) -> MetaMerge:\n",
    "    \"\"\"\n",
    "    Check column mapping and variable typing differences\n",
    "    Returns unique columns, type mismatches, and uncaptured mappings\n",
    "    \"\"\"\n",
    "    #>>> Find columns without documented mappings <<<#\n",
    "    unmapped_pcds = (\n",
    "        pcds.query('aws_colname != aws_colname')\n",
    "        ['column_name'].str.lower().to_list()\n",
    "    )\n",
    "    unmapped_aws = (\n",
    "        aws.query('~column_name.isin(@pcds.aws_colname)')\n",
    "        ['column_name'].to_list()\n",
    "    )\n",
    "    \n",
    "    #>>> Use substring matching to find undocumented pairs <<<#\n",
    "    map_uncaptured = Misc.common(unmapped_pcds, unmapped_aws)\n",
    "    map_uncaptured = {\n",
    "        k.upper(): v for k, v in map_uncaptured.items()\n",
    "        if k not in tokenised_cols\n",
    "    }\n",
    "    uncaptured = SEP.join('{}->{}'.format(k, v) for k, v in map_uncaptured.items())\n",
    "\n",
    "    #>>> Update column mappings with discovered pairs <<<#\n",
    "    pcds['aws_colname'] = (\n",
    "        pcds['aws_colname']\n",
    "        .combine_first(pcds['column_name'].map(map_uncaptured))\n",
    "    )\n",
    "    \n",
    "    #>>> Merge PCDS and AWS metadata <<<#\n",
    "    df_match = pd.merge(\n",
    "        left=pcds, right=aws,\n",
    "        left_on='aws_colname', right_on='column_name',\n",
    "        suffixes=['_pcds', '_aws'],\n",
    "        how='outer', indicator=True\n",
    "    )\n",
    "    \n",
    "    #>>> Separate unique columns from each platform <<<#\n",
    "    pcds_cols = ['column_name_pcds', 'data_type_pcds']\n",
    "    pcds_unique = df_match.query('_merge == \"left_only\"')[pcds_cols]\n",
    "    aws_cols = ['column_name_aws', 'data_type_aws']\n",
    "    aws_unique = df_match.query('_merge == \"right_only\"')[aws_cols]\n",
    "\n",
    "    #>>> Check data type compatibility for matched columns <<<#\n",
    "    merged = (\n",
    "        df_match.query('_merge == \"both\"')\n",
    "        .drop(columns=['aws_colname', '_merge'])\n",
    "    )\n",
    "    merged['type_match'] = merged.apply(map_pcds_aws, axis=1)\n",
    "    mismatch_d = (\n",
    "        merged.query('~type_match')\n",
    "        [['data_type_pcds', 'data_type_aws']]\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "    mismatched = SEP.join('{}->{}'.format(*x[1:]) for x in mismatch_d.itertuples())\n",
    "\n",
    "    #>>> Filter out previously known unique columns <<<#\n",
    "    unmapped_pcds = pcds_unique['column_name_pcds'].str.upper().to_list()\n",
    "    unmapped_aws = aws_unique['column_name_aws'].str.lower().to_list()\n",
    "\n",
    "    return MetaMerge(\n",
    "        unique_pcds=unmapped_pcds,\n",
    "        unique_aws=unmapped_aws,\n",
    "        col_mapping=merged,\n",
    "        mismatches=mismatched,\n",
    "        uncaptured=uncaptured\n",
    "    )\n",
    "\n",
    "#--- Compare column mappings and total records between PCDS and AWS ---#\n",
    "def process_meta(pcds_t: dict, aws_t: dict, tokenised_cols: list) -> dict:\n",
    "    uncaptured = \"\"\n",
    "    pcds_c, aws_c = pcds_t['column'], aws_t['column']\n",
    "    \n",
    "    #>>> Handle missing column mapping <<<#\n",
    "    if 'aws_colname' not in pcds_c.columns or pcds_c['aws_colname'].isna().all():\n",
    "        pcds_c['aws_colname'] = pcds_c['column_name'].str.lower()\n",
    "        uncaptured = \"Column Mapping Not Provided\"\n",
    "    \n",
    "    profile = process_merge(pcds_c, aws_c, tokenised_cols)\n",
    "    logger.info(\">>> Finish Merging Type Data\")\n",
    "\n",
    "    #>>> Prepare comparison results <<<#\n",
    "    pcds_nrows = int(pcds_t['row'].iloc[0, 0])\n",
    "    aws_nrows = int(aws_t['row'].iloc[0, 0])\n",
    "    \n",
    "    return {\n",
    "        'Row UnMatch': pcds_nrows != aws_nrows,\n",
    "        'Row UnMatch Details': f\"PCDS({pcds_nrows}) : AWS({aws_nrows})\",\n",
    "        'Type UnMatch Details': profile.mismatches,\n",
    "        'Column Type UnMatch': len(profile.mismatches) > 0,\n",
    "        'PCDS Extra Columns': len(profile.unique_pcds) > 0,\n",
    "        'PCDS Unique Columns': SEP.join(profile.unique_pcds),\n",
    "        'AWS Extra Columns': len(profile.unique_aws) > 0,\n",
    "        'AWS Unique Columns': SEP.join(profile.unique_aws),\n",
    "        'Uncaptured Column Mappings': uncaptured or profile.uncaptured,\n",
    "        'col_mapping': profile.col_mapping\n",
    "    }\n",
    "\n",
    "#--- Identify specific dates with row count discrepancies ---#\n",
    "def process_date(cnt_pcds: pd.DataFrame, cnt_aws: pd.DataFrame, pcds_dateraw: str, aws_dateraw: str):\n",
    "    def get_date(a, b):\n",
    "        \"\"\"Return first non-null date\"\"\"\n",
    "        return b if pd.isna(a) else a\n",
    "\n",
    "    def get_detailed_mismatch():\n",
    "        \"\"\"Format mismatch details\"\"\"\n",
    "        a, _, b, _ = time_mismatch.columns\n",
    "        return '; '.join(\n",
    "            f\"{get_date(r[a], r[b])} ({r['nrows_pcds']} : {Misc.convert2int(r['nrows_aws'])})\"\n",
    "            for _, r in time_mismatch.iterrows()\n",
    "        )\n",
    "\n",
    "    def get_time_excludes_sql():\n",
    "        \"\"\"Build SQL to exclude problematic dates\"\"\"\n",
    "        pcds_col = pcds_dateraw.upper()\n",
    "        aws_col = aws_dateraw.lower()\n",
    "        exclude = ','.join(\"'%s'\" % x for x in time_mismatch[pcds_col].fillna(time_mismatch[aws_col]) if x)\n",
    "        return {\n",
    "            'pcds_exclude': f'{pcds_col} not in ({exclude})',\n",
    "            'aws_exclude': f'{aws_col} not in ({exclude})',\n",
    "        }\n",
    "\n",
    "    pcds_dt = pcds_dateraw.upper()\n",
    "    aws_dt = aws_dateraw.lower()\n",
    "    \n",
    "    # Rename columns for consistency\n",
    "    if 'NROWS' in cnt_pcds.columns:\n",
    "        cnt_pcds = cnt_pcds.rename(columns={'NROWS': 'nrows_pcds'})\n",
    "    else:\n",
    "        cnt_pcds = cnt_pcds.rename(columns={'nrows': 'nrows_pcds'})\n",
    "    \n",
    "    cnt_aws = cnt_aws.rename(columns={'nrows': 'nrows_aws'})\n",
    "    \n",
    "    #>>> Merge date-wise row counts <<<#\n",
    "    df_all = pd.merge(\n",
    "        left=cnt_pcds,\n",
    "        right=cnt_aws,\n",
    "        left_on=pcds_dt,\n",
    "        right_on=aws_dt,\n",
    "        suffixes=['_pcds', '_aws'],\n",
    "        how='outer'\n",
    "    )\n",
    "    \n",
    "    time_mismatch = df_all.query('nrows_pcds != nrows_aws')\n",
    "    logger.warning(\"Out of {} days to compare, issues are found on {} days\".format(\n",
    "        len(cnt_aws), len(time_mismatch)\n",
    "    ))\n",
    "    \n",
    "    return {\n",
    "        'Time Span UnMatch': len(time_mismatch) > 0,\n",
    "        'Time Span Variable': f'{pcds_dt} : {aws_dt}',\n",
    "        'Time UnMatch Details (PCDS : AWS)': get_detailed_mismatch(),\n",
    "        'time_excludes': get_time_excludes_sql() if len(time_mismatch) > 0 else {}\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Configuration paths (modify as needed) ---#\n",
    "OUTPUT_FOLDER = UPath('files/outputs/meta_analysis')\n",
    "CSV_OUTPUT = OUTPUT_FOLDER / 'meta_analysis_comparison.csv'\n",
    "JSON_OUTPUT = OUTPUT_FOLDER / 'meta_analysis_results.json'\n",
    "S3_UPLOAD_ENABLED = False\n",
    "S3_OUTPUT_PATH = 's3://your-bucket/meta_analysis/'\n",
    "\n",
    "# CSV column headers\n",
    "CSV_COLUMNS = [\n",
    "    'Consumer Loans Data Product',\n",
    "    'PCDS Table Details with DB Name',\n",
    "    'Tables delivered in AWS with DB Name',\n",
    "    'Status',\n",
    "    'Row UnMatch',\n",
    "    'Row UnMatch Details',\n",
    "    'Time Span UnMatch',\n",
    "    'Time Span Variable',\n",
    "    'Time UnMatch Details (PCDS : AWS)',\n",
    "    'Column Type UnMatch',\n",
    "    'Type UnMatch Details',\n",
    "    'PCDS Extra Columns',\n",
    "    'PCDS Unique Columns',\n",
    "    'AWS Extra Columns',\n",
    "    'AWS Unique Columns',\n",
    "    'Uncaptured Column Mappings',\n",
    "]\n",
    "\n",
    "# Column filtering for cleanup\n",
    "DROP_COLS = []  # Columns to remove from PCDS unique list\n",
    "ADD_COLS = []   # Columns to remove from AWS unique list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Main Comparison Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def main():\n    \"\"\"Main execution function for comparison and aggregation\"\"\"\n\n    #>>> Setup logging <<<#\n    OUTPUT_FOLDER.mkdir(parents=True, exist_ok=True)\n    log_file = OUTPUT_FOLDER / 'meta_analysis_3_compare.log'\n    logger.add(log_file, level='INFO', mode='w')\n\n    start_run()\n    logger.info('Starting Meta Analysis Step 3: Comparison and Aggregation')\n\n    #>>> Load summary files from Steps 1 and 2 <<<#\n    pcds_summary_path = OUTPUT_FOLDER / 'pcds_summary.json'\n    aws_summary_path = OUTPUT_FOLDER / 'aws_summary.json'\n\n    if not pcds_summary_path.exists():\n        logger.error(f\"PCDS summary not found: {pcds_summary_path}\")\n        logger.error(\"Please run Step 1 (meta_analysis_1_pcds.ipynb) first!\")\n        return\n\n    if not aws_summary_path.exists():\n        logger.error(f\"AWS summary not found: {aws_summary_path}\")\n        logger.error(\"Please run Step 2 (meta_analysis_2_aws.ipynb) first!\")\n        return\n\n    pcds_summary = IO.read_json(pcds_summary_path)\n    aws_summary = IO.read_json(aws_summary_path)\n\n    logger.info(f\"Loaded PCDS summary for {len(pcds_summary)} tables\")\n    logger.info(f\"Loaded AWS summary for {len(aws_summary)} tables\")\n\n    #>>> Load metadata from both steps <<<#\n    pcds_metadata = IO.read_json(OUTPUT_FOLDER / 'pcds_metadata.json')\n    aws_metadata = IO.read_json(OUTPUT_FOLDER / 'aws_metadata.json')\n\n    #>>> Initialize output structures <<<#\n    csv_rows = []\n    json_output = {}\n\n    #>>> Process each table <<<#\n    common_tables = set(pcds_summary.keys()) & set(aws_summary.keys())\n    logger.info(f\"Found {len(common_tables)} tables in both PCDS and AWS\")\n\n    for table_name in tqdm(sorted(common_tables), desc='Comparing tables'):\n        logger.info(f\"\\n>>> Processing {table_name}\")\n\n        pcds_info = pcds_summary[table_name]\n        aws_info = aws_summary[table_name]\n        pcds_meta = pcds_metadata.get(table_name, {})\n        aws_meta = aws_metadata.get(table_name, {})\n\n        #>>> Initialize result row <<<#\n        row_result = {\n            'Consumer Loans Data Product': pcds_meta.get('group', 'N/A'),\n            'PCDS Table Details with DB Name': table_name,\n            'Tables delivered in AWS with DB Name': pcds_meta.get('aws_tbl', 'N/A'),\n            'Status': pcds_info.get('status', 'Unknown'),\n            'Row UnMatch': False,\n            'Row UnMatch Details': '',\n            'Time Span UnMatch': False,\n            'Time Span Variable': f\"{pcds_meta.get('pcds_dt', 'N/A')} : {pcds_meta.get('aws_dt', 'N/A')}\",\n            'Time UnMatch Details (PCDS : AWS)': '',\n            'Column Type UnMatch': False,\n            'Type UnMatch Details': '',\n            'PCDS Extra Columns': False,\n            'PCDS Unique Columns': '',\n            'AWS Extra Columns': False,\n            'AWS Unique Columns': '',\n            'Uncaptured Column Mappings': '',\n        }\n\n        #>>> Load parquet files <<<#\n        pcds_col_file = UPath(pcds_info['column_file'])\n        aws_col_file = UPath(aws_info['column_file'])\n\n        if not pcds_col_file.exists() or not aws_col_file.exists():\n            logger.warning(f\"Missing column files for {table_name}\")\n            csv_rows.append(row_result)\n            continue\n\n        pcds_c = IO.read_dataframe(pcds_col_file)\n        aws_c = IO.read_dataframe(aws_col_file)\n\n        #>>> Perform metadata comparison <<<#\n        try:\n            tokenised_cols = pcds_meta.get('tokenised_cols', [])\n\n            # Build dict from metadata\n            pcds_t = {'column': pcds_c, 'row': pd.DataFrame([{'nrow': pcds_meta.get('pcds_nrows', 0)}])}\n            aws_t = {'column': aws_c, 'row': pd.DataFrame([{'nrow': aws_meta.get('aws_nrows', 0)}])}\n\n            meta_result = process_meta(pcds_t, aws_t, tokenised_cols)\n            row_result.update(meta_result)\n\n            #>>> Prepare data for next step <<<#\n            col_mapping_df = meta_result.pop('col_mapping', pd.DataFrame())\n            if not col_mapping_df.empty:\n                d = (\n                    col_mapping_df\n                    .drop(columns='type_match')\n                    .apply(lambda x: SEP.join(x.astype(str).tolist()), axis=0)\n                    .to_dict()\n                )\n                json_output[table_name] = {\n                    **pcds_meta,\n                    **aws_meta,\n                    'pcds_cols': d.get('column_name_pcds', ''),\n                    'pcds_types': d.get('data_type_pcds', ''),\n                    'aws_cols': d.get('column_name_aws', ''),\n                    'aws_types': d.get('data_type_aws', ''),\n                }\n        except Exception as e:\n            logger.error(f\"Error processing metadata for {table_name}: {e}\")\n            csv_rows.append(row_result)\n            continue\n\n        #>>> Compare date-wise row counts if available <<<#\n        if row_result['Row UnMatch']:\n            pcds_date_file = UPath(pcds_info['date_file'])\n            aws_date_file = UPath(aws_info['date_file'])\n\n            if pcds_date_file.exists() and aws_date_file.exists():\n                try:\n                    pcds_d = IO.read_dataframe(pcds_date_file)\n                    aws_d = IO.read_dataframe(aws_date_file)\n\n                    # Load SQL engine info\n                    pcds_engine = IO.read_json(UPath(pcds_info['meta_file']))['sql_engine']\n                    aws_engine = IO.read_json(UPath(aws_info['meta_file']))['sql_engine']\n\n                    date_result = process_date(pcds_d, aws_d, pcds_engine['dateraw'], aws_engine['dateraw'])\n                    row_result.update(date_result)\n\n                    # Add time exclusions to JSON output\n                    if table_name in json_output:\n                        json_output[table_name].update(date_result.get('time_excludes', {}))\n                except Exception as e:\n                    logger.error(f\"Error processing dates for {table_name}: {e}\")\n\n        #>>> Clean up column lists <<<#\n        row_result['PCDS Unique Columns'] = Misc.remove_items(\n            row_result['PCDS Unique Columns'], DROP_COLS\n        )\n        row_result['AWS Unique Columns'] = Misc.remove_items(\n            row_result['AWS Unique Columns'], ADD_COLS\n        )\n\n        csv_rows.append(row_result)\n        logger.info(f\"Finished processing {table_name}\")\n\n    #>>> Write CSV report <<<#\n    logger.info(f\"\\nWriting CSV report to {CSV_OUTPUT}\")\n    with open(CSV_OUTPUT, 'w', newline='') as fp:\n        writer = csv.DictWriter(fp, fieldnames=CSV_COLUMNS)\n        writer.writeheader()\n        writer.writerows(csv_rows)\n\n    logger.info(f\"CSV report written with {len(csv_rows)} rows\")\n\n    #>>> Write JSON output for next step <<<#\n    logger.info(f\"Writing JSON output to {JSON_OUTPUT}\")\n    IO.write_json(JSON_OUTPUT, json_output)\n    logger.info(f\"JSON output written with {len(json_output)} tables\")\n\n    #>>> Upload to S3 if enabled <<<#\n    if S3_UPLOAD_ENABLED:\n        logger.info(\"Uploading results to S3...\")\n        try:\n            S3.upload_multiple(\n                s3_url=S3_OUTPUT_PATH,\n                folder=OUTPUT_FOLDER,\n                prefix='meta_analysis'\n            )\n            logger.info(\"S3 upload completed\")\n        except Exception as e:\n            logger.error(f\"S3 upload failed: {e}\")\n\n    #>>> Summary statistics <<<#\n    logger.info(\"\\n\" + \"=\" * WIDTH)\n    logger.info(\"Summary Statistics:\")\n    logger.info(f\"  Total tables compared: {len(csv_rows)}\")\n\n    row_mismatches = sum(1 for r in csv_rows if r['Row UnMatch'])\n    type_mismatches = sum(1 for r in csv_rows if r['Column Type UnMatch'])\n    time_mismatches = sum(1 for r in csv_rows if r['Time Span UnMatch'])\n\n    logger.info(f\"  Tables with row count mismatches: {row_mismatches}\")\n    logger.info(f\"  Tables with type mismatches: {type_mismatches}\")\n    logger.info(f\"  Tables with date range mismatches: {time_mismatches}\")\n\n    end_run()\n    logger.info(\"Meta Analysis Step 3 completed successfully!\")\n\nif __name__ == '__main__':\n    main()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Analysis\n",
    "\n",
    "Uncomment the cell below to run the comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}