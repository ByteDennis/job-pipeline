{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Column Statistics Analysis - Step 1: PCDS Data Collection\n",
    "\n",
    "This notebook collects column-level statistics from the PCDS (Oracle) platform.\n",
    "It will save the results for use in step 3 (comparison)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "import re\nimport os\nimport csv\nimport json\nimport pickle\nimport shutil\nimport argparse\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport datetime as dt\nimport time\n\nfrom upath import UPath\nfrom loguru import logger\nfrom tqdm import tqdm\nfrom typing import Literal\nfrom dataclasses import dataclass, field, fields\n\nwarnings.filterwarnings('ignore', message=r'pandas only supports SQLAlchemy connectable .*', category=UserWarning)\n\n# Note: This notebook uses Parquet format for cross-platform compatibility\n# Install pyarrow if needed: pip install pyarrow or conda install -c conda-forge pyarrow\nimport pyarrow",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Constants and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# --- Global Constants ---\n",
    "SEP = '; '\n",
    "TODAY = dt.datetime.now()\n",
    "WIDTH = 80\n",
    "inWindows = os.name == 'nt'\n",
    "\n",
    "TPartition = Literal['whole', 'year', 'year_month', 'empty', 'year_week', 'week', 'snapshot']\n",
    "\n",
    "# --- SQL Template for PCDS Column Statistics ---\n",
    "PCDS_SQL_COLUMN = \"\"\"\n",
    "DECLARE\n",
    "    v_sql          VARCHAR2(20000);\n",
    "    v_col_name     VARCHAR2(128);\n",
    "    v_data_type    VARCHAR2(128);\n",
    "    v_table_name   VARCHAR2(128) := UPPER('{table}');\n",
    "\n",
    "    v_ret_data_type  VARCHAR2(128);\n",
    "    v_count          NUMBER;\n",
    "    v_distinct       NUMBER;\n",
    "    v_max            VARCHAR2(4000);\n",
    "    v_min            VARCHAR2(4000);\n",
    "    v_missing        NUMBER;\n",
    "    v_avg            NUMBER;\n",
    "    v_std            NUMBER;\n",
    "    v_sum            NUMBER;\n",
    "    v_sum_sq         NUMBER;\n",
    "    v_freq           VARCHAR2(4000);\n",
    "\n",
    "    l_column_ref VARCHAR2(256);\n",
    "\n",
    "BEGIN\n",
    "    FOR rec IN (\n",
    "        SELECT column_name, data_type\n",
    "        FROM all_tab_cols\n",
    "        WHERE table_name = v_table_name\n",
    "        ORDER BY column_id\n",
    "    ) LOOP\n",
    "        v_col_name := rec.column_name;\n",
    "        v_data_type := rec.data_type;\n",
    "\n",
    "        v_ret_data_type := NULL; v_count := NULL; v_distinct := NULL; \n",
    "        v_max := NULL; v_min := NULL; v_missing := NULL; \n",
    "        v_avg := NULL; v_std := NULL; v_sum := NULL; v_sum_sq := NULL; v_freq := NULL;\n",
    "        \n",
    "        IF v_data_type LIKE 'TIMESTAMP%' THEN \n",
    "            l_column_ref := 'TRUNC(' || v_col_name || ')';\n",
    "        ELSE\n",
    "            l_column_ref := v_col_name;\n",
    "        END IF;\n",
    "\n",
    "        IF v_data_type IN ('NUMBER', 'FLOAT', 'BINARY_FLOAT', 'BINARY_DOUBLE') THEN\n",
    "            v_sql := 'SELECT ';\n",
    "            v_sql := v_sql || '''' || v_data_type || ''' AS col_type, ';\n",
    "            v_sql := v_sql || 'COUNT(' || v_col_name || ') AS col_count, ';\n",
    "            v_sql := v_sql || 'COUNT(DISTINCT ' || v_col_name || ') AS col_distinct, ';\n",
    "            v_sql := v_sql || 'MAX(' || v_col_name || ') AS col_max, ';\n",
    "            v_sql := v_sql || 'MIN(' || v_col_name || ') AS col_min, ';\n",
    "            v_sql := v_sql || 'AVG(' || v_col_name || ') AS col_avg, ';\n",
    "            v_sql := v_sql || 'STDDEV_SAMP(' || v_col_name || ') AS col_std, ';\n",
    "            v_sql := v_sql || 'SUM(' || v_col_name || ') AS col_sum, ';\n",
    "            v_sql := v_sql || 'SUM(' || v_col_name || ' * ' || v_col_name || ') AS col_sum_sq, ';\n",
    "            v_sql := v_sql || 'COUNT(*) - COUNT(' || v_col_name || ') AS col_missing, ';\n",
    "            v_sql := v_sql || 'EMPTY_CLOB() AS col_freq ';\n",
    "            v_sql := v_sql || 'FROM ' || v_table_name || ' WHERE {limit} ';\n",
    "        ELSE\n",
    "            v_sql := 'WITH FreqTable_RAW AS ( ';\n",
    "            v_sql := v_sql || 'SELECT ' || l_column_ref || ' AS p_col, COUNT(*) AS value_freq '; \n",
    "            v_sql := v_sql || 'FROM ' || v_table_name || ' WHERE {limit} '; \n",
    "            v_sql := v_sql || 'GROUP BY ' || l_column_ref || ' '; \n",
    "            v_sql := v_sql || '), FreqTable AS ( '; \n",
    "            v_sql := v_sql || 'SELECT p_col, value_freq, '; \n",
    "            v_sql := v_sql || 'ROW_NUMBER() OVER (ORDER BY value_freq DESC, p_col ASC) AS rn '; \n",
    "            v_sql := v_sql || 'FROM FreqTable_RAW), AggStats AS ( ';\n",
    "            v_sql := v_sql || 'SELECT SUM(ft.value_freq) AS col_count, '; \n",
    "            v_sql := v_sql || 'COUNT(ft.value_freq) AS col_distinct, '; \n",
    "            v_sql := v_sql || 'MAX(ft.value_freq) AS col_max, '; \n",
    "            v_sql := v_sql || 'MIN(ft.value_freq) AS col_min, '; \n",
    "            v_sql := v_sql || 'AVG(ft.value_freq) AS col_avg, '; \n",
    "            v_sql := v_sql || 'STDDEV_SAMP(ft.value_freq) AS col_std, ';\n",
    "            v_sql := v_sql || 'SUM(ft.value_freq) AS col_sum, ';\n",
    "            v_sql := v_sql || 'SUM(ft.value_freq * ft.value_freq) AS col_sum_sq '; \n",
    "            v_sql := v_sql || 'FROM FreqTable ft) SELECT ';\n",
    "            v_sql := v_sql || '''' || v_data_type || ''' AS col_type, ast.*, '; \n",
    "            v_sql := v_sql || '(SELECT NVL(value_freq, 0) FROM FreqTable WHERE p_col IS NULL) AS col_missing, '; \n",
    "            v_sql := v_sql || '(SELECT LISTAGG(p_col || ''('' || value_freq || '')'', ''; '') WITHIN GROUP (ORDER BY value_freq DESC) FROM FreqTable WHERE rn <= 10) AS col_freq ';\n",
    "            v_sql := v_sql || 'FROM AggStats ast';\n",
    "        END IF;\n",
    "\n",
    "        EXECUTE IMMEDIATE v_sql INTO\n",
    "           v_ret_data_type, v_count, v_distinct, v_max, v_min, \n",
    "           v_avg, v_std, v_sum, v_sum_sq, v_missing, v_freq;\n",
    "\n",
    "        DBMS_OUTPUT.PUT_LINE('Column: ' || v_col_name);\n",
    "        DBMS_OUTPUT.PUT_LINE('  col_type: ' || v_ret_data_type); \n",
    "        DBMS_OUTPUT.PUT_LINE('  col_count: ' || v_count);\n",
    "        DBMS_OUTPUT.PUT_LINE('  col_distinct: ' || v_distinct);\n",
    "        DBMS_OUTPUT.PUT_LINE('  col_max: ' || v_max);\n",
    "        DBMS_OUTPUT.PUT_LINE('  col_min: ' || v_min);\n",
    "        DBMS_OUTPUT.PUT_LINE('  col_avg: ' || v_avg);\n",
    "        DBMS_OUTPUT.PUT_LINE('  col_std: ' || v_std);\n",
    "        DBMS_OUTPUT.PUT_LINE('  col_sum: ' || v_sum);\n",
    "        DBMS_OUTPUT.PUT_LINE('  col_sum_sq: ' || v_sum_sq);\n",
    "        DBMS_OUTPUT.PUT_LINE('  col_freq: ' || v_freq);\n",
    "        DBMS_OUTPUT.PUT_LINE('  col_missing: ' || v_missing);\n",
    "        DBMS_OUTPUT.PUT_LINE('---');\n",
    "    END LOOP;\n",
    "END;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Core Data Types and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class Timer:\n",
    "    \"\"\"Context manager for timing code execution\"\"\"\n",
    "    \n",
    "    def __enter__(self):\n",
    "        self.start = time.perf_counter()\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_value, exc_tb):\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def time(self):\n",
    "        return time.perf_counter() - self.start\n",
    "    \n",
    "    def pause(self):\n",
    "        \"\"\"Return elapsed time and reset timer\"\"\"\n",
    "        elapsed = self.time\n",
    "        self.start = time.perf_counter()\n",
    "        return elapsed\n",
    "\n",
    "    @staticmethod\n",
    "    def to_str(value):\n",
    "        \"\"\"Convert seconds to human-readable format\"\"\"\n",
    "        minutes, seconds = divmod(value, 60)\n",
    "        hours, minutes = divmod(minutes, 60)\n",
    "        return f'{hours} hours {minutes} minutes {seconds:.0f} seconds'\n",
    "\n",
    "@dataclass\n",
    "class MetaOut:\n",
    "    \"\"\"Metadata output structure\"\"\"\n",
    "    col2COL: dict\n",
    "    col2type: dict\n",
    "    infostr: str\n",
    "    rowvar: str\n",
    "    rowexclude: list\n",
    "    rowtype: str\n",
    "    nrows: int\n",
    "    where: str\n",
    "\n",
    "@dataclass(init=False)\n",
    "class MetaJSON:\n",
    "    \"\"\"Container for metadata from previous meta analysis step\"\"\"\n",
    "    pcds: MetaOut\n",
    "    aws: MetaOut\n",
    "    last_modified: str\n",
    "    partition: TPartition = 'whole'\n",
    "    tokenised_cols: list = field(default_factory=list)\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        field_names = [f.name for f in fields(self)]\n",
    "        for k, v in kwargs.items():\n",
    "            if k in field_names:\n",
    "                setattr(self, k, v)\n",
    "        \n",
    "        def col2col(a_str, b_str, sep=SEP):\n",
    "            return {k: v for k, v in zip(a_str.split(sep), b_str.split(sep))}\n",
    "        \n",
    "        for key, other in [('pcds', 'aws'), ('aws', 'pcds')]:\n",
    "            out = MetaOut(\n",
    "                rowvar=kwargs['%s_dt' % key],\n",
    "                infostr=kwargs['%s_tbl' % key],\n",
    "                where=kwargs['%s_where' % key],\n",
    "                nrows=kwargs['%s_nrows' % key],\n",
    "                col2COL=col2col(kwargs['%s_cols' % key], kwargs['%s_cols' % other]),\n",
    "                col2type=col2col(kwargs['%s_cols' % key], kwargs['%s_types' % key]),\n",
    "                rowtype=kwargs['%s_dt_type' % key],\n",
    "                rowexclude=kwargs['%s_exclude' % key]\n",
    "            )\n",
    "            setattr(self, key, out)\n",
    "\n",
    "@dataclass\n",
    "class CSMeta:\n",
    "    \"\"\"Metadata for column statistics comparison\"\"\"\n",
    "    pcds_table: str\n",
    "    aws_table: str\n",
    "    partition: TPartition\n",
    "    vintage: str\n",
    "    pcds_time: int\n",
    "    aws_time: int = 0\n",
    "\n",
    "    def todict(self):\n",
    "        return {f.name: getattr(self, f.name) for f in fields(self)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "def start_run():\n    logger.info('\\n\\n' + '=' * WIDTH)\n\ndef end_run():\n    logger.info('\\n\\n' + '=' * WIDTH)\n\ndef load_env(file):\n    if inWindows:\n        from dotenv import load_dotenv\n        load_dotenv(file)\n\nclass IO:\n    \"\"\"File I/O utilities - uses Parquet/JSON for cross-platform compatibility\"\"\"\n\n    @staticmethod\n    def write_dataframe(file, df):\n        \"\"\"Save DataFrame in portable Parquet format\"\"\"\n        file = UPath(file)\n        df.to_parquet(file, index=True, engine='pyarrow', compression='snappy')\n\n    @staticmethod\n    def read_dataframe(file):\n        \"\"\"Load DataFrame from Parquet format\"\"\"\n        file = UPath(file)\n        return pd.read_parquet(file, engine='pyarrow')\n\n    @staticmethod\n    def write_json(file, data, cls=None):\n        \"\"\"Save to JSON with proper serialization\"\"\"\n        import numpy as np\n        import pandas as pd\n        import datetime as dt\n\n        def convert(obj):\n            if isinstance(obj, (np.integer, np.floating)):\n                return obj.item()\n            elif isinstance(obj, np.ndarray):\n                return obj.tolist()\n            elif pd.isna(obj):\n                return None\n            elif isinstance(obj, (dt.datetime, dt.date)):\n                return obj.isoformat()\n            elif isinstance(obj, set):\n                return list(obj)\n            raise TypeError(f\"Object of type {type(obj)} is not JSON serializable\")\n\n        with open(file, 'w') as f:\n            json.dump(data, f, indent=2, default=convert, cls=cls)\n\n    @staticmethod\n    def read_json(file):\n        \"\"\"Load from JSON\"\"\"\n        with open(file, 'r') as f:\n            return json.load(f)\n\n    @staticmethod\n    def write_pickle(file, data):\n        \"\"\"Deprecated: Use write_dataframe or write_json instead\"\"\"\n        with open(file, 'wb') as f:\n            pickle.dump(data, f)\n\n    @staticmethod\n    def read_pickle(file):\n        \"\"\"Deprecated: Use read_dataframe or read_json instead\"\"\"\n        with open(file, 'rb') as f:\n            return pickle.load(f)\n\n    @staticmethod\n    def read_meta_json(json_file):\n        \"\"\"Read metadata JSON and convert to MetaJSON objects\"\"\"\n        data = IO.read_json(json_file)\n        return {k: MetaJSON(**v) for k, v in data.items()}\n\n    @staticmethod\n    def delete_file(file):\n        if (filepath := UPath(file)).exists():\n            filepath.unlink()",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Date Handling Functions for PCDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def get_iso_week_dates(year, week):\n",
    "    jan01, dec31 = dt.datetime(year, 1, 1), dt.datetime(year, 12, 31)\n",
    "    first_day = jan01 - dt.timedelta(days=jan01.weekday())\n",
    "    start = first_day + dt.timedelta(weeks=week - 1)\n",
    "    end = start + dt.timedelta(days=6)\n",
    "    start, end = max(start, jan01), min(end, dec31)\n",
    "    return start.strftime('%Y-%m-%d'), end.strftime('%Y-%m-%d')\n",
    "\n",
    "def parse_format_date(str_w_format):\n",
    "    pattern = r'^(.+?)(?:\\s*\\(([^)]+)\\))?$'\n",
    "    return re.match(pattern, str_w_format)\n",
    "\n",
    "def parse_exclude_date(exclude_clause):\n",
    "    \"\"\"Convert date exclusions to PCDS format\"\"\"\n",
    "    p1 = r\"TO_CHAR\\((?P<col>\\w+),\\s*'YYYY-MM-DD'\\)\\s+(?P<op>not in|in)\\s+\\((?P<dates>.*?)\\)\"\n",
    "    if m := re.match(p1, exclude_clause, flags=re.I):\n",
    "        col, op, dates = m.groups()\n",
    "        new_dates = ', '.join(f\"DATE {date.strip()}\" for date in dates.split(','))\n",
    "        return '%s %s (%s)' % (col, op, new_dates)\n",
    "    return exclude_clause\n",
    "\n",
    "def get_pcds_where(date_var, date_type, date_partition, date_range, date_format, snapshot=None, exclude_clauses=[]):\n",
    "    if date_type and ('char' in date_type.lower() or 'varchar' in date_type.lower()):\n",
    "        date_var = f\"TO_DATE({date_var}, '{date_format}')\"\n",
    "    \n",
    "    if snapshot:\n",
    "        return ' AND '.join(parse_exclude_date(x) for x in exclude_clauses if x)\n",
    "    elif date_partition == 'whole':\n",
    "        base_clause = \"1=1\"\n",
    "    elif date_partition == 'year':\n",
    "        start_dt = f\"TO_DATE('{date_range}-01-01', 'YYYY-MM-DD')\"\n",
    "        end_dt = f\"TO_DATE('{date_range}-12-31', 'YYYY-MM-DD')\"\n",
    "        base_clause = f\"{date_var} >= {start_dt} AND {date_var} <= {end_dt}\"\n",
    "    elif date_partition == 'year_month':\n",
    "        start_dt = f\"TO_DATE('{date_range}', 'YYYY-MM')\"\n",
    "        end_dt = f\"LAST_DAY(TO_DATE('{date_range}', 'YYYY-MM'))\"\n",
    "        base_clause = f\"{date_var} >= {start_dt} AND {date_var} <= {end_dt}\"\n",
    "    elif date_partition in ('year_week', 'week'):\n",
    "        year, week = date_range.split('-W')\n",
    "        start_dt, end_dt = get_iso_week_dates(int(year), int(week))\n",
    "        base_clause = f\"{date_var} >= DATE '{start_dt}' AND {date_var} <= DATE '{end_dt}'\"\n",
    "    elif date_partition == 'daily':\n",
    "        target_dt = f\"TO_DATE('{date_range}', 'YYYY-MM-DD')\"\n",
    "        base_clause = f\"{date_var} = {target_dt}\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported partition type: {date_partition}\")\n",
    "    \n",
    "    if (exclude_clauses := [x for x in exclude_clauses if x]):\n",
    "        exclude_clause = ' AND '.join(parse_exclude_date(x) for x in exclude_clauses if x)\n",
    "        return f\"({base_clause}) AND ({exclude_clause})\"\n",
    "    else:\n",
    "        return base_clause"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: PCDS SQL Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas.io.sql as psql\n",
    "\n",
    "class SQLengine:\n",
    "    \"\"\"SQL query engine for PCDS\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self._where = None\n",
    "        self._type = None\n",
    "        self._date = None\n",
    "        self._dateraw = None\n",
    "        self._table = None\n",
    "\n",
    "    def query(self, query, connection, **query_kwargs):\n",
    "        \"\"\"Execute SQL query and return DataFrame\"\"\"\n",
    "        df = psql.read_sql_query(query, connection, **query_kwargs)\n",
    "        df.columns = [x.upper() for x in df.columns]\n",
    "        return df\n",
    "\n",
    "    def execute_PCDS(self, query, service_name):\n",
    "        \"\"\"Execute PCDS PL/SQL block and parse DBMS_OUTPUT\"\"\"\n",
    "        from oracledb import STRING, NUMBER\n",
    "        import oracledb\n",
    "        \n",
    "        query_stmt = query\n",
    "        \n",
    "        # TODO: Implement your PCDS connection logic here\n",
    "        # with pcds_connect(service_name=service_name) as CONN:\n",
    "        #     cursor = CONN.cursor()\n",
    "        #     cursor.callproc(\"dbms_output.enable\", [None])\n",
    "        #     cursor.execute(query_stmt)\n",
    "        #     ...\n",
    "        raise NotImplementedError(\"Implement PCDS connection\")\n",
    "\n",
    "    def query_PCDS(self, query_stmt: str, service_name: str, **query_kwargs):\n",
    "        \"\"\"Execute query on PCDS\"\"\"\n",
    "        # TODO: Implement your PCDS connection logic here\n",
    "        # with pcds_connect(service_name=service_name) as CONN:\n",
    "        #     return self.query(query_stmt, CONN, **query_kwargs)\n",
    "        raise NotImplementedError(\"Implement PCDS connection\")\n",
    "\n",
    "proc_pcds = SQLengine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: PCDS Column Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class ColumnAnalyzer:\n",
    "    \"\"\"PCDS column analysis engine\"\"\"\n",
    "    \n",
    "    def run_pcds_column_analysis(self, info_str: str, where_clause: str = \"1=1\") -> pd.DataFrame:\n",
    "        \"\"\"Run comprehensive column statistics on PCDS table\"\"\"\n",
    "        try:\n",
    "            service, table_name = info_str.split('.')\n",
    "            where_clause = where_clause.replace(\"'\", \"''\")\n",
    "            sql_stmt = PCDS_SQL_COLUMN.format(table=table_name, limit=where_clause)\n",
    "            logger.info(f\"Executing PCDS analysis for {table_name}\")\n",
    "            result = proc_pcds.execute_PCDS(sql_stmt, service_name=service)\n",
    "            return pd.DataFrame(result).T\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in PCDS analysis: {e}\")\n",
    "            raise\n",
    "\n",
    "    def download_pcds_snapshot(self, info_str, columns, where_clause) -> pd.DataFrame:\n",
    "        \"\"\"Download PCDS table snapshot for backup\"\"\"\n",
    "        try:\n",
    "            service, table_name = info_str.split('.')\n",
    "            columns = ', '.join(columns)\n",
    "            sql_stmt = f\"SELECT {columns} FROM {table_name} WHERE {where_clause}\"\n",
    "            logger.info(f\"Downloading PCDS table {table_name}\")\n",
    "            return proc_pcds.query_PCDS(sql_stmt, service_name=service)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in PCDS pulling: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Main Execution - PCDS Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "def main_pcds():\n    \"\"\"Main execution function for PCDS column statistics collection\"\"\"\n\n    # Configuration - adjust these paths as needed\n    meta_json_path = 'path/to/meta_analysis_output.json'  # From meta_analysis step\n    meta_csv_path = 'path/to/meta_analysis.csv'\n    output_folder = UPath('output/column_stats_pcds')\n    output_folder.mkdir(exist_ok=True, parents=True)\n\n    start_run()\n\n    # Load metadata from previous meta analysis\n    meta_json = IO.read_meta_json(meta_json_path)\n    meta_csv = pd.read_csv(meta_csv_path)\n\n    CA = ColumnAnalyzer()\n    summary_data = {}\n\n    for i, row in tqdm(meta_csv.iterrows(), desc='Processing PCDS tables...', total=len(meta_csv)):\n        name = row.get('PCDS Table Details with DB Name')\n        logger.info(f\"Processing dataset: {name}\")\n\n        # Load metadata for this table\n        meta_info = meta_json.get(name)\n        if not meta_info:\n            continue\n\n        meta_pcds = meta_info.pcds\n        partition = meta_info.partition\n\n        if partition == 'empty':\n            continue\n\n        # TODO: Implement vintage determination logic\n        vintages = ['entire_dataset']  # Placeholder\n\n        # Clean table name for file naming\n        table_name = name.split('.')[-1].lower()\n\n        for vintage in vintages:\n            logger.info(f\"Processing vintage: {vintage}\")\n\n            # Build WHERE clause for this vintage\n            pcds_where = get_pcds_where(\n                date_var=meta_pcds.rowvar,\n                date_type=meta_pcds.rowtype,\n                date_partition=partition,\n                date_range=vintage,\n                date_format='YYYY-MM-DD',\n                snapshot=partition == 'snapshot',\n                exclude_clauses=[meta_pcds.where, meta_pcds.rowexclude]\n            )\n\n            # Compute column statistics\n            with Timer() as timer:\n                pcds_stats = CA.run_pcds_column_analysis(\n                    meta_pcds.infostr, pcds_where\n                )\n                pcds_time = timer.pause()\n\n            # Save individual parquet file for this table/vintage\n            stats_file = output_folder / f'pcds_stats_{table_name}_{vintage}.parquet'\n            IO.write_dataframe(stats_file, pcds_stats)\n            logger.info(f\"Saved stats to {stats_file}\")\n\n            # Save metadata as JSON\n            meta_data = CSMeta(\n                pcds_table=meta_pcds.infostr,\n                aws_table=meta_info.aws.infostr,\n                partition=partition,\n                vintage=vintage,\n                pcds_time=pcds_time,\n            ).todict()\n\n            meta_file = output_folder / f'pcds_meta_{table_name}_{vintage}.json'\n            IO.write_json(meta_file, {\n                'meta_data': meta_data,\n                'pcds_where': pcds_where\n            })\n            logger.info(f\"Saved metadata to {meta_file}\")\n\n            # Add to summary\n            if name not in summary_data:\n                summary_data[name] = {}\n            summary_data[name][vintage] = {\n                'stats_file': str(stats_file),\n                'meta_file': str(meta_file),\n                'table_name': table_name,\n                'meta_data': meta_data\n            }\n\n            logger.info(f\"Completed PCDS analysis for vintage {vintage}\")\n\n        proc_pcds.reset()\n\n    # Save summary file\n    summary_file = output_folder / 'pcds_summary.json'\n    IO.write_json(summary_file, summary_data)\n    logger.info(f\"Summary saved to {summary_file}\")\n    logger.info(f\"Processed {len(summary_data)} tables with {sum(len(v) for v in summary_data.values())} total vintages\")\n\n    end_run()\n    return summary_data\n\nif __name__ == '__main__':\n    results = main_pcds()",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Analysis\n",
    "\n",
    "Uncomment the cell below to run the PCDS data collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# results = main_pcds()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}