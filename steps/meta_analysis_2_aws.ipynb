{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Analysis Step 2: AWS Platform Processing\n",
    "\n",
    "This notebook processes AWS (Athena) database tables and extracts metadata.\n",
    "\n",
    "**Purpose:**\n",
    "- Connect to AWS Athena database (no credential renewal - uses IAM role)\n",
    "- Load PCDS metadata from Step 1\n",
    "- Extract AWS table metadata (columns, data types)\n",
    "- Process row counts and date ranges\n",
    "- Save results as pickle/JSON for Step 3\n",
    "\n",
    "**Inputs:**\n",
    "- `pcds_metadata.json` - Metadata from Step 1\n",
    "\n",
    "**Outputs:**\n",
    "- `aws_meta_results.pkl` - Complete metadata results\n",
    "- `aws_metadata.json` - Structured metadata for next steps\n",
    "\n",
    "**Note:** This runs on Linux/remote server with IAM role. No AWS credential renewal needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import re\nimport os\nimport csv\nimport json\nimport shutil\nimport pickle\nimport argparse\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport pyathena as pa\nimport pandas.io.sql as psql\n\nfrom upath import UPath\nfrom loguru import logger\nfrom tqdm import tqdm\nfrom datetime import datetime, timedelta, timezone\nfrom dataclasses import dataclass, field, fields, is_dataclass\nfrom configparser import ConfigParser\nfrom confection import Config\nfrom unittest import mock\nfrom enum import Enum\nfrom typing import Literal, Dict, List\nfrom collections import defaultdict, abc\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning, message='.*pandas only supports SQLAlchemy connectable.*')\n\n# Note: This notebook uses Parquet format for cross-platform compatibility\n# Install pyarrow if needed: pip install pyarrow or conda install -c conda-forge pyarrow\nimport pyarrow"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Constants and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Global Constants ---\n",
    "SEP = '; '\n",
    "AWS_DT_FORMAT = '%Y-%m-%d'\n",
    "TODAY = datetime.now()\n",
    "ONEDAY = timedelta(days=1)\n",
    "WIDTH = 80\n",
    "NO_DATE = 'no_date_provided'\n",
    "\n",
    "class PullStatus(Enum):\n",
    "    \"\"\"Enumeration for data pull status codes\"\"\"\n",
    "    NONEXIST_AWS = 'Nonexisting AWS Table'\n",
    "    NONDATE_AWS = 'Nonexisting Date Variable in AWS'\n",
    "    EMPTY_AWS = 'Empty AWS Table'\n",
    "    NO_MAPPING = 'Column Mapping Not Provided'\n",
    "    SUCCESS = 'Successful Data Access'\n",
    "\n",
    "# --- SQL Templates for AWS (Athena) ---\n",
    "AWS_SQL_META = \"\"\"\n",
    "select column_name, data_type from information_schema.columns\n",
    "where table_schema = LOWER('{db}') and table_name = LOWER('{table}')\n",
    "\"\"\"\n",
    "\n",
    "AWS_SQL_NROW = \"\"\"\n",
    "SELECT COUNT(*) AS nrow FROM {db}.{table}\n",
    "where {limit}\n",
    "\"\"\"\n",
    "\n",
    "AWS_SQL_DATE = \"\"\"\n",
    "SELECT {date}, count(*) AS nrows\n",
    "FROM {db}.{table} \n",
    "WHERE {limit}\n",
    "GROUP BY {date}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Exception Classes and Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Custom Exceptions ---\n",
    "class NONEXIST_TABLE(Exception):\n",
    "    \"\"\"Exception raised when database view does not exist\"\"\"\n",
    "    pass\n",
    "\n",
    "class NONEXIST_DATEVAR(Exception):\n",
    "    \"\"\"Exception raised when no date-like variable exists\"\"\"\n",
    "    pass\n",
    "\n",
    "# --- Helper Functions for Configuration Reading ---\n",
    "def read_str_lst(lst_str, sep='\\n'):\n",
    "    \"\"\"Parse newline-separated string into list\"\"\"\n",
    "    return [x for x in lst_str.strip().split(sep) if x]\n",
    "\n",
    "def read_dstr_lst(dct_str, sep='='):\n",
    "    \"\"\"Parse key=value pairs into dictionary\"\"\"\n",
    "    d = dict(line.split(sep, 1) for line in read_str_lst(dct_str))\n",
    "    return {k.strip(): v.strip() for k, v in d.items()}\n",
    "\n",
    "# --- Base Type Class ---\n",
    "class BaseType:\n",
    "    \"\"\"Base class with logging and nested dataclass support\"\"\"\n",
    "    def __post_init__(self):\n",
    "        for _field in fields(self):\n",
    "            if is_dataclass(_field.type):\n",
    "                field_val = _field.type(**getattr(self, _field.name))\n",
    "                setattr(self, _field.name, field_val)\n",
    "\n",
    "    def tolog(self, indent=1, padding=''):\n",
    "        \"\"\"Convert dataclass to formatted string for logging\"\"\"\n",
    "        import pprint as pp\n",
    "        def get_val(x, pad):\n",
    "            if isinstance(x, BaseType):\n",
    "                return x.tolog(indent, pad)\n",
    "            elif isinstance(x, Dict):\n",
    "                return pp.pformat(x, indent)\n",
    "            else:\n",
    "                return repr(x)\n",
    "        cls_name = self.__class__.__name__\n",
    "        padding = padding + '\\t' * indent\n",
    "        fields_str = [f'{padding}{k}={get_val(v, padding)}' for k, v in vars(self).items()]\n",
    "        return f'{cls_name}(\\n' + ',\\n'.join(fields_str) + '\\n)'\n",
    "\n",
    "# --- Configuration Dataclasses ---\n",
    "@dataclass\n",
    "class MetaRange:\n",
    "    \"\"\"Range configuration for row selection\"\"\"\n",
    "    start_rows: int | None\n",
    "    end_rows: int | None\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield from [self.start_rows or 1, self.end_rows or float('inf')]\n",
    "\n",
    "@dataclass\n",
    "class MetaInput(BaseType):\n",
    "    \"\"\"Input configuration\"\"\"\n",
    "    name: str\n",
    "    step: str\n",
    "    env: str\n",
    "    range: MetaRange\n",
    "    category: Literal['loan', 'dpst']\n",
    "    clear_cache: bool = True\n",
    "\n",
    "@dataclass\n",
    "class MetaCSV:\n",
    "    \"\"\"CSV output configuration\"\"\"\n",
    "    file: UPath\n",
    "    columns: str\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.columns = read_str_lst(self.columns)\n",
    "\n",
    "@dataclass\n",
    "class S3Config:\n",
    "    \"\"\"S3 path configuration\"\"\"\n",
    "    run: UPath\n",
    "    data: UPath\n",
    "\n",
    "@dataclass\n",
    "class LogConfig:\n",
    "    \"\"\"Logging configuration\"\"\"\n",
    "    level: Literal['info', 'warning', 'debug', 'error']\n",
    "    format: str\n",
    "    file: str\n",
    "    overwrite: bool\n",
    "\n",
    "    def todict(self):\n",
    "        return {\n",
    "            'level': self.level.upper(),\n",
    "            'format': self.format,\n",
    "            'sink': self.file,\n",
    "            'mode': 'w' if self.overwrite else 'a'\n",
    "        }\n",
    "\n",
    "@dataclass\n",
    "class NextConfig:\n",
    "    \"\"\"Next step configuration\"\"\"\n",
    "    file: UPath\n",
    "    fields: str\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.fields = read_dstr_lst(self.fields)\n",
    "\n",
    "@dataclass\n",
    "class CacheConfig:\n",
    "    \"\"\"Cache configuration (not used in this notebook)\"\"\"\n",
    "    enable: bool\n",
    "    directory: UPath\n",
    "    expire_hours: int = None\n",
    "    force_restart: bool = False\n",
    "    verbose: bool = False\n",
    "\n",
    "@dataclass\n",
    "class MetaOutput(BaseType):\n",
    "    \"\"\"Output configuration\"\"\"\n",
    "    folder: UPath\n",
    "    to_pkl: UPath\n",
    "    csv: MetaCSV\n",
    "    to_s3: S3Config\n",
    "    log: LogConfig\n",
    "    next: NextConfig\n",
    "    cache: CacheConfig\n",
    "\n",
    "@dataclass\n",
    "class MetaConfig(BaseType):\n",
    "    \"\"\"Main configuration class\"\"\"\n",
    "    input: MetaInput\n",
    "    output: MetaOutput\n",
    "\n",
    "@dataclass\n",
    "class MetaRecord:\n",
    "    \"\"\"Record tracking during processing\"\"\"\n",
    "    next_d: dict = field(default_factory=dict)\n",
    "    pull_status: PullStatus = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Configuration Reading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Patch confection library to preserve case sensitivity ---#\n",
    "def patch_confection():\n",
    "    def get_configparser(interpolate: bool = True):\n",
    "        from confection import CustomInterpolation\n",
    "        config = ConfigParser(\n",
    "            interpolation=CustomInterpolation() if interpolate else None,\n",
    "            allow_no_value=True,\n",
    "        )\n",
    "        config.optionxform = str\n",
    "        return config\n",
    "    mock_obj = mock.patch('confection.get_configparser', wraps=get_configparser)\n",
    "    if not hasattr(mock_obj, 'is_local'):\n",
    "        mock_obj.start()\n",
    "\n",
    "#--- Read configuration file and create config object ---#\n",
    "def read_config(config_class: BaseType, config_path: None | UPath | str = None, overrides={}):\n",
    "    patch_confection()\n",
    "    if UPath(config_path).is_file():\n",
    "        config = Config().from_disk(config_path, overrides=overrides)\n",
    "    else:\n",
    "        config = Config().from_str(config_path, overrides=overrides)\n",
    "    return config_class(**{**config.pop('root', {}), **config})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Utility Classes and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#--- Start logging session with separator ---#\ndef start_run():\n    logger.info('\\n\\n' + '=' * WIDTH)\n\n#--- End logging session with separator ---#\ndef end_run():\n    logger.info('\\n\\n' + '=' * WIDTH)\n\nclass IO:\n    \"\"\"File I/O utility class - uses Parquet/JSON for cross-platform compatibility\"\"\"\n\n    @staticmethod\n    def write_dataframe(file, df):\n        \"\"\"Save DataFrame in portable Parquet format\"\"\"\n        file = UPath(file)\n        df.to_parquet(file, index=True, engine='pyarrow', compression='snappy')\n\n    @staticmethod\n    def read_dataframe(file):\n        \"\"\"Load DataFrame from Parquet format\"\"\"\n        file = UPath(file)\n        return pd.read_parquet(file, engine='pyarrow')\n\n    @staticmethod\n    def write_json(file, data, cls=None):\n        \"\"\"Save to JSON with proper serialization\"\"\"\n        import numpy as np\n        import pandas as pd\n        import datetime as dt\n\n        def convert(obj):\n            if isinstance(obj, (np.integer, np.floating)):\n                return obj.item()\n            elif isinstance(obj, np.ndarray):\n                return obj.tolist()\n            elif pd.isna(obj):\n                return None\n            elif isinstance(obj, (dt.datetime, dt.date)):\n                return obj.isoformat()\n            elif isinstance(obj, set):\n                return list(obj)\n            raise TypeError(f\"Object of type {type(obj)} is not JSON serializable\")\n\n        with open(file, 'w') as f:\n            json.dump(data, f, indent=2, default=convert, cls=cls)\n\n    @staticmethod\n    def read_json(file):\n        \"\"\"Read JSON file into dictionary\"\"\"\n        with open(file, 'r') as fp:\n            data = json.load(fp)\n        return data\n\n    @staticmethod\n    def write_pickle(file, data):\n        \"\"\"Deprecated: Use write_dataframe or write_json instead\"\"\"\n        with open(file, 'wb') as f:\n            pickle.dump(data, f)\n\n    @staticmethod\n    def read_pickle(file):\n        \"\"\"Deprecated: Use read_dataframe or read_json instead\"\"\"\n        with open(file, 'rb') as fp:\n            data = pickle.load(fp)\n        return data\n\n    @staticmethod\n    def delete_file(file):\n        \"\"\"Delete file if it exists\"\"\"\n        if (filepath := UPath(file)).exists():\n            filepath.unlink()\n\nclass UDict(dict):\n    \"\"\"Case-insensitive dictionary for flexible key matching\"\"\"\n\n    def __getitem__(self, key):\n        return super().__getitem__(self._match(key))\n\n    def __contains__(self, key):\n        try:\n            self._match(key)\n            return True\n        except KeyError:\n            return False\n\n    def _match(self, key):\n        \"\"\"Find matching key regardless of case\"\"\"\n        for k in self:\n            if k.lower() == key.lower():\n                return k\n        raise KeyError(key)\n\n    def update(self, other=None, **kwargs):\n        if other is not None:\n            for k, v in other.items() if isinstance(other, abc.Mapping) else other:\n                self[k] = v\n        for k, v in kwargs.items():\n            assert self._match(k)\n            self[k] = v\n\n    def get(self, key, default_value=None):\n        try:\n            return self[key]\n        except KeyError:\n            return default_value\n\nclass Misc:\n    \"\"\"Miscellaneous utility functions\"\"\"\n\n    @staticmethod\n    def convert2int(a):\n        \"\"\"Safely convert value to integer\"\"\"\n        try:\n            return int(a)\n        except (TypeError, ValueError):\n            return None\n\n    @staticmethod\n    def convert2datestr(a):\n        \"\"\"Convert datetime to string format\"\"\"\n        if isinstance(a, datetime):\n            return a.strftime('%Y-%m-%d')\n        return a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: AWS Database Connection and SQL Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQLengine:\n",
    "    \"\"\"SQL query engine for AWS Athena database\"\"\"\n",
    "    \n",
    "    def __init__(self, platform: Literal['AWS']):\n",
    "        self._platform = platform\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset internal state\"\"\"\n",
    "        self._where = None\n",
    "        self._type = None\n",
    "        self._date = None\n",
    "        self._dateraw = None\n",
    "        self._table = None\n",
    "        self._format = AWS_DT_FORMAT\n",
    "\n",
    "    def extract_var(self, stmt):\n",
    "        \"\"\"Extract variable names from SQL date expression\"\"\"\n",
    "        def _extract_var():\n",
    "            word, time, tagt = r'\\w+_\\w+', r\"'[^']*'\", r'[^,]+'\n",
    "            pattern1 = fr\"{word}\\({word}\\(({tagt}),\\s*{time}\\),\\s*{time}\\)\"\n",
    "            pattern2 = fr\"{word}\\(({tagt}),\\s*{time}\\)\"\n",
    "            if (m := re.match(pattern1, stmt)):\n",
    "                return stmt, m.group(1)\n",
    "            elif (m := re.match(pattern2, stmt)):\n",
    "                return stmt, m.group(1)\n",
    "            return stmt, stmt\n",
    "        \n",
    "        date_var, date_raw = _extract_var()\n",
    "        return date_var, date_raw.lower()\n",
    "\n",
    "    def query(self, query, connection, **query_kwargs):\n",
    "        \"\"\"Execute SQL query and return DataFrame\"\"\"\n",
    "        query = self.clean_query(query)\n",
    "        df = psql.read_sql_query(query, connection, **query_kwargs)\n",
    "        \n",
    "        #>>> Normalize column names to lowercase <<<#\n",
    "        df.columns = [x.lower() for x in df.columns]\n",
    "        return df\n",
    "\n",
    "    def clean_query(self, query: str):\n",
    "        \"\"\"Clean and prepare SQL query for execution\"\"\"\n",
    "        #>>> Extract table name from query <<<#\n",
    "        table_pattern = r'([\\w.]+)\\s+MORF\\b'\n",
    "        self._table = re.search(table_pattern, query[::-1], flags=re.I).group(1)[::-1]\n",
    "        \n",
    "        #>>> Add alias to date column if needed <<<#\n",
    "        date_pattern = r'(?!\\\\s+(?:AS\\s+)\\w+)'\n",
    "        if self._date and (match := re.search(\n",
    "            re.escape(self._date) + date_pattern,\n",
    "            re.split(r'\\b(?:FROM|WHERE)\\b', query, flags=re.I)[0],\n",
    "            flags=re.I\n",
    "        )):\n",
    "            st, ed = match.span()\n",
    "            query = query[:st] + f'{self._date} as {self._dateraw}' + query[ed:]\n",
    "        \n",
    "        #>>> Remove empty WHERE clauses <<<#\n",
    "        where_pattern = r'^\\s*where\\s*$'\n",
    "        return re.sub(where_pattern, '', query, flags=re.I | re.M)\n",
    "\n",
    "    def get_where_sql(self, date_var: str, date_type: str, start_dt=None, end_dt=None, where_cstr='') -> str:\n",
    "        \"\"\"Build WHERE clause for date filtering\"\"\"\n",
    "        self._type = date_type\n",
    "        \n",
    "        #>>> Handle subquery in where constraint <<<#\n",
    "        if not pd.isna(where_cstr) and (m := re.search(r'(?<=\\()select.*(?=\\))', where_cstr)):\n",
    "            rhs = self.query_AWS(m.group()).iloc[0, 0]\n",
    "            if isinstance(rhs, str):\n",
    "                where_cstr = \"%s '%s'\" % (where_cstr[:m.start() - 1], rhs)\n",
    "            else:\n",
    "                where_cstr = \"%s '%s'\" % (where_cstr[:m.start() - 1], rhs.strftime('%Y-%m-%d'))\n",
    "        \n",
    "        where_sql = [where_cstr]\n",
    "        self.get_date_sql(date_var, date_type)\n",
    "        \n",
    "        #>>> Add date range filters <<<#\n",
    "        if not pd.isna(start_dt):\n",
    "            start_dt = Misc.convert2datestr(start_dt)\n",
    "            where_sql.append(f\"{self._date} >= '{start_dt}'\")\n",
    "        if not pd.isna(end_dt):\n",
    "            end_dt = Misc.convert2datestr(end_dt)\n",
    "            where_sql.append(f\"{self._date} <= '{end_dt}'\")\n",
    "        \n",
    "        self._where = ' AND '.join(x for x in where_sql if not pd.isna(x))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_date_format(date_var):\n",
    "        \"\"\"Extract date format from variable specification\"\"\"\n",
    "        pattern = r'^(.+?)(?:\\s*\\(([^)]+)\\))?$'\n",
    "        date_var, date_format = re.match(pattern, date_var).groups()\n",
    "        return date_var, date_format\n",
    "\n",
    "    def get_date_sql(self, date_var: str, date_type: str):\n",
    "        \"\"\"Convert date column to standard format in SQL\"\"\"\n",
    "        date_var, date_format = self.get_date_format(date_var)\n",
    "        is_date = re.search(r'time|date', date_type, re.IGNORECASE)\n",
    "        \n",
    "        #>>> Parse string dates if format provided <<<#\n",
    "        if date_format and (not is_date):\n",
    "            date_var = f\"DATE_PARSE({date_var}, '{date_format}')\"\n",
    "            is_date = True\n",
    "        \n",
    "        #>>> Convert to standard string format <<<#\n",
    "        if is_date:\n",
    "            date_var = f\"DATE_FORMAT({date_var}, '%Y-%m-%d')\"\n",
    "        \n",
    "        self._date, self._dateraw = self.extract_var(date_var)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'SQL({self._platform})\\n' \\\n",
    "               f'   table: {self._table}\\n' \\\n",
    "               f'   where: {self._where}\\n' \\\n",
    "               f'   date : {self._date} ({self._dateraw})'\n",
    "\n",
    "    def query_AWS(self, query_stmt: str, **query_kwargs):\n",
    "        \"\"\"Execute query on AWS Athena (no credential renewal - uses IAM role)\"\"\"\n",
    "        CONN = pa.connect(\n",
    "            s3_staging_dir=\"s3://355538383407-us-east-1-athena-output/uscb-analytics/\",\n",
    "            region_name=\"us-east-1\",\n",
    "        )\n",
    "        return self.query(query_stmt, CONN, **query_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: AWS Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize global objects\n",
    "proc_aws = SQLengine('AWS')\n",
    "record = MetaRecord()\n",
    "\n",
    "#--- Process AWS table metadata (columns and row count) ---#\n",
    "def process_aws_meta(row):\n",
    "    database, table = (info_str := row['aws_tbl']).split('.', maxsplit=1)\n",
    "    CONN = pa.connect(\n",
    "        s3_staging_dir=\"s3://355538383407-us-east-1-athena-output/uscb-analytics/\",\n",
    "        region_name=\"us-east-1\",\n",
    "    )\n",
    "    logger.info(f\"\\tStart processing {info_str}\")\n",
    "    \n",
    "    #>>> Query column metadata and row counts <<<#\n",
    "    try:\n",
    "        df_type = proc_aws.query(AWS_SQL_META.format(table=table, db=database), CONN)\n",
    "        date_var = re.match(r'(\\w+)(?=\\s*\\()?', row['aws_dt']).group(1)\n",
    "        if date_var == NO_DATE:\n",
    "            proc_aws._where = row.get('aws_where')\n",
    "        else:\n",
    "            proc_aws.get_where_sql(\n",
    "                date_var=row['aws_dt'],\n",
    "                date_type=df_type.query(f\"column_name == '{date_var.lower()}'\")['data_type'].item(),\n",
    "                start_dt=row.get('start_dt'),\n",
    "                end_dt=row.get('end_dt'),\n",
    "                where_cstr=row.get('aws_where')\n",
    "            )\n",
    "        nrow_sql = AWS_SQL_NROW.format(table=table, db=database, limit=proc_aws._where)\n",
    "        df_nrow = proc_aws.query(nrow_sql, CONN)\n",
    "    except pd.errors.DatabaseError:\n",
    "        logger.warning(f\"Couldn't find {table.lower()} in {database.lower()}\")\n",
    "        raise NONEXIST_TABLE(\"AWS View Not Existing\")\n",
    "    \n",
    "    df_type.columns = [x.lower() for x in df_type.columns]\n",
    "    return {'column': df_type, 'row': df_nrow}\n",
    "\n",
    "#--- Query AWS table for date-wise row counts ---#\n",
    "def process_aws_date(row):\n",
    "    database, table = (info_str := row['aws_tbl']).split('.', maxsplit=1)\n",
    "    CONN = pa.connect(\n",
    "        s3_staging_dir=\"s3://355538383407-us-east-1-athena-output/uscb-analytics/\",\n",
    "        region_name=\"us-east-1\",\n",
    "    )\n",
    "    try:\n",
    "        date_sql = AWS_SQL_DATE.format(\n",
    "            table=table, limit=proc_aws._where, date=proc_aws._date, db=database\n",
    "        )\n",
    "        df_meta = proc_aws.query(date_sql, CONN)\n",
    "        logger.info(f\"\\tFinish Processing {info_str}\")\n",
    "    except pd.errors.DatabaseError:\n",
    "        if proc_aws._dateraw:\n",
    "            logger.warning(f\"Column {proc_aws._dateraw.upper()} not found in {table.upper()}\")\n",
    "        raise NONEXIST_DATEVAR(\"Date-like Variable Not In AWS\")\n",
    "    \n",
    "    df_meta.columns = [x.lower() for x in df_meta.columns]\n",
    "    return df_meta\n",
    "\n",
    "#--- Initialize output folders and logging ---#\n",
    "def start_setup(start_row, C_out):\n",
    "    try:\n",
    "        assert start_row <= 1\n",
    "        os.remove(C_out.csv.file)\n",
    "    except (TypeError, AssertionError, FileNotFoundError):\n",
    "        pass\n",
    "    os.makedirs(C_out.folder, exist_ok=True)\n",
    "    logger.add(**C_out.log.todict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Configuration Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Parse command line arguments and load configuration ---#\n",
    "def parse_config():\n",
    "    parser = argparse.ArgumentParser(description='Conduct Meta Info Analysis - AWS Step')\n",
    "    parser.add_argument(\n",
    "        '--category',\n",
    "        choices=['loan', 'dpst'],\n",
    "        default='dpst',\n",
    "        help='which meta template to use',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--name', type=str,\n",
    "        default='test_0827',\n",
    "        help='how to name this analysis (override)'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--query', type=str,\n",
    "        default='group == \"test_0827\"',\n",
    "        help='how to name this analysis (override)'\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.category == 'dpst':\n",
    "        config_path = r'files/inputs/config_meta_dpst.cfg'\n",
    "    elif args.category == 'loan':\n",
    "        config_path = r'files/inputs/config_meta_loan.cfg'\n",
    "    \n",
    "    config = read_config(\n",
    "        MetaConfig,\n",
    "        config_path=config_path,\n",
    "        overrides={\n",
    "            'input.table.select_rows': args.query,\n",
    "            'input.name': args.name\n",
    "        }\n",
    "    )\n",
    "    (out_folder := UPath(config.output.folder)).mkdir(exist_ok=True)\n",
    "    shutil.copy(config_path, out_folder.joinpath(f'{config.input.step}_aws.cfg'))\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Main Execution - AWS Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def main():\n    \"\"\"Main execution function for AWS meta analysis\"\"\"\n    config = parse_config()\n    df_dict, df_next = {}, {}\n    C_out, C_in = config.output, config.input\n    start_row, end_row = C_in.range\n    start_setup(start_row, C_out)\n    logger.info('Configuration:\\n' + config.tolog())\n\n    start_run()\n\n    #>>> Load PCDS metadata from Step 1 <<<#\n    output_folder = UPath(C_out.folder)\n    pcds_metadata_file = output_folder / 'pcds_metadata.json'\n\n    if not pcds_metadata_file.exists():\n        logger.error(f\"PCDS metadata file not found: {pcds_metadata_file}\")\n        logger.error(\"Please run Step 1 (meta_analysis_1_pcds.ipynb) first!\")\n        return\n\n    pcds_metadata = IO.read_json(pcds_metadata_file)\n    logger.info(f\"Loaded PCDS metadata for {len(pcds_metadata)} tables\")\n\n    #>>> Process each table <<<#\n    total = len(pcds_metadata)\n    for i, (name, pcds_meta) in enumerate(tqdm(\n        pcds_metadata.items(), desc='Processing AWS ...', total=total\n    ), start=1):\n        if (i < start_row or i > end_row):\n            continue\n\n        aws_m, aws_d = {}, None\n        record.next_d = UDict(pcds_meta)\n\n        logger.info(f\">>> Start {name}\")\n\n        pull_status = PullStatus.SUCCESS\n\n        #>>> Try to pull AWS table metadata <<<#\n        try:\n            aws_m = process_aws_meta(pcds_meta)\n        except NONEXIST_TABLE:\n            pull_status = PullStatus.NONEXIST_AWS\n            logger.error(f\"AWS table {name} does not exist\")\n            continue\n\n        #>>> Try to get date-wise counts from AWS <<<#\n        try:\n            aws_d = process_aws_date(pcds_meta)\n            if len(aws_m) == 0:\n                pull_status = PullStatus.EMPTY_AWS\n        except NONEXIST_DATEVAR:\n            if pull_status == PullStatus.SUCCESS:\n                pull_status = PullStatus.NONDATE_AWS\n\n        #>>> Store results <<<#\n        df_dict[name] = {\n            'aws_meta': aws_m,\n            'aws_date': aws_d,\n            'status': pull_status.value,\n            'sql_engine': {\n                'where': proc_aws._where,\n                'date': proc_aws._date,\n                'dateraw': proc_aws._dateraw,\n                'type': proc_aws._type\n            }\n        }\n\n        #>>> Save metadata for next step <<<#\n        if aws_m and 'column' in aws_m:\n            record.next_d.update(\n                aws_cols=SEP.join(aws_m['column']['column_name'].tolist()),\n                aws_types=SEP.join(aws_m['column']['data_type'].tolist()),\n                aws_nrows=int(aws_m['row'].iloc[0, 0]) if 'row' in aws_m else 0,\n                aws_where=proc_aws._where,\n                aws_dt_type=proc_aws._type\n            )\n\n        df_next[name] = record.next_d.copy()\n\n        #>>> Reset engine for next iteration <<<#\n        proc_aws.reset()\n\n    #>>> Save results using Parquet/JSON format <<<#\n    # Save individual parquet files for each table\n    for table_name, table_data in df_dict.items():\n        if table_data['aws_meta'] and 'column' in table_data['aws_meta']:\n            # Save column info as parquet\n            col_file = output_folder / f'aws_column_info_{table_name}.parquet'\n            IO.write_dataframe(col_file, table_data['aws_meta']['column'])\n\n        if table_data['aws_date'] is not None:\n            # Save date counts as parquet\n            date_file = output_folder / f'aws_date_counts_{table_name}.parquet'\n            IO.write_dataframe(date_file, table_data['aws_date'])\n\n        # Save other metadata as JSON\n        meta_file = output_folder / f'aws_metadata_{table_name}.json'\n        IO.write_json(meta_file, {\n            'status': table_data['status'],\n            'sql_engine': table_data['sql_engine'],\n            'nrows': table_data['aws_meta'].get('row', pd.DataFrame()).to_dict() if table_data['aws_meta'] else {}\n        })\n\n    # Save summary with file paths\n    summary_data = {\n        name: {\n            'column_file': str(output_folder / f'aws_column_info_{name}.parquet'),\n            'date_file': str(output_folder / f'aws_date_counts_{name}.parquet'),\n            'meta_file': str(output_folder / f'aws_metadata_{name}.json'),\n            'status': data['status']\n        } for name, data in df_dict.items()\n    }\n    summary_file = output_folder / 'aws_summary.json'\n    IO.write_json(summary_file, summary_data)\n\n    # Save next step metadata (merged PCDS+AWS)\n    IO.write_json(output_folder / 'aws_metadata.json', df_next)\n\n    logger.info(f\"Saved AWS results to {output_folder}\")\n    logger.info(f\"  - Individual parquet/JSON files: {len(df_dict)} tables\")\n    logger.info(f\"  - aws_summary.json: summary of all files\")\n    logger.info(f\"  - aws_metadata.json: metadata for next steps\")\n\n    end_run()\n\nif __name__ == '__main__':\n    main()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Analysis\n",
    "\n",
    "Uncomment the cell below to run the AWS processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}