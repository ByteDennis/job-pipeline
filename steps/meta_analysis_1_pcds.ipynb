{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Analysis Step 1: PCDS Platform Processing\n",
    "\n",
    "This notebook processes PCDS (Oracle) database tables and extracts metadata.\n",
    "\n",
    "**Purpose:**\n",
    "- Connect to PCDS Oracle database\n",
    "- Extract table metadata (columns, data types)\n",
    "- Process row counts and date ranges\n",
    "- Save results as pickle/JSON for Step 3\n",
    "\n",
    "**Outputs:**\n",
    "- `pcds_meta_results.pkl` - Complete metadata results\n",
    "- `pcds_metadata.json` - Structured metadata for next steps\n",
    "\n",
    "**Note:** This runs on Windows with PCDS access. No AWS credential renewal needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import re\nimport os\nimport csv\nimport json\nimport shutil\nimport pickle\nimport argparse\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport pandas.io.sql as psql\n\nfrom upath import UPath\nfrom loguru import logger\nfrom tqdm import tqdm\nfrom datetime import datetime, timedelta, timezone\nfrom dataclasses import dataclass, field, fields, is_dataclass\nfrom configparser import ConfigParser\nfrom confection import Config\nfrom unittest import mock\nfrom enum import Enum\nfrom typing import Literal, Dict, List\nfrom collections import defaultdict, abc\nfrom dotenv import load_dotenv\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning, message='.*pandas only supports SQLAlchemy connectable.*')\n\n# Note: This notebook uses Parquet format for cross-platform compatibility\n# Install pyarrow if needed: pip install pyarrow or conda install -c conda-forge pyarrow\nimport pyarrow"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Constants and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Global Constants ---\n",
    "SEP = '; '\n",
    "PCDS_DT_FORMAT = 'YYYY-MM-DD'\n",
    "TODAY = datetime.now()\n",
    "ONEDAY = timedelta(days=1)\n",
    "WIDTH = 80\n",
    "NO_DATE = 'no_date_provided'\n",
    "inWindows = os.name == 'nt'\n",
    "\n",
    "class PullStatus(Enum):\n",
    "    \"\"\"Enumeration for data pull status codes\"\"\"\n",
    "    NONEXIST_PCDS = 'Nonexisting PCDS Table'\n",
    "    NONDATE_PCDS = 'Nonexisting Date Variable in PCDS'\n",
    "    EMPTY_PCDS = 'Empty PCDS Table'\n",
    "    NO_MAPPING = 'Column Mapping Not Provided'\n",
    "    SUCCESS = 'Successful Data Access'\n",
    "\n",
    "# --- SQL Templates for PCDS (Oracle) ---\n",
    "PCDS_SQL_META = \"\"\"\n",
    "select\n",
    "    column_name,\n",
    "    data_type || case\n",
    "    when data_type = 'NUMBER' then \n",
    "        case when data_precision is NULL AND data_scale is NULL\n",
    "            then NULL\n",
    "        else\n",
    "            '(' || TO_CHAR(data_precision) || ',' || TO_CHAR(data_scale) || ')'\n",
    "        end\n",
    "    when data_type LIKE '%CHAR%'\n",
    "        then\n",
    "            '(' || TO_CHAR(data_length) || ')'\n",
    "        else NULL\n",
    "    end AS data_type\n",
    "from all_tab_cols\n",
    "where table_name = UPPER('{table}')\n",
    "order by column_id\n",
    "\"\"\"\n",
    "\n",
    "PCDS_SQL_NROW = \"\"\"\n",
    "SELECT COUNT(*) AS nrow FROM {table}\n",
    "where {limit}\n",
    "\"\"\"\n",
    "\n",
    "PCDS_SQL_DATE = \"\"\"\n",
    "SELECT {date}, count(*) AS NROWS\n",
    "FROM {table} \n",
    "WHERE {limit}\n",
    "GROUP BY {date}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Exception Classes and Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Custom Exceptions ---\n",
    "class NONEXIST_TABLE(Exception):\n",
    "    \"\"\"Exception raised when database view does not exist\"\"\"\n",
    "    pass\n",
    "\n",
    "class NONEXIST_DATEVAR(Exception):\n",
    "    \"\"\"Exception raised when no date-like variable exists\"\"\"\n",
    "    pass\n",
    "\n",
    "# --- Helper Functions for Configuration Reading ---\n",
    "def read_str_lst(lst_str, sep='\\n'):\n",
    "    \"\"\"Parse newline-separated string into list\"\"\"\n",
    "    return [x for x in lst_str.strip().split(sep) if x]\n",
    "\n",
    "def read_dstr_lst(dct_str, sep='='):\n",
    "    \"\"\"Parse key=value pairs into dictionary\"\"\"\n",
    "    d = dict(line.split(sep, 1) for line in read_str_lst(dct_str))\n",
    "    return {k.strip(): v.strip() for k, v in d.items()}\n",
    "\n",
    "# --- Base Type Class ---\n",
    "class BaseType:\n",
    "    \"\"\"Base class with logging and nested dataclass support\"\"\"\n",
    "    def __post_init__(self):\n",
    "        for _field in fields(self):\n",
    "            if is_dataclass(_field.type):\n",
    "                field_val = _field.type(**getattr(self, _field.name))\n",
    "                setattr(self, _field.name, field_val)\n",
    "\n",
    "    def tolog(self, indent=1, padding=''):\n",
    "        \"\"\"Convert dataclass to formatted string for logging\"\"\"\n",
    "        import pprint as pp\n",
    "        def get_val(x, pad):\n",
    "            if isinstance(x, BaseType):\n",
    "                return x.tolog(indent, pad)\n",
    "            elif isinstance(x, Dict):\n",
    "                return pp.pformat(x, indent)\n",
    "            else:\n",
    "                return repr(x)\n",
    "        cls_name = self.__class__.__name__\n",
    "        padding = padding + '\\t' * indent\n",
    "        fields_str = [f'{padding}{k}={get_val(v, padding)}' for k, v in vars(self).items()]\n",
    "        return f'{cls_name}(\\n' + ',\\n'.join(fields_str) + '\\n)'\n",
    "\n",
    "# --- Configuration Dataclasses ---\n",
    "@dataclass\n",
    "class MetaRange:\n",
    "    \"\"\"Range configuration for row selection\"\"\"\n",
    "    start_rows: int | None\n",
    "    end_rows: int | None\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield from [self.start_rows or 1, self.end_rows or float('inf')]\n",
    "\n",
    "@dataclass\n",
    "class MetaTable(BaseType):\n",
    "    \"\"\"Excel table configuration\"\"\"\n",
    "    file: UPath\n",
    "    sheet: str\n",
    "    skip_rows: int\n",
    "    select_cols: dict\n",
    "    select_rows: dict\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.select_cols = read_dstr_lst(self.select_cols)\n",
    "        self.select_rows = read_str_lst(str(self.select_rows))\n",
    "\n",
    "@dataclass\n",
    "class MetaInput(BaseType):\n",
    "    \"\"\"Input configuration\"\"\"\n",
    "    name: str\n",
    "    step: str\n",
    "    env: str\n",
    "    range: MetaRange\n",
    "    category: Literal['loan', 'dpst']\n",
    "    clear_cache: bool = True\n",
    "    table: MetaTable = None\n",
    "\n",
    "@dataclass\n",
    "class MetaCSV:\n",
    "    \"\"\"CSV output configuration\"\"\"\n",
    "    file: UPath\n",
    "    columns: str\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.columns = read_str_lst(self.columns)\n",
    "\n",
    "@dataclass\n",
    "class S3Config:\n",
    "    \"\"\"S3 path configuration\"\"\"\n",
    "    run: UPath\n",
    "    data: UPath\n",
    "\n",
    "@dataclass\n",
    "class LogConfig:\n",
    "    \"\"\"Logging configuration\"\"\"\n",
    "    level: Literal['info', 'warning', 'debug', 'error']\n",
    "    format: str\n",
    "    file: str\n",
    "    overwrite: bool\n",
    "\n",
    "    def todict(self):\n",
    "        return {\n",
    "            'level': self.level.upper(),\n",
    "            'format': self.format,\n",
    "            'sink': self.file,\n",
    "            'mode': 'w' if self.overwrite else 'a'\n",
    "        }\n",
    "\n",
    "@dataclass\n",
    "class NextConfig:\n",
    "    \"\"\"Next step configuration\"\"\"\n",
    "    file: UPath\n",
    "    fields: str\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.fields = read_dstr_lst(self.fields)\n",
    "\n",
    "@dataclass\n",
    "class CacheConfig:\n",
    "    \"\"\"Cache configuration (not used in this notebook)\"\"\"\n",
    "    enable: bool\n",
    "    directory: UPath\n",
    "    expire_hours: int = None\n",
    "    force_restart: bool = False\n",
    "    verbose: bool = False\n",
    "\n",
    "@dataclass\n",
    "class MetaOutput(BaseType):\n",
    "    \"\"\"Output configuration\"\"\"\n",
    "    folder: UPath\n",
    "    to_pkl: UPath\n",
    "    csv: MetaCSV\n",
    "    to_s3: S3Config\n",
    "    log: LogConfig\n",
    "    next: NextConfig\n",
    "    cache: CacheConfig\n",
    "\n",
    "@dataclass\n",
    "class MetaMatch:\n",
    "    \"\"\"Column matching configuration\"\"\"\n",
    "    candidates: str\n",
    "    drop_cols: dict\n",
    "    add_cols: dict\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.candidates = read_str_lst(self.candidates)\n",
    "        self.drop_cols = list(self.drop_cols)\n",
    "        self.add_cols = list(self.add_cols)\n",
    "\n",
    "@dataclass\n",
    "class ColumnMap(BaseType):\n",
    "    \"\"\"Column mapping configuration\"\"\"\n",
    "    output: UPath\n",
    "    input: UPath\n",
    "    na_str: str\n",
    "    overwrite: bool\n",
    "    excludes: list[str]\n",
    "    pcds_col: str\n",
    "    aws_col: str\n",
    "    pcds_view: str\n",
    "    aws_view: str\n",
    "\n",
    "    def __post_init__(self):\n",
    "        def transform(p):\n",
    "            if isinstance(p, str):\n",
    "                return ['_'.join(x for x in c.split()) for c in read_str_lst(p)]\n",
    "            return p\n",
    "        self.pcds_col = transform(self.pcds_col)\n",
    "        self.pcds_view = transform(self.pcds_view)\n",
    "        self.aws_col = transform(self.aws_col)\n",
    "        self.aws_view = transform(self.aws_view)\n",
    "        if '_+_' in self.aws_view[0]:\n",
    "            self.aws_view = '.'.join('{%s}' % x.lower() for x in self.aws_view[0].split('_+_'))\n",
    "        self.excludes = [x] if isinstance(self.excludes, str) else list(self.excludes) if self.excludes else []\n",
    "\n",
    "@dataclass\n",
    "class MetaConfig(BaseType):\n",
    "    \"\"\"Main configuration class\"\"\"\n",
    "    input: MetaInput\n",
    "    output: MetaOutput\n",
    "    match: MetaMatch\n",
    "    column_maps: ColumnMap\n",
    "\n",
    "@dataclass\n",
    "class MetaRecord:\n",
    "    \"\"\"Record tracking during processing\"\"\"\n",
    "    next_d: dict = field(default_factory=dict)\n",
    "    col_maps: dict = field(default_factory=dict)\n",
    "    pull_status: PullStatus = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Configuration Reading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Patch confection library to preserve case sensitivity ---#\n",
    "def patch_confection():\n",
    "    def get_configparser(interpolate: bool = True):\n",
    "        from confection import CustomInterpolation\n",
    "        config = ConfigParser(\n",
    "            interpolation=CustomInterpolation() if interpolate else None,\n",
    "            allow_no_value=True,\n",
    "        )\n",
    "        config.optionxform = str\n",
    "        return config\n",
    "    mock_obj = mock.patch('confection.get_configparser', wraps=get_configparser)\n",
    "    if not hasattr(mock_obj, 'is_local'):\n",
    "        mock_obj.start()\n",
    "\n",
    "#--- Read configuration file and create config object ---#\n",
    "def read_config(config_class: BaseType, config_path: None | UPath | str = None, overrides={}):\n",
    "    patch_confection()\n",
    "    if UPath(config_path).is_file():\n",
    "        config = Config().from_disk(config_path, overrides=overrides)\n",
    "    else:\n",
    "        config = Config().from_str(config_path, overrides=overrides)\n",
    "    return config_class(**{**config.pop('root', {}), **config})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Utility Classes and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#--- Start logging session with separator ---#\ndef start_run():\n    logger.info('\\n\\n' + '=' * WIDTH)\n\n#--- End logging session with separator ---#\ndef end_run():\n    logger.info('\\n\\n' + '=' * WIDTH)\n\n#--- Load environment variables from file ---#\ndef load_env(file):\n    inWindows and load_dotenv(file)\n\nclass IO:\n    \"\"\"File I/O utility class - uses Parquet/JSON for cross-platform compatibility\"\"\"\n\n    @staticmethod\n    def write_dataframe(file, df):\n        \"\"\"Save DataFrame in portable Parquet format\"\"\"\n        file = UPath(file)\n        df.to_parquet(file, index=True, engine='pyarrow', compression='snappy')\n\n    @staticmethod\n    def read_dataframe(file):\n        \"\"\"Load DataFrame from Parquet format\"\"\"\n        file = UPath(file)\n        return pd.read_parquet(file, engine='pyarrow')\n\n    @staticmethod\n    def write_json(file, data, cls=None):\n        \"\"\"Save to JSON with proper serialization\"\"\"\n        import numpy as np\n        import pandas as pd\n        import datetime as dt\n\n        def convert(obj):\n            if isinstance(obj, (np.integer, np.floating)):\n                return obj.item()\n            elif isinstance(obj, np.ndarray):\n                return obj.tolist()\n            elif pd.isna(obj):\n                return None\n            elif isinstance(obj, (dt.datetime, dt.date)):\n                return obj.isoformat()\n            elif isinstance(obj, set):\n                return list(obj)\n            raise TypeError(f\"Object of type {type(obj)} is not JSON serializable\")\n\n        with open(file, 'w') as f:\n            json.dump(data, f, indent=2, default=convert, cls=cls)\n\n    @staticmethod\n    def read_json(file):\n        \"\"\"Read JSON file into dictionary\"\"\"\n        with open(file, 'r') as fp:\n            data = json.load(fp)\n        return data\n\n    @staticmethod\n    def write_pickle(file, data):\n        \"\"\"Deprecated: Use write_dataframe or write_json instead\"\"\"\n        with open(file, 'wb') as f:\n            pickle.dump(data, f)\n\n    @staticmethod\n    def read_pickle(file):\n        \"\"\"Deprecated: Use read_dataframe or read_json instead\"\"\"\n        with open(file, 'rb') as fp:\n            data = pickle.load(fp)\n        return data\n\n    @staticmethod\n    def delete_file(file):\n        \"\"\"Delete file if it exists\"\"\"\n        if (filepath := UPath(file)).exists():\n            filepath.unlink()\n\nclass UDict(dict):\n    \"\"\"Case-insensitive dictionary for flexible key matching\"\"\"\n\n    def __getitem__(self, key):\n        return super().__getitem__(self._match(key))\n\n    def __contains__(self, key):\n        try:\n            self._match(key)\n            return True\n        except KeyError:\n            return False\n\n    def _match(self, key):\n        \"\"\"Find matching key regardless of case\"\"\"\n        for k in self:\n            if k.lower() == key.lower():\n                return k\n        raise KeyError(key)\n\n    def update(self, other=None, **kwargs):\n        if other is not None:\n            for k, v in other.items() if isinstance(other, abc.Mapping) else other:\n                self[k] = v\n        for k, v in kwargs.items():\n            assert self._match(k)\n            self[k] = v\n\n    def get(self, key, default_value=None):\n        try:\n            return self[key]\n        except KeyError:\n            return default_value\n\nclass Misc:\n    \"\"\"Miscellaneous utility functions\"\"\"\n\n    @staticmethod\n    def convert2int(a):\n        \"\"\"Safely convert value to integer\"\"\"\n        try:\n            return int(a)\n        except (TypeError, ValueError):\n            return None\n\n    @staticmethod\n    def convert2datestr(a):\n        \"\"\"Convert datetime to string format\"\"\"\n        if isinstance(a, datetime):\n            return a.strftime('%Y-%m-%d')\n        return a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: PCDS Database Connection and SQL Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Connect to PCDS Oracle database ---#\n",
    "def pcds_connect(service_name, ldap_service='X'):\n",
    "    \"\"\"Establish connection to PCDS Oracle database\"\"\"\n",
    "    import oracledb\n",
    "    # Map service names to connection strings\n",
    "    svc2server = {\n",
    "        'A': 'PBCS21P',\n",
    "        'B': 'PBCS30P',\n",
    "        'C': 'PCDS',\n",
    "        'D': 'PBCS23P',\n",
    "    }\n",
    "    # Implement connection logic based on your environment\n",
    "    # Example: return oracledb.connect(user=usr, password=pwd, dsn=dns_tns)\n",
    "    raise NotImplementedError(\"Please implement PCDS connection logic\")\n",
    "\n",
    "class SQLengine:\n",
    "    \"\"\"SQL query engine for PCDS database\"\"\"\n",
    "    \n",
    "    def __init__(self, platform: Literal['PCDS']):\n",
    "        self._platform = platform\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset internal state\"\"\"\n",
    "        self._where = None\n",
    "        self._type = None\n",
    "        self._date = None\n",
    "        self._dateraw = None\n",
    "        self._table = None\n",
    "        self._format = PCDS_DT_FORMAT\n",
    "\n",
    "    def extract_var(self, stmt):\n",
    "        \"\"\"Extract variable names from SQL date expression\"\"\"\n",
    "        def _extract_var():\n",
    "            word, time, tagt = r'\\w+_\\w+', r\"'[^']*'\", r'[^,]+'\n",
    "            pattern1 = fr\"{word}\\({word}\\(({tagt}),\\s*{time}\\),\\s*{time}\\)\"\n",
    "            pattern2 = fr\"{word}\\(({tagt}),\\s*{time}\\)\"\n",
    "            if (m := re.match(pattern1, stmt)):\n",
    "                return stmt, m.group(1)\n",
    "            elif (m := re.match(pattern2, stmt)):\n",
    "                return stmt, m.group(1)\n",
    "            return stmt, stmt\n",
    "        \n",
    "        date_var, date_raw = _extract_var()\n",
    "        return date_var, date_raw.upper()\n",
    "\n",
    "    def query(self, query, connection, **query_kwargs):\n",
    "        \"\"\"Execute SQL query and return DataFrame\"\"\"\n",
    "        query = self.clean_query(query)\n",
    "        df = psql.read_sql_query(query, connection, **query_kwargs)\n",
    "        \n",
    "        #>>> Normalize column names to uppercase <<<#\n",
    "        df.columns = [x.upper() for x in df.columns]\n",
    "        return df\n",
    "\n",
    "    def clean_query(self, query: str):\n",
    "        \"\"\"Clean and prepare SQL query for execution\"\"\"\n",
    "        #>>> Extract table name from query <<<#\n",
    "        table_pattern = r'([\\w.]+)\\s+MORF\\b'\n",
    "        self._table = re.search(table_pattern, query[::-1], flags=re.I).group(1)[::-1]\n",
    "        \n",
    "        #>>> Add alias to date column if needed <<<#\n",
    "        date_pattern = r'(?!\\\\s+(?:AS\\s+)\\w+)'\n",
    "        if self._date and (match := re.search(\n",
    "            re.escape(self._date) + date_pattern,\n",
    "            re.split(r'\\b(?:FROM|WHERE)\\b', query, flags=re.I)[0],\n",
    "            flags=re.I\n",
    "        )):\n",
    "            st, ed = match.span()\n",
    "            query = query[:st] + f'{self._date} as {self._dateraw}' + query[ed:]\n",
    "        \n",
    "        #>>> Remove empty WHERE clauses <<<#\n",
    "        where_pattern = r'^\\s*where\\s*$'\n",
    "        return re.sub(where_pattern, '', query, flags=re.I | re.M)\n",
    "\n",
    "    def get_where_sql(self, date_var: str, date_type: str, start_dt=None, end_dt=None, where_cstr='') -> str:\n",
    "        \"\"\"Build WHERE clause for date filtering\"\"\"\n",
    "        self._type = date_type\n",
    "        \n",
    "        #>>> Handle subquery in where constraint <<<#\n",
    "        if not pd.isna(where_cstr) and (m := re.search(r'(?<=\\()select.*(?=\\))', where_cstr)):\n",
    "            rhs = self.query_PCDS(m.group()).iloc[0, 0]\n",
    "            if isinstance(rhs, str):\n",
    "                where_cstr = \"%s '%s'\" % (where_cstr[:m.start() - 1], rhs)\n",
    "            else:\n",
    "                where_cstr = \"%s '%s'\" % (where_cstr[:m.start() - 1], rhs.strftime('%Y-%m-%d'))\n",
    "        \n",
    "        where_sql = [where_cstr]\n",
    "        self.get_date_sql(date_var, date_type)\n",
    "        \n",
    "        #>>> Add date range filters <<<#\n",
    "        if not pd.isna(start_dt):\n",
    "            start_dt = Misc.convert2datestr(start_dt)\n",
    "            where_sql.append(f\"{self._date} >= '{start_dt}'\")\n",
    "        if not pd.isna(end_dt):\n",
    "            end_dt = Misc.convert2datestr(end_dt)\n",
    "            where_sql.append(f\"{self._date} <= '{end_dt}'\")\n",
    "        \n",
    "        #>>> Convert TO_CHAR comparisons to TO_DATE for PCDS <<<#\n",
    "        for i, sql_stmt in enumerate(where_sql):\n",
    "            if pd.isna(sql_stmt):\n",
    "                continue\n",
    "            if (match := re.match(r\"^TO_CHAR\\(([^,]+),\\s*'(.*)'\\)\\s*([><=!]+)\\s*'([^']+)'\", sql_stmt)):\n",
    "                a, b, c, d = match.groups()\n",
    "                where_sql[i] = f\"{a} {c} TO_DATE('{d}', '{b}')\"\n",
    "        \n",
    "        self._where = ' AND '.join(x for x in where_sql if not pd.isna(x))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_date_format(date_var):\n",
    "        \"\"\"Extract date format from variable specification\"\"\"\n",
    "        pattern = r'^(.+?)(?:\\s*\\(([^)]+)\\))?$'\n",
    "        date_var, date_format = re.match(pattern, date_var).groups()\n",
    "        return date_var, date_format\n",
    "\n",
    "    def get_date_sql(self, date_var: str, date_type: str):\n",
    "        \"\"\"Convert date column to standard format in SQL\"\"\"\n",
    "        date_var, date_format = self.get_date_format(date_var)\n",
    "        is_date = re.search(r'time|date', date_type, re.IGNORECASE)\n",
    "        \n",
    "        #>>> Parse string dates if format provided <<<#\n",
    "        if date_format and (not is_date):\n",
    "            date_var = f\"TO_DATE({date_var}, '{date_format}')\"\n",
    "            is_date = True\n",
    "        \n",
    "        #>>> Convert to standard string format <<<#\n",
    "        if is_date:\n",
    "            date_var = f\"TO_CHAR({date_var}, 'YYYY-MM-DD')\"\n",
    "        \n",
    "        self._date, self._dateraw = self.extract_var(date_var)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'SQL({self._platform})\\n' \\\n",
    "               f'   table: {self._table}\\n' \\\n",
    "               f'   where: {self._where}\\n' \\\n",
    "               f'   date : {self._date} ({self._dateraw})'\n",
    "\n",
    "    def query_PCDS(self, query_stmt: str, service_name: str = None, **query_kwargs):\n",
    "        \"\"\"Execute query on PCDS database\"\"\"\n",
    "        with pcds_connect(service_name=service_name) as CONN:\n",
    "            return self.query(query_stmt, CONN, **query_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Excel Input Processing and Column Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Read and process Excel input file with table configurations ---#\n",
    "def read_excel_input(config: MetaTable) -> pd.DataFrame:\n",
    "    def trim_me(x):\n",
    "        \"\"\"Trim whitespace from strings\"\"\"\n",
    "        return x.strip() if isinstance(x, str) else x\n",
    "\n",
    "    def extract_name(name):\n",
    "        \"\"\"Remove parenthetical notes from names\"\"\"\n",
    "        if pd.isna(name):\n",
    "            return pd.NA\n",
    "        if not isinstance(name, str):\n",
    "            return name\n",
    "        remove_extra = r'\\(.*\\)'\n",
    "        return re.sub(remove_extra, '', name).strip()\n",
    "\n",
    "    def merge_pcds_svc_tbl(df):\n",
    "        \"\"\"Combine service and table names into qualified names\"\"\"\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore', pd.errors.SettingWithCopyWarning)\n",
    "            cols = [x for x in df.columns if x not in (\n",
    "                'group', 'pcds_dt', 'aws_dt', 'pcds_where', 'aws_where'\n",
    "            )]\n",
    "            df[cols] = df[cols].map(extract_name)\n",
    "            tbl = df.pop('pcds_tbl')\n",
    "            df['col_map'] = df['col_map'].fillna(tbl).copy()\n",
    "            svc = df.pop('pcds_svc').fillna('no_server_provided')\n",
    "            df.loc[:, ['pcds_tbl']] = svc + '.' + tbl.str.lower()\n",
    "            df['pcds_dt'] = df['pcds_dt'].copy().fillna(NO_DATE)\n",
    "            df['aws_dt'] = df['aws_dt'].copy().fillna(NO_DATE)\n",
    "            df[['pcds_where', 'aws_where']] = df[['pcds_where', 'aws_where']].replace(np.nan, None)\n",
    "\n",
    "    file_path = config.file\n",
    "    try:\n",
    "        df = pd.read_excel(\n",
    "            file_path, sheet_name=config.sheet,\n",
    "            skiprows=config.skip_rows, usecols=list(config.select_cols)\n",
    "        )\n",
    "        df = df.rename(columns=config.select_cols).map(trim_me)\n",
    "        if len(config.select_rows) > 0:\n",
    "            df = df.query(' & '.join(config.select_rows))\n",
    "        merge_pcds_svc_tbl(df)\n",
    "        logger.info(f\"Read {len(df)} rows from {file_path}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to read Excel file {file_path}: {e}\")\n",
    "        raise\n",
    "\n",
    "class ColmapUtils:\n",
    "    \"\"\"Utility class for processing column mapping files\"\"\"\n",
    "    \n",
    "    def __init__(self, category: Literal['loan', 'dpst']):\n",
    "        self.category = category\n",
    "        if category == 'dpst':\n",
    "            self._obtain_table = self.process_dpst\n",
    "        elif category == 'loan':\n",
    "            self._obtain_table = self.process_loan\n",
    "\n",
    "    def is_column_tokenized(self, row):\n",
    "        \"\"\"Check if column contains PII that should be tokenized\"\"\"\n",
    "        if row.get('pii_encryption', None) == 'Y':\n",
    "            return True\n",
    "        try:\n",
    "            return 'tokenise' in row.get('note', \"\").lower()\n",
    "        except (TypeError, AttributeError):\n",
    "            return False\n",
    "\n",
    "    def process_loan(self, config: ColumnMap):\n",
    "        \"\"\"Process loan category mapping file\"\"\"\n",
    "        all_sheets = pd.read_excel(config.input, sheet_name=None)\n",
    "        yield from all_sheets.items()\n",
    "\n",
    "    def process_dpst(self, config: ColumnMap):\n",
    "        \"\"\"Process deposit category mapping file\"\"\"\n",
    "        all_sheets = pd.read_excel(config.input, sheet_name='Column_Details')\n",
    "        return all_sheets.groupby(config.pcds_view[0])\n",
    "\n",
    "    def process(self, config: ColumnMap) -> UDict:\n",
    "        \"\"\"Process column mapping configuration file\"\"\"\n",
    "        if os.path.exists(config.output) and (not config.overwrite):\n",
    "            return IO.read_json(config.output)\n",
    "        \n",
    "        table_excludes = config.excludes or []\n",
    "        mappings = {}\n",
    "        \n",
    "        #>>> Process each table's column mappings <<<#\n",
    "        for pcds_name, df in self._obtain_table(config):\n",
    "            if pcds_name in table_excludes:\n",
    "                continue\n",
    "            \n",
    "            if self.category == 'loan' and 'Source' in df.columns:\n",
    "                df = pd.DataFrame(df.iloc[1:].values, columns=df.iloc[0])\n",
    "\n",
    "            df = df.rename(columns=self.colfunc)\n",
    "            pcds2aws, aws2pcds = {}, {}\n",
    "            s = {\n",
    "                'aws_unique': [],\n",
    "                'pcds_unique': [],\n",
    "                'duplicated_pcds': set(),\n",
    "                'duplicated_aws': set(),\n",
    "                'pii_cols': set()\n",
    "            }\n",
    "            \n",
    "            #>>> Build bidirectional column mappings <<<#\n",
    "            for i, row in enumerate(df.itertuples()):\n",
    "                row = UDict(**row._asdict())\n",
    "                if i == 0:\n",
    "                    if self.category == 'loan':\n",
    "                        pcds_name = self.fetchcol(row, config.pcds_view, config.na_str)\n",
    "                        aws_name = self.fetchcol(row, config.aws_view, config.na_str)\n",
    "                    elif self.category == 'dpst':\n",
    "                        aws_name = config.aws_view.format(**row)\n",
    "                \n",
    "                pcds_col = self.fetchcol(row, config.pcds_col, config.na_str)\n",
    "                aws_col = self.fetchcol(row, config.aws_col, config.na_str)\n",
    "                pcds_na, aws_na = pd.isna(pcds_col), pd.isna(aws_col)\n",
    "                \n",
    "                if pcds_na and aws_na:\n",
    "                    continue\n",
    "                elif pcds_na:\n",
    "                    s['aws_unique'].append(aws_col.lower())\n",
    "                elif aws_na:\n",
    "                    s['pcds_unique'].append(pcds_col.upper())\n",
    "                else:\n",
    "                    pcds_col, aws_col = pcds_col.upper(), aws_col.lower()\n",
    "                    \n",
    "                    #>>> Check for duplicate AWS columns <<<#\n",
    "                    has_dupl, aws_dup = self.get_duplicates(aws_col, pcds_col, aws2pcds, s['aws_unique'])\n",
    "                    if has_dupl:\n",
    "                        logger.warning(f'Table {pcds_name} has duplicated AWS column {aws_col}')\n",
    "                        s['duplicated_aws'] |= aws_dup\n",
    "                    else:\n",
    "                        aws2pcds[aws_col] = pcds_col\n",
    "\n",
    "                    #>>> Check for duplicate PCDS columns <<<#\n",
    "                    has_dupl, pcds_dup = self.get_duplicates(pcds_col, aws_col, pcds2aws, s['pcds_unique'])\n",
    "                    if has_dupl:\n",
    "                        logger.warning(f'Table {pcds_name} has duplicated PCDS column {pcds_col}')\n",
    "                        s['duplicated_pcds'] |= pcds_dup\n",
    "                    else:\n",
    "                        pcds2aws[pcds_col] = aws_col\n",
    "                \n",
    "                if self.is_column_tokenized(row):\n",
    "                    s['pii_cols'].add(pcds_col)\n",
    "            \n",
    "            if len(pcds2aws) == 0:\n",
    "                logger.info(f\"No match key is found in {pcds_name}\")\n",
    "            \n",
    "            s['duplicated_pcds'] = list(s['duplicated_pcds'])\n",
    "            s['duplicated_aws'] = list(s['duplicated_aws'])\n",
    "            s['pii_cols'] = list(x for x in s['pii_cols'] if pd.notna(x))\n",
    "            mappings[pcds_name] = {\n",
    "                'pcds_table': pcds_name,\n",
    "                'aws_table': aws_name,\n",
    "                'pcds2aws': pcds2aws,\n",
    "                **s\n",
    "            }\n",
    "        \n",
    "        IO.write_json(config.output, mappings)\n",
    "        return UDict(mappings)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_duplicates(col_a, col_b, a2b: dict, unique_a: list):\n",
    "        \"\"\"Check for duplicate column mappings\"\"\"\n",
    "        exist_key = set(list(a2b) + unique_a)\n",
    "        exist_val = a2b.get(col_a, pd.NA)\n",
    "        if col_a in exist_key and col_b != exist_val:\n",
    "            return True, {f'{col_a}:{exist_val}', f'{col_a}:{col_b}'}\n",
    "        return False, None\n",
    "\n",
    "    @staticmethod\n",
    "    def colfunc(col):\n",
    "        \"\"\"Normalize column names\"\"\"\n",
    "        if pd.isna(col):\n",
    "            return 'comment'\n",
    "        col = col.split('\\n')[-1]\n",
    "        return '_'.join(x.lower() for x in col.split())\n",
    "\n",
    "    @staticmethod\n",
    "    def fetchcol(row, names, na_str):\n",
    "        \"\"\"Fetch first non-null column value from list of column names\"\"\"\n",
    "        for name in names:\n",
    "            name = name.lower()\n",
    "            if (name in row) and (not pd.isna(row[name])) and (row[name] != na_str):\n",
    "                return row[name].strip()\n",
    "        return pd.NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: PCDS Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize global objects\n",
    "proc_pcds = SQLengine('PCDS')\n",
    "record = MetaRecord()\n",
    "\n",
    "#--- Process PCDS table metadata (columns and row count) ---#\n",
    "def process_pcds_meta(row, rename_columns={}):\n",
    "    service, table = (info_str := row.pcds_tbl).split('.', maxsplit=1)\n",
    "    logger.info(f\"\\tStart processing {info_str}\")\n",
    "    \n",
    "    #>>> Query column metadata and row counts <<<#\n",
    "    try:\n",
    "        with pcds_connect(service) as CONN:\n",
    "            df_type = proc_pcds.query(PCDS_SQL_META.format(table=table), CONN)\n",
    "            if hasattr(df_type, 'last_modified'):\n",
    "                record.next_d.update(last_modified=df_type.last_modified)\n",
    "            \n",
    "            #>>> Extract date variable and build WHERE clause <<<#\n",
    "            date_var = re.match(r'(\\w+)(?=\\s*\\()?', row.pcds_dt).group(1)\n",
    "            if date_var == NO_DATE:\n",
    "                proc_pcds._where = row.pcds_where\n",
    "            else:\n",
    "                proc_pcds.get_where_sql(\n",
    "                    date_var=row.pcds_dt,\n",
    "                    date_type=df_type.query(f\"COLUMN_NAME == '{date_var.upper()}'\")['DATA_TYPE'].item(),\n",
    "                    start_dt=row.start_dt,\n",
    "                    end_dt=row.end_dt,\n",
    "                    where_cstr=row.pcds_where\n",
    "                )\n",
    "            \n",
    "            nrow_sql = PCDS_SQL_NROW.format(table=table, limit=proc_pcds._where)\n",
    "            df_nrow = proc_pcds.query(nrow_sql, CONN)\n",
    "    except (pd.errors.DatabaseError, ValueError):\n",
    "        logger.warning(f\"Couldn't find {table.upper()} in {service.upper()}\")\n",
    "        raise NONEXIST_TABLE(\"PCDS View Not Existing\")\n",
    "    \n",
    "    df_type.columns = [x.lower() for x in df_type.columns]\n",
    "    df_type['aws_colname'] = df_type['column_name'].map(rename_columns)\n",
    "    return {'column': df_type, 'row': df_nrow}, len(rename_columns) > 0\n",
    "\n",
    "#--- Query PCDS table for date-wise row counts ---#\n",
    "def process_pcds_date(row):\n",
    "    service, table = (info_str := row.pcds_tbl).split('.', maxsplit=1)\n",
    "    \n",
    "    try:\n",
    "        with pcds_connect(service) as CONN:\n",
    "            date_sql = PCDS_SQL_DATE.format(\n",
    "                table=table, limit=proc_pcds._where, date=proc_pcds._date\n",
    "            )\n",
    "            df_meta = proc_pcds.query(date_sql, CONN)\n",
    "        logger.info(f\"\\tFinish Processing {info_str}\")\n",
    "    except pd.errors.DatabaseError:\n",
    "        if proc_pcds._dateraw:\n",
    "            logger.warning(f\"Column {proc_pcds._dateraw.upper()} not found in {table.upper()}\")\n",
    "        raise NONEXIST_DATEVAR(\"Date-like Variable Not In PCDS\")\n",
    "    return df_meta\n",
    "\n",
    "#--- Initialize output folders and logging ---#\n",
    "def start_setup(start_row, C_out):\n",
    "    try:\n",
    "        assert start_row <= 1\n",
    "        os.remove(C_out.csv.file)\n",
    "    except (TypeError, AssertionError, FileNotFoundError):\n",
    "        pass\n",
    "    os.makedirs(C_out.folder, exist_ok=True)\n",
    "    logger.add(**C_out.log.todict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Configuration Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Parse command line arguments and load configuration ---#\n",
    "def parse_config():\n",
    "    parser = argparse.ArgumentParser(description='Conduct Meta Info Analysis - PCDS Step')\n",
    "    parser.add_argument(\n",
    "        '--category',\n",
    "        choices=['loan', 'dpst'],\n",
    "        default='dpst',\n",
    "        help='which meta template to use',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--name', type=str,\n",
    "        default='test_0827',\n",
    "        help='how to name this analysis (override)'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--query', type=str,\n",
    "        default='group == \"test_0827\"',\n",
    "        help='how to name this analysis (override)'\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.category == 'dpst':\n",
    "        config_path = r'files/inputs/config_meta_dpst.cfg'\n",
    "    elif args.category == 'loan':\n",
    "        config_path = r'files/inputs/config_meta_loan.cfg'\n",
    "    \n",
    "    config = read_config(\n",
    "        MetaConfig,\n",
    "        config_path=config_path,\n",
    "        overrides={\n",
    "            'input.table.select_rows': args.query,\n",
    "            'input.name': args.name\n",
    "        }\n",
    "    )\n",
    "    (out_folder := UPath(config.output.folder)).mkdir(exist_ok=True)\n",
    "    shutil.copy(config_path, out_folder.joinpath(f'{config.input.step}_pcds.cfg'))\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Main Execution - PCDS Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def main():\n    \"\"\"Main execution function for PCDS meta analysis\"\"\"\n    config = parse_config()\n    df_dict, df_next = {}, {}\n    C_out, C_in = config.output, config.input\n    start_row, end_row = C_in.range\n    start_setup(start_row, C_out)\n    logger.info('Configuration:\\n' + config.tolog())\n\n    #>>> Load environment variables <<<#\n    load_env(C_in.env)\n    start_run()\n\n    #>>> Load input table list and column mappings <<<#\n    tbl_list = (\n        read_excel_input(C_in.table)\n        .groupby('pcds_tbl')\n        .first()\n        .reset_index()\n    )\n    record.col_maps = (\n        ColmapUtils(C_in.category)\n        .process(config.column_maps)\n    )\n\n    #>>> Process each table <<<#\n    total = len(tbl_list)\n    for i, row in enumerate(tqdm(\n        tbl_list.itertuples(), desc='Processing PCDS ...', total=total\n    ), start=1):\n        if (i < start_row or i > end_row):\n            continue\n\n        pcds_m, pcds_d = {}, None\n        record.next_d = UDict(C_out.next.fields)\n        name: str = row.pcds_tbl.split('.')[1].lower()\n\n        logger.info(f\">>> Start {name}\")\n\n        #>>> Initialize record for this table <<<#\n        record.next_d.update(\n            pcds_tbl=row.pcds_tbl,\n            aws_tbl=row.aws_tbl,\n            pcds_dt=row.pcds_dt,\n            aws_dt=row.aws_dt,\n            partition=row.partition,\n            last_modified=datetime.now().strftime('%Y-%m-%d'),\n            tokenised_cols=[]\n        )\n\n        pull_status = PullStatus.SUCCESS\n\n        #>>> Try to pull PCDS table metadata <<<#\n        try:\n            rename_columns = {}\n            if (c := row.col_map) and (c in record.col_maps):\n                meta_info = record.col_maps[c]\n                rename_columns = {\n                    k: v for k, v in meta_info['pcds2aws'].items()\n                    if k not in meta_info['pii_cols']\n                }\n                record.next_d.update(tokenised_cols=meta_info['pii_cols'].copy())\n            pcds_m, exist_mapping = process_pcds_meta(row, rename_columns)\n        except NONEXIST_TABLE:\n            pull_status = PullStatus.NONEXIST_PCDS\n            logger.error(f\"PCDS table {name} does not exist\")\n            continue\n\n        #>>> Check if column mapping was provided <<<#\n        if pull_status == PullStatus.SUCCESS and (not exist_mapping):\n            pull_status = PullStatus.NO_MAPPING\n\n        #>>> Try to get date-wise counts from PCDS <<<#\n        try:\n            pcds_d = process_pcds_date(row)\n            if len(pcds_m) == 0:\n                pull_status = PullStatus.EMPTY_PCDS\n        except NONEXIST_DATEVAR:\n            if pull_status == PullStatus.SUCCESS:\n                pull_status = PullStatus.NONDATE_PCDS\n\n        #>>> Store results <<<#\n        df_dict[name] = {\n            'pcds_meta': pcds_m,\n            'pcds_date': pcds_d,\n            'status': pull_status.value,\n            'sql_engine': {\n                'where': proc_pcds._where,\n                'date': proc_pcds._date,\n                'dateraw': proc_pcds._dateraw,\n                'type': proc_pcds._type\n            }\n        }\n\n        #>>> Save metadata for next step <<<#\n        if pcds_m and 'column' in pcds_m:\n            record.next_d.update(\n                pcds_cols=SEP.join(pcds_m['column']['column_name'].tolist()),\n                pcds_types=SEP.join(pcds_m['column']['data_type'].tolist()),\n                pcds_nrows=int(pcds_m['row'].iloc[0, 0]) if 'row' in pcds_m else 0,\n                pcds_where=proc_pcds._where,\n                pcds_dt_type=proc_pcds._type\n            )\n\n        df_next[name] = record.next_d.copy()\n\n        #>>> Reset engine for next iteration <<<#\n        proc_pcds.reset()\n\n    #>>> Save results using Parquet/JSON format <<<#\n    output_folder = UPath(C_out.folder)\n\n    # Save individual parquet files for each table\n    for table_name, table_data in df_dict.items():\n        if table_data['pcds_meta'] and 'column' in table_data['pcds_meta']:\n            # Save column info as parquet\n            col_file = output_folder / f'pcds_column_info_{table_name}.parquet'\n            IO.write_dataframe(col_file, table_data['pcds_meta']['column'])\n\n        if table_data['pcds_date'] is not None:\n            # Save date counts as parquet\n            date_file = output_folder / f'pcds_date_counts_{table_name}.parquet'\n            IO.write_dataframe(date_file, table_data['pcds_date'])\n\n        # Save other metadata as JSON\n        meta_file = output_folder / f'pcds_metadata_{table_name}.json'\n        IO.write_json(meta_file, {\n            'status': table_data['status'],\n            'sql_engine': table_data['sql_engine'],\n            'nrows': table_data['pcds_meta'].get('row', pd.DataFrame()).to_dict() if table_data['pcds_meta'] else {}\n        })\n\n    # Save summary with file paths\n    summary_data = {\n        name: {\n            'column_file': str(output_folder / f'pcds_column_info_{name}.parquet'),\n            'date_file': str(output_folder / f'pcds_date_counts_{name}.parquet'),\n            'meta_file': str(output_folder / f'pcds_metadata_{name}.json'),\n            'status': data['status']\n        } for name, data in df_dict.items()\n    }\n    summary_file = output_folder / 'pcds_summary.json'\n    IO.write_json(summary_file, summary_data)\n\n    # Save next step metadata\n    IO.write_json(output_folder / 'pcds_metadata.json', df_next)\n\n    logger.info(f\"Saved PCDS results to {output_folder}\")\n    logger.info(f\"  - Individual parquet/JSON files: {len(df_dict)} tables\")\n    logger.info(f\"  - pcds_summary.json: summary of all files\")\n    logger.info(f\"  - pcds_metadata.json: metadata for next steps\")\n\n    end_run()\n\nif __name__ == '__main__':\n    main()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Analysis\n",
    "\n",
    "Uncomment the cell below to run the PCDS processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}