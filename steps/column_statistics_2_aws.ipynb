{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Column Statistics Analysis - Step 2: AWS Data Collection\n",
    "\n",
    "This notebook collects column-level statistics from AWS (Athena) platform.\n",
    "It reads metadata from step 1 and will save results for use in step 3 (comparison).\n",
    "\n",
    "**Note:** This notebook runs on Linux/remote server, so aws_creds_renew is not needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "import re\nimport os\nimport json\nimport pickle\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport pyathena as pa\nimport functools as ft\nimport multiprocessing as mp\nimport threading as td\nimport datetime as dt\nimport time\n\nfrom upath import UPath\nfrom loguru import logger\nfrom tqdm import tqdm\nfrom typing import Literal\nfrom dataclasses import dataclass, field, fields\nfrom concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\n\nwarnings.filterwarnings('ignore', message=r'pandas only supports SQLAlchemy connectable .*', category=UserWarning)\n\n# Note: This notebook uses Parquet format for cross-platform compatibility\n# Install pyarrow if needed: pip install pyarrow or conda install -c conda-forge pyarrow\nimport pyarrow",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Constants and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# --- Global Constants ---\n",
    "SEP = '; '\n",
    "AWS_DT_FORMAT = '%Y-%m-%d'\n",
    "TODAY = dt.datetime.now()\n",
    "WIDTH = 80\n",
    "\n",
    "TPartition = Literal['whole', 'year', 'year_month', 'empty', 'year_week', 'week', 'snapshot']\n",
    "\n",
    "# --- SQL Templates for AWS Column Statistics ---\n",
    "AWS_Cont_SQL = \"\"\"\n",
    "SELECT\n",
    "    '{data_type}' AS col_type,\n",
    "    COUNT({column_name}) AS col_count,\n",
    "    COUNT(DISTINCT {column_name}) AS col_distinct,\n",
    "    MAX({column_name}) AS col_max,\n",
    "    MIN({column_name}) AS col_min,\n",
    "    AVG(CAST({column_name} AS DOUBLE)) AS col_avg,\n",
    "    STDDEV_SAMP(CAST({column_name} AS DOUBLE)) AS col_std,\n",
    "    SUM(CAST({column_name} AS DOUBLE)) AS col_sum,\n",
    "    SUM(CAST({column_name} AS DOUBLE) * CAST({column_name} AS DOUBLE)) AS col_sum_sq,\n",
    "    '' AS col_freq,\n",
    "    COUNT(*) - COUNT({column_name}) AS col_missing\n",
    "FROM {db}.{table}\n",
    "WHERE {limit};\n",
    "\"\"\"\n",
    "\n",
    "AWS_Catg_SQL = \"\"\"\n",
    "WITH FreqTable_RAW AS (\n",
    "    SELECT\n",
    "        {column_name} AS p_col,\n",
    "        COUNT(*) AS value_freq\n",
    "    FROM  {db}.{table}\n",
    "    WHERE {limit}\n",
    "    GROUP BY {column_name}\n",
    "),FreqTable AS (\n",
    "    SELECT\n",
    "        p_col, value_freq, \n",
    "        ROW_NUMBER() OVER (ORDER BY value_freq DESC, p_col ASC) AS rn\n",
    "    FROM FreqTable_RAW\n",
    ")\n",
    "SELECT\n",
    "    '{data_type}' AS col_type,\n",
    "    SUM(value_freq) AS col_count,\n",
    "    COUNT(value_freq) AS col_distinct,\n",
    "    MAX(value_freq) AS col_max,\n",
    "    MIN(value_freq) AS col_min,\n",
    "    AVG(CAST(value_freq AS DOUBLE)) AS col_avg,\n",
    "    STDDEV_SAMP(CAST(value_freq AS DOUBLE)) AS col_std,\n",
    "    SUM(value_freq) AS col_sum,\n",
    "    SUM(value_freq * value_freq) AS col_sum_sq,\n",
    "    (SELECT ARRAY_JOIN(ARRAY_AGG(COALESCE(CAST(p_col AS VARCHAR), '') || '(' || CAST(value_freq AS VARCHAR) || ')' ORDER BY value_freq DESC), '; ') FROM FreqTable WHERE rn <= 10) AS col_freq, \n",
    "    (SELECT COALESCE(value_freq, 0) FROM FreqTable Where p_col is NULL) AS col_missing\n",
    "FROM FreqTable\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Core Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class Timer:\n",
    "    \"\"\"Context manager for timing code execution\"\"\"\n",
    "    \n",
    "    def __enter__(self):\n",
    "        self.start = time.perf_counter()\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_value, exc_tb):\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def time(self):\n",
    "        return time.perf_counter() - self.start\n",
    "    \n",
    "    def pause(self):\n",
    "        \"\"\"Return elapsed time and reset timer\"\"\"\n",
    "        elapsed = self.time\n",
    "        self.start = time.perf_counter()\n",
    "        return elapsed\n",
    "\n",
    "    @staticmethod\n",
    "    def to_str(value):\n",
    "        \"\"\"Convert seconds to human-readable format\"\"\"\n",
    "        minutes, seconds = divmod(value, 60)\n",
    "        hours, minutes = divmod(minutes, 60)\n",
    "        return f'{hours} hours {minutes} minutes {seconds:.0f} seconds'\n",
    "\n",
    "@dataclass\n",
    "class MetaOut:\n",
    "    \"\"\"Metadata output structure\"\"\"\n",
    "    col2COL: dict\n",
    "    col2type: dict\n",
    "    infostr: str\n",
    "    rowvar: str\n",
    "    rowexclude: list\n",
    "    rowtype: str\n",
    "    nrows: int\n",
    "    where: str\n",
    "\n",
    "@dataclass(init=False)\n",
    "class MetaJSON:\n",
    "    \"\"\"Container for metadata from previous meta analysis step\"\"\"\n",
    "    pcds: MetaOut\n",
    "    aws: MetaOut\n",
    "    last_modified: str\n",
    "    partition: TPartition = 'whole'\n",
    "    tokenised_cols: list = field(default_factory=list)\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        field_names = [f.name for f in fields(self)]\n",
    "        for k, v in kwargs.items():\n",
    "            if k in field_names:\n",
    "                setattr(self, k, v)\n",
    "        \n",
    "        def col2col(a_str, b_str, sep=SEP):\n",
    "            return {k: v for k, v in zip(a_str.split(sep), b_str.split(sep))}\n",
    "        \n",
    "        for key, other in [('pcds', 'aws'), ('aws', 'pcds')]:\n",
    "            out = MetaOut(\n",
    "                rowvar=kwargs['%s_dt' % key],\n",
    "                infostr=kwargs['%s_tbl' % key],\n",
    "                where=kwargs['%s_where' % key],\n",
    "                nrows=kwargs['%s_nrows' % key],\n",
    "                col2COL=col2col(kwargs['%s_cols' % key], kwargs['%s_cols' % other]),\n",
    "                col2type=col2col(kwargs['%s_cols' % key], kwargs['%s_types' % key]),\n",
    "                rowtype=kwargs['%s_dt_type' % key],\n",
    "                rowexclude=kwargs['%s_exclude' % key]\n",
    "            )\n",
    "            setattr(self, key, out)\n",
    "\n",
    "@dataclass\n",
    "class CSMeta:\n",
    "    \"\"\"Metadata for column statistics comparison\"\"\"\n",
    "    pcds_table: str\n",
    "    aws_table: str\n",
    "    partition: TPartition\n",
    "    vintage: str\n",
    "    pcds_time: int = 0\n",
    "    aws_time: int = 0\n",
    "\n",
    "    def todict(self):\n",
    "        return {f.name: getattr(self, f.name) for f in fields(self)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "def start_run():\n    logger.info('\\n\\n' + '=' * WIDTH)\n\ndef end_run():\n    logger.info('\\n\\n' + '=' * WIDTH)\n\nclass IO:\n    \"\"\"File I/O utilities - uses Parquet/JSON for cross-platform compatibility\"\"\"\n\n    @staticmethod\n    def write_dataframe(file, df):\n        \"\"\"Save DataFrame in portable Parquet format\"\"\"\n        file = UPath(file)\n        df.to_parquet(file, index=True, engine='pyarrow', compression='snappy')\n\n    @staticmethod\n    def read_dataframe(file):\n        \"\"\"Load DataFrame from Parquet format\"\"\"\n        file = UPath(file)\n        return pd.read_parquet(file, engine='pyarrow')\n\n    @staticmethod\n    def write_json(file, data, cls=None):\n        \"\"\"Save to JSON with proper serialization\"\"\"\n        import numpy as np\n        import pandas as pd\n        import datetime as dt\n\n        def convert(obj):\n            if isinstance(obj, (np.integer, np.floating)):\n                return obj.item()\n            elif isinstance(obj, np.ndarray):\n                return obj.tolist()\n            elif pd.isna(obj):\n                return None\n            elif isinstance(obj, (dt.datetime, dt.date)):\n                return obj.isoformat()\n            elif isinstance(obj, set):\n                return list(obj)\n            raise TypeError(f\"Object of type {type(obj)} is not JSON serializable\")\n\n        with open(file, 'w') as f:\n            json.dump(data, f, indent=2, default=convert, cls=cls)\n\n    @staticmethod\n    def read_json(file):\n        \"\"\"Load from JSON\"\"\"\n        with open(file, 'r') as f:\n            return json.load(f)\n\n    @staticmethod\n    def write_pickle(file, data):\n        \"\"\"Deprecated: Use write_dataframe or write_json instead\"\"\"\n        with open(file, 'wb') as f:\n            pickle.dump(data, f)\n\n    @staticmethod\n    def read_pickle(file):\n        \"\"\"Deprecated: Use read_dataframe or read_json instead\"\"\"\n        with open(file, 'rb') as f:\n            return pickle.load(f)\n\n    @staticmethod\n    def read_meta_json(json_file):\n        \"\"\"Read metadata JSON and convert to MetaJSON objects\"\"\"\n        data = IO.read_json(json_file)\n        return {k: MetaJSON(**v) for k, v in data.items()}",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Date Handling Functions for AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def get_iso_week_dates(year, week):\n",
    "    jan01, dec31 = dt.datetime(year, 1, 1), dt.datetime(year, 12, 31)\n",
    "    first_day = jan01 - dt.timedelta(days=jan01.weekday())\n",
    "    start = first_day + dt.timedelta(weeks=week - 1)\n",
    "    end = start + dt.timedelta(days=6)\n",
    "    start, end = max(start, jan01), min(end, dec31)\n",
    "    return start.strftime('%Y-%m-%d'), end.strftime('%Y-%m-%d')\n",
    "\n",
    "def parse_format_date(str_w_format):\n",
    "    pattern = r'^(.+?)(?:\\s*\\(([^)]+)\\))?$'\n",
    "    return re.match(pattern, str_w_format)\n",
    "\n",
    "def parse_exclude_date(exclude_clause):\n",
    "    \"\"\"Convert date exclusions to AWS format\"\"\"\n",
    "    p2 = r\"DATE_FORMAT\\(DATE_PARSE\\((?P<col>\\w+),\\s*'(?P<fmt>%Y%m%d)'\\),\\s*'%Y-%m-%d'\\)\\s+(?P<op>not in|in)\\s+\\((?P<dates>.*?)\\)\"\n",
    "    if m := re.match(p2, exclude_clause, flags=re.I):\n",
    "        col, fmt, op, dates = m.groups()\n",
    "        new_dates = ', '.join(\n",
    "            \"'%s'\" % dt.datetime.strptime(date.strip(\"'\"), '%Y-%m-%d').strftime(fmt)\n",
    "            for date in dates.split(',')\n",
    "        )\n",
    "        return '%s %s (%s)' % (col, op, new_dates)\n",
    "    return exclude_clause\n",
    "\n",
    "def get_aws_where(date_var, date_type, date_partition, date_range, date_format, snapshot=None, exclude_clauses=[]):\n",
    "    if '=' in date_range:\n",
    "        _date_var, date_range = date_range.split('=', 1)\n",
    "        assert date_var.split()[0] == _date_var, f\"Date Variable Should Match: {date_var} vs {_date_var}\"\n",
    "    \n",
    "    if (m := parse_format_date(date_var)):\n",
    "        date_var, date_format = m.groups()\n",
    "    \n",
    "    if date_type and re.match(r'^(string|varchar)', date_type, re.IGNORECASE):\n",
    "        if date_format:\n",
    "            date_var = f\"DATE_PARSE({date_var}, '{date_format}')\"\n",
    "        else:\n",
    "            date_var = f\"DATE_PARSE({date_var}, '%Y%m%d')\"\n",
    "    \n",
    "    if snapshot:\n",
    "        return ' AND '.join('(%s)' % parse_exclude_date(x) for x in exclude_clauses if x)\n",
    "    elif date_partition == 'whole':\n",
    "        base_clause = \"1=1\"\n",
    "    elif date_partition == 'year':\n",
    "        base_clause = f\"DATE_FORMAT({date_var}, '%Y') = '{date_range}'\"\n",
    "    elif date_partition == 'year_month':\n",
    "        base_clause = f\"DATE_FORMAT({date_var}, '%Y-%m') = '{date_range}'\"\n",
    "    elif date_partition in ('year_week', 'week'):\n",
    "        if '-W' in date_range:\n",
    "            year, week = date_range.split('-W')\n",
    "        else:\n",
    "            year, week = map(int, date_range.split('-'))\n",
    "            week = f\"W{week:02d}\"\n",
    "        base_clause = f\"DATE_FORMAT({date_var}, '%Y-%v') = '{year}-{week}'\"\n",
    "    elif date_partition == 'daily':\n",
    "        base_clause = f\"DATE({date_var}) = DATE('{date_range}')\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported partition type: {date_partition}\")\n",
    "    \n",
    "    if (exclude_clauses := [x for x in exclude_clauses if x]):\n",
    "        exclude_clause = ' AND '.join('(%s)' % parse_exclude_date(x) for x in exclude_clauses if x)\n",
    "        return f\"({base_clause}) AND ({exclude_clause})\"\n",
    "    else:\n",
    "        return base_clause"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: AWS SQL Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas.io.sql as psql\n",
    "\n",
    "class SQLengine:\n",
    "    \"\"\"SQL query engine for AWS\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self._where = None\n",
    "        self._type = None\n",
    "        self._date = None\n",
    "        self._dateraw = None\n",
    "        self._table = None\n",
    "\n",
    "    def query(self, query, connection, **query_kwargs):\n",
    "        \"\"\"Execute SQL query and return DataFrame\"\"\"\n",
    "        df = psql.read_sql_query(query, connection, **query_kwargs)\n",
    "        df.columns = [x.lower() for x in df.columns]\n",
    "        return df\n",
    "\n",
    "    def query_AWS(self, query_stmt: str, **query_kwargs):\n",
    "        \"\"\"Execute query on AWS Athena\"\"\"\n",
    "        # No aws_creds_renew needed - running on Linux/remote server with IAM role\n",
    "        CONN = pa.connect(\n",
    "            s3_staging_dir=\"s3://355538383407-us-east-1-athena-output/uscb-analytics/\",\n",
    "            region_name=\"us-east-1\",\n",
    "        )\n",
    "        return self.query(query_stmt, CONN, **query_kwargs)\n",
    "\n",
    "proc_aws = SQLengine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: AWS Column Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class PsuedoLock:\n",
    "    \"\"\"Dummy lock for single-threaded execution\"\"\"\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        pass\n",
    "\n",
    "class ColumnAnalyzer:\n",
    "    \"\"\"AWS column analysis engine\"\"\"\n",
    "    \n",
    "    parallel = 'process'\n",
    "\n",
    "    @staticmethod\n",
    "    def run_aws_single(data_type, column_name, db, table, limit='', lock=None):\n",
    "        \"\"\"Run statistics on single AWS column\"\"\"\n",
    "        continuous_types = ('tinyint', 'smallint', 'integer', 'bigint', 'float', 'double', 'decimal')\n",
    "        \n",
    "        if re.match('^time', data_type, flags=re.I):\n",
    "            column_name = 'CAST(%s AS DATE)' % column_name\n",
    "        \n",
    "        is_continuous = any(ct in data_type.lower() for ct in continuous_types)\n",
    "        if is_continuous:\n",
    "            sql_template = AWS_Cont_SQL\n",
    "        else:\n",
    "            sql_template = AWS_Catg_SQL\n",
    "        \n",
    "        sql_stmt = sql_template.format(\n",
    "            db=db, table=table, column_name=column_name, data_type=data_type, limit=limit\n",
    "        )\n",
    "        \n",
    "        # No lock needed for credential renewal on Linux/remote server\n",
    "        proc_aws_local = SQLengine()\n",
    "        return proc_aws_local.query_AWS(sql_stmt)\n",
    "\n",
    "    def run_aws_column_analysis(self, info_str: str, columns_info: dict, where_clause: str = \"1=1\", n_jobs=None) -> pd.DataFrame:\n",
    "        \"\"\"Run column analysis on AWS table with optional parallelization\"\"\"\n",
    "        results = {}\n",
    "        db_name, table_name = info_str.split('.')\n",
    "        logger.info(f\"Executing AWS analysis for {table_name}\")\n",
    "        worker = ft.partial(self.run_aws_single, db=db_name, table=table_name, limit=where_clause)\n",
    "        \n",
    "        try:\n",
    "            if n_jobs is None:\n",
    "                locker = PsuedoLock()\n",
    "                for col_name, data_type in tqdm(columns_info.items()):\n",
    "                    results[col_name] = worker(data_type, col_name, lock=locker)\n",
    "            else:\n",
    "                if self.parallel == \"thread\":\n",
    "                    executor_class = ThreadPoolExecutor\n",
    "                    locker = td.Lock()\n",
    "                elif self.parallel == \"process\":\n",
    "                    executor_class = ft.partial(ProcessPoolExecutor, mp_context=mp.get_context('spawn'))\n",
    "                    locker = mp.Manager().Lock()\n",
    "                \n",
    "                with executor_class(max_workers=n_jobs) as executor:\n",
    "                    futures = {}\n",
    "                    for col_name, data_type in tqdm(columns_info.items()):\n",
    "                        futures[executor.submit(worker, data_type, col_name, lock=locker)] = col_name\n",
    "                    \n",
    "                    for future in tqdm(as_completed(futures), total=len(futures), desc='Processing ... '):\n",
    "                        try:\n",
    "                            col_name = futures[future]\n",
    "                            results[col_name] = future.result()\n",
    "                        except Exception as e:\n",
    "                            logger.error(f\"Task failed: {e}\")\n",
    "                    executor.shutdown()\n",
    "            \n",
    "            df = pd.concat(results.values(), keys=results).droplevel(1)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in AWS analysis: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Main Execution - AWS Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "def main_aws():\n    \"\"\"Main execution function for AWS column statistics collection\"\"\"\n\n    # Configuration - adjust these paths as needed\n    meta_json_path = 'path/to/meta_analysis_output.json'  # From meta_analysis step\n    meta_csv_path = 'path/to/meta_analysis.csv'\n    pcds_summary_path = 'output/column_stats_pcds/pcds_summary.json'  # From step 1\n    output_folder = UPath('output/column_stats_aws')\n    output_folder.mkdir(exist_ok=True, parents=True)\n\n    n_process = 4  # Number of parallel processes\n\n    start_run()\n\n    # Load metadata from previous steps\n    meta_json = IO.read_meta_json(meta_json_path)\n    meta_csv = pd.read_csv(meta_csv_path)\n\n    # Load PCDS summary to get vintages\n    if UPath(pcds_summary_path).exists():\n        pcds_summary = IO.read_json(pcds_summary_path)\n        logger.info(f\"Loaded PCDS summary from {pcds_summary_path}\")\n    else:\n        logger.warning(f\"PCDS summary not found at {pcds_summary_path}, using default vintages\")\n        pcds_summary = {}\n\n    CA = ColumnAnalyzer()\n    summary_data = {}\n\n    for i, row in tqdm(meta_csv.iterrows(), desc='Processing AWS tables...', total=len(meta_csv)):\n        name = row.get('PCDS Table Details with DB Name')\n        logger.info(f\"Processing dataset: {name}\")\n\n        # Load metadata for this table\n        meta_info = meta_json.get(name)\n        if not meta_info:\n            continue\n\n        meta_aws = meta_info.aws\n        partition = meta_info.partition\n\n        if partition == 'empty':\n            continue\n\n        # Get vintages from PCDS summary\n        if name in pcds_summary:\n            vintages = list(pcds_summary[name].keys())\n        else:\n            vintages = ['entire_dataset']\n\n        # Clean table name for file naming\n        table_name = name.split('.')[-1].lower()\n\n        # Remove PII and tokenized columns\n        avai_cols = [x for x in meta_aws.col2type if x not in meta_info.tokenised_cols]\n        col2type_filtered = {k: v for k, v in meta_aws.col2type.items() if k in avai_cols}\n\n        for vintage in vintages:\n            logger.info(f\"Processing vintage: {vintage}\")\n\n            # Build WHERE clause for this vintage\n            rowvar = meta_aws.rowvar\n            aws_where = get_aws_where(\n                date_var=rowvar,\n                date_type=meta_aws.rowtype,\n                date_partition=partition,\n                date_range='%s=%s' % (re.sub(r\"\\s*\\(.*?\\)$\", \"\", rowvar), vintage) if vintage != 'entire_dataset' else vintage,\n                date_format='%Y%m%d',\n                snapshot=partition == 'snapshot',\n                exclude_clauses=[meta_aws.where, meta_aws.rowexclude]\n            )\n\n            # Compute column statistics\n            with Timer() as timer:\n                aws_stats = CA.run_aws_column_analysis(\n                    meta_aws.infostr, col2type_filtered, aws_where, n_process\n                )\n                aws_time = timer.pause()\n\n            # Save individual parquet file for this table/vintage\n            stats_file = output_folder / f'aws_stats_{table_name}_{vintage}.parquet'\n            IO.write_dataframe(stats_file, aws_stats)\n            logger.info(f\"Saved stats to {stats_file}\")\n\n            # Save metadata as JSON\n            meta_data = CSMeta(\n                pcds_table=meta_info.pcds.infostr,\n                aws_table=meta_aws.infostr,\n                partition=partition,\n                vintage=vintage,\n                aws_time=aws_time,\n            ).todict()\n\n            meta_file = output_folder / f'aws_meta_{table_name}_{vintage}.json'\n            IO.write_json(meta_file, {\n                'meta_data': meta_data,\n                'aws_where': aws_where\n            })\n            logger.info(f\"Saved metadata to {meta_file}\")\n\n            # Add to summary\n            if name not in summary_data:\n                summary_data[name] = {}\n            summary_data[name][vintage] = {\n                'stats_file': str(stats_file),\n                'meta_file': str(meta_file),\n                'table_name': table_name,\n                'meta_data': meta_data\n            }\n\n            logger.info(f\"Completed AWS analysis for vintage {vintage}\")\n\n        proc_aws.reset()\n\n    # Save summary file\n    summary_file = output_folder / 'aws_summary.json'\n    IO.write_json(summary_file, summary_data)\n    logger.info(f\"Summary saved to {summary_file}\")\n    logger.info(f\"Processed {len(summary_data)} tables with {sum(len(v) for v in summary_data.values())} total vintages\")\n\n    end_run()\n    return summary_data\n\nif __name__ == '__main__':\n    results = main_aws()",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Analysis\n",
    "\n",
    "Uncomment the cell below to run the AWS data collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# results = main_aws()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}