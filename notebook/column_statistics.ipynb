{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Column Statistics Analysis - Comparing PCDS and AWS Data\n",
    "\n",
    "This notebook performs detailed column-level statistical analysis comparing data between PCDS (Oracle) and AWS (Athena) databases.\n",
    "It computes and compares:\n",
    "- Count, distinct values, min/max\n",
    "- Mean, standard deviation, sum\n",
    "- Frequency distributions\n",
    "- Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import shutil\n",
    "import argparse\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import functools as ft\n",
    "import datetime as dt\n",
    "import multiprocessing as mp\n",
    "import threading as td\n",
    "import time\n",
    "\n",
    "from upath import UPath\n",
    "from loguru import logger\n",
    "from tqdm import tqdm\n",
    "from typing import get_args, Literal, Dict, List\n",
    "from dataclasses import dataclass, field, fields\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\n",
    "from enum import Enum\n",
    "import xlwings as xw\n",
    "from xlwings.constants import VAlign, HAlign\n",
    "from PIL import ImageColor\n",
    "\n",
    "warnings.filterwarnings('ignore', message=r'pandas only supports SQLAlchemy connectable .*', category=UserWarning)\n",
    "\n",
    "get_rgb = ImageColor.getrgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Constants and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Global Constants ---\n",
    "SEP = '; '\n",
    "AWS_DT_FORMAT = '%Y-%m-%d'\n",
    "TODAY = dt.datetime.now()\n",
    "WIDTH = 80\n",
    "inWindows = os.name == 'nt'\n",
    "\n",
    "TPartition = Literal['whole', 'year', 'year_month', 'empty', 'year_week', 'week', 'snapshot']\n",
    "\n",
    "# --- SQL Templates for AWS Column Statistics ---\n",
    "AWS_Cont_SQL = \"\"\"\n",
    "SELECT\n",
    "    '{data_type}' AS col_type,\n",
    "    COUNT({column_name}) AS col_count,\n",
    "    COUNT(DISTINCT {column_name}) AS col_distinct,\n",
    "    MAX({column_name}) AS col_max,\n",
    "    MIN({column_name}) AS col_min,\n",
    "    AVG(CAST({column_name} AS DOUBLE)) AS col_avg,\n",
    "    STDDEV_SAMP(CAST({column_name} AS DOUBLE)) AS col_std,\n",
    "    SUM(CAST({column_name} AS DOUBLE)) AS col_sum,\n",
    "    SUM(CAST({column_name} AS DOUBLE) * CAST({column_name} AS DOUBLE)) AS col_sum_sq,\n",
    "    '' AS col_freq,\n",
    "    COUNT(*) - COUNT({column_name}) AS col_missing\n",
    "FROM {db}.{table}\n",
    "WHERE {limit};\n",
    "\"\"\"\n",
    "\n",
    "AWS_Catg_SQL = \"\"\"\n",
    "WITH FreqTable_RAW AS (\n",
    "    SELECT\n",
    "        {column_name} AS p_col,\n",
    "        COUNT(*) AS value_freq\n",
    "    FROM  {db}.{table}\n",
    "    WHERE {limit}\n",
    "    GROUP BY {column_name}\n",
    "),FreqTable AS (\n",
    "    SELECT\n",
    "        p_col, value_freq, \n",
    "        ROW_NUMBER() OVER (ORDER BY value_freq DESC, p_col ASC) AS rn\n",
    "    FROM FreqTable_RAW\n",
    ")\n",
    "SELECT\n",
    "    '{data_type}' AS col_type,\n",
    "    SUM(value_freq) AS col_count,\n",
    "    COUNT(value_freq) AS col_distinct,\n",
    "    MAX(value_freq) AS col_max,\n",
    "    MIN(value_freq) AS col_min,\n",
    "    AVG(CAST(value_freq AS DOUBLE)) AS col_avg,\n",
    "    STDDEV_SAMP(CAST(value_freq AS DOUBLE)) AS col_std,\n",
    "    SUM(value_freq) AS col_sum,\n",
    "    SUM(value_freq * value_freq) AS col_sum_sq,\n",
    "    (SELECT ARRAY_JOIN(ARRAY_AGG(COALESCE(CAST(p_col AS VARCHAR), '') || '(' || CAST(value_freq AS VARCHAR) || ')' ORDER BY value_freq DESC), '; ') FROM FreqTable WHERE rn <= 10) AS col_freq, \n",
    "    (SELECT COALESCE(value_freq, 0) FROM FreqTable Where p_col is NULL) AS col_missing\n",
    "FROM FreqTable\n",
    "\"\"\"\n",
    "\n",
    "# --- SQL Template for PCDS Column Statistics ---\n",
    "PCDS_SQL_COLUMN = \"\"\"\n",
    "DECLARE\n",
    "    v_sql          VARCHAR2(20000);\n",
    "    v_col_name     VARCHAR2(128);\n",
    "    v_data_type    VARCHAR2(128);\n",
    "    v_table_name   VARCHAR2(128) := UPPER('{table}');\n",
    "\n",
    "    v_ret_data_type  VARCHAR2(128);\n",
    "    v_count          NUMBER;\n",
    "    v_distinct       NUMBER;\n",
    "    v_max            VARCHAR2(4000);\n",
    "    v_min            VARCHAR2(4000);\n",
    "    v_missing        NUMBER;\n",
    "    v_avg            NUMBER;\n",
    "    v_std            NUMBER;\n",
    "    v_sum            NUMBER;\n",
    "    v_sum_sq         NUMBER;\n",
    "    v_freq           VARCHAR2(4000);\n",
    "\n",
    "    l_column_ref VARCHAR2(256);\n",
    "\n",
    "BEGIN\n",
    "    FOR rec IN (\n",
    "        SELECT column_name, data_type\n",
    "        FROM all_tab_cols\n",
    "        WHERE table_name = v_table_name\n",
    "        ORDER BY column_id\n",
    "    ) LOOP\n",
    "        v_col_name := rec.column_name;\n",
    "        v_data_type := rec.data_type;\n",
    "\n",
    "        v_ret_data_type := NULL; v_count := NULL; v_distinct := NULL; \n",
    "        v_max := NULL; v_min := NULL; v_missing := NULL; \n",
    "        v_avg := NULL; v_std := NULL; v_sum := NULL; v_sum_sq := NULL; v_freq := NULL;\n",
    "        \n",
    "        IF v_data_type LIKE 'TIMESTAMP%' THEN \n",
    "            l_column_ref := 'TRUNC(' || v_col_name || ')';\n",
    "        ELSE\n",
    "            l_column_ref := v_col_name;\n",
    "        END IF;\n",
    "\n",
    "        IF v_data_type IN ('NUMBER', 'FLOAT', 'BINARY_FLOAT', 'BINARY_DOUBLE') THEN\n",
    "            v_sql := 'SELECT ';\n",
    "            v_sql := v_sql || '''' || v_data_type || ''' AS col_type, ';\n",
    "            v_sql := v_sql || 'COUNT(' || v_col_name || ') AS col_count, ';\n",
    "            v_sql := v_sql || 'COUNT(DISTINCT ' || v_col_name || ') AS col_distinct, ';\n",
    "            v_sql := v_sql || 'MAX(' || v_col_name || ') AS col_max, ';\n",
    "            v_sql := v_sql || 'MIN(' || v_col_name || ') AS col_min, ';\n",
    "            v_sql := v_sql || 'AVG(' || v_col_name || ') AS col_avg, ';\n",
    "            v_sql := v_sql || 'STDDEV_SAMP(' || v_col_name || ') AS col_std, ';\n",
    "            v_sql := v_sql || 'SUM(' || v_col_name || ') AS col_sum, ';\n",
    "            v_sql := v_sql || 'SUM(' || v_col_name || ' * ' || v_col_name || ') AS col_sum_sq, ';\n",
    "            v_sql := v_sql || 'COUNT(*) - COUNT(' || v_col_name || ') AS col_missing, ';\n",
    "            v_sql := v_sql || 'EMPTY_CLOB() AS col_freq ';\n",
    "            v_sql := v_sql || 'FROM ' || v_table_name || ' WHERE {limit} ';\n",
    "        ELSE\n",
    "            v_sql := 'WITH FreqTable_RAW AS ( ';\n",
    "            v_sql := v_sql || 'SELECT ' || l_column_ref || ' AS p_col, COUNT(*) AS value_freq '; \n",
    "            v_sql := v_sql || 'FROM ' || v_table_name || ' WHERE {limit} '; \n",
    "            v_sql := v_sql || 'GROUP BY ' || l_column_ref || ' '; \n",
    "            v_sql := v_sql || '), FreqTable AS ( '; \n",
    "            v_sql := v_sql || 'SELECT p_col, value_freq, '; \n",
    "            v_sql := v_sql || 'ROW_NUMBER() OVER (ORDER BY value_freq DESC, p_col ASC) AS rn '; \n",
    "            v_sql := v_sql || 'FROM FreqTable_RAW), AggStats AS ( ';\n",
    "            v_sql := v_sql || 'SELECT SUM(ft.value_freq) AS col_count, '; \n",
    "            v_sql := v_sql || 'COUNT(ft.value_freq) AS col_distinct, '; \n",
    "            v_sql := v_sql || 'MAX(ft.value_freq) AS col_max, '; \n",
    "            v_sql := v_sql || 'MIN(ft.value_freq) AS col_min, '; \n",
    "            v_sql := v_sql || 'AVG(ft.value_freq) AS col_avg, '; \n",
    "            v_sql := v_sql || 'STDDEV_SAMP(ft.value_freq) AS col_std, ';\n",
    "            v_sql := v_sql || 'SUM(ft.value_freq) AS col_sum, ';\n",
    "            v_sql := v_sql || 'SUM(ft.value_freq * ft.value_freq) AS col_sum_sq '; \n",
    "            v_sql := v_sql || 'FROM FreqTable ft) SELECT ';\n",
    "            v_sql := v_sql || '''' || v_data_type || ''' AS col_type, ast.*, '; \n",
    "            v_sql := v_sql || '(SELECT NVL(value_freq, 0) FROM FreqTable WHERE p_col IS NULL) AS col_missing, '; \n",
    "            v_sql := v_sql || '(SELECT LISTAGG(p_col || ''('' || value_freq || '')'', ''; '') WITHIN GROUP (ORDER BY value_freq DESC) FROM FreqTable WHERE rn <= 10) AS col_freq ';\n",
    "            v_sql := v_sql || 'FROM AggStats ast';\n",
    "        END IF;\n",
    "\n",
    "        EXECUTE IMMEDIATE v_sql INTO\n",
    "           v_ret_data_type, v_count, v_distinct, v_max, v_min, \n",
    "           v_avg, v_std, v_sum, v_sum_sq, v_missing, v_freq;\n",
    "\n",
    "        DBMS_OUTPUT.PUT_LINE('Column: ' || v_col_name);\n",
    "        DBMS_OUTPUT.PUT_LINE('  col_type: ' || v_ret_data_type); \n",
    "        DBMS_OUTPUT.PUT_LINE('  col_count: ' || v_count);\n",
    "        DBMS_OUTPUT.PUT_LINE('  col_distinct: ' || v_distinct);\n",
    "        DBMS_OUTPUT.PUT_LINE('  col_max: ' || v_max);\n",
    "        DBMS_OUTPUT.PUT_LINE('  col_min: ' || v_min);\n",
    "        DBMS_OUTPUT.PUT_LINE('  col_avg: ' || v_avg);\n",
    "        DBMS_OUTPUT.PUT_LINE('  col_std: ' || v_std);\n",
    "        DBMS_OUTPUT.PUT_LINE('  col_sum: ' || v_sum);\n",
    "        DBMS_OUTPUT.PUT_LINE('  col_sum_sq: ' || v_sum_sq);\n",
    "        DBMS_OUTPUT.PUT_LINE('  col_freq: ' || v_freq);\n",
    "        DBMS_OUTPUT.PUT_LINE('  col_missing: ' || v_missing);\n",
    "        DBMS_OUTPUT.PUT_LINE('---');\n",
    "    END LOOP;\n",
    "END;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Core Data Types and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    \"\"\"Context manager for timing code execution\"\"\"\n",
    "    \n",
    "    def __enter__(self):\n",
    "        self.start = time.perf_counter()\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_value, exc_tb):\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def time(self):\n",
    "        return time.perf_counter() - self.start\n",
    "    \n",
    "    def pause(self):\n",
    "        \"\"\"Return elapsed time and reset timer\"\"\"\n",
    "        elapsed = self.time\n",
    "        self.start = time.perf_counter()\n",
    "        return elapsed\n",
    "\n",
    "    @staticmethod\n",
    "    def to_str(value):\n",
    "        \"\"\"Convert seconds to human-readable format\"\"\"\n",
    "        minutes, seconds = divmod(value, 60)\n",
    "        hours, minutes = divmod(minutes, 60)\n",
    "        return f'{hours} hours {minutes} minutes {seconds:.0f} seconds'\n",
    "\n",
    "@dataclass\n",
    "class MetaOut:\n",
    "    \"\"\"Metadata output structure\"\"\"\n",
    "    col2COL: dict\n",
    "    col2type: dict\n",
    "    infostr: str\n",
    "    rowvar: str\n",
    "    rowexclude: list\n",
    "    rowtype: str\n",
    "    nrows: int\n",
    "    where: str\n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        field_names = [f.name for f in fields(self)]\n",
    "        for k, v in kwargs.items():\n",
    "            if k in field_names:\n",
    "                setattr(self, k, v)\n",
    "\n",
    "@dataclass(init=False)\n",
    "class MetaJSON:\n",
    "    \"\"\"Container for metadata from previous meta analysis step\"\"\"\n",
    "    aws: MetaOut\n",
    "    pcds: MetaOut\n",
    "    last_modified: str\n",
    "    partition: TPartition = 'whole'\n",
    "    tokenised_cols: list = field(default_factory=list)\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        field_names = [f.name for f in fields(self)]\n",
    "        for k, v in kwargs.items():\n",
    "            if k in field_names:\n",
    "                setattr(self, k, v)\n",
    "        \n",
    "        #>>> Build MetaOut objects for PCDS and AWS <<<#\n",
    "        def col2col(a_str, b_str, sep=SEP):\n",
    "            return {k: v for k, v in zip(a_str.split(sep), b_str.split(sep))}\n",
    "        \n",
    "        for key, other in [('pcds', 'aws'), ('aws', 'pcds')]:\n",
    "            out = MetaOut(\n",
    "                rowvar=kwargs['%s_dt' % key],\n",
    "                infostr=kwargs['%s_tbl' % key],\n",
    "                where=kwargs['%s_where' % key],\n",
    "                nrows=kwargs['%s_nrows' % key],\n",
    "                col2COL=col2col(kwargs['%s_cols' % key], kwargs['%s_cols' % other]),\n",
    "                col2type=col2col(kwargs['%s_cols' % key], kwargs['%s_types' % key]),\n",
    "                rowtype=kwargs['%s_dt_type' % key],\n",
    "                rowexclude=kwargs['%s_exclude' % key]\n",
    "            )\n",
    "            setattr(self, key, out)\n",
    "\n",
    "@dataclass\n",
    "class CSMeta:\n",
    "    \"\"\"Metadata for column statistics comparison\"\"\"\n",
    "    pcds_table: str\n",
    "    aws_table: str\n",
    "    partition: TPartition\n",
    "    vintage: str\n",
    "    pcds_time: int\n",
    "    aws_time: int\n",
    "\n",
    "    def todict(self):\n",
    "        return {f.name: getattr(self, f.name) for f in fields(self)}\n",
    "\n",
    "@dataclass\n",
    "class CSResult:\n",
    "    \"\"\"Results from column statistics comparison\"\"\"\n",
    "    pcds_stats: pd.DataFrame\n",
    "    aws_stats: pd.DataFrame\n",
    "    miss_columns: set\n",
    "    miss_details: dict\n",
    "    meta_data: CSMeta\n",
    "\n",
    "@dataclass\n",
    "class SQLRecord:\n",
    "    \"\"\"Record tracking for SQL analysis\"\"\"\n",
    "    name: str\n",
    "    unmatched: set = field(default_factory=set)\n",
    "    nrow: int = 0\n",
    "    ncol: int = 0\n",
    "    pcds_time: int = 0\n",
    "    aws_time: int = 0\n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            old_v = getattr(self, k)\n",
    "            if k in ('unmatched',):\n",
    "                self.unmatched |= v\n",
    "            elif k in ('nrow', 'pcds_time', 'aws_time'):\n",
    "                setattr(self, k, old_v + v)\n",
    "            else:\n",
    "                setattr(self, k, v)\n",
    "\n",
    "    def toJSON(self):\n",
    "        return {\n",
    "            'Column Stats UnMatch': 'Yes' if len(self.unmatched) > 0 else 'No',\n",
    "            'Stats UnMatch Details': SEP.join(self.unmatched),\n",
    "            'Compared Dataset Shape': f'Row({self.nrow}) : Col({self.ncol})',\n",
    "            'Execution Time': 'PCDS({}) : AWS({})'.format(Timer.to_str(self.pcds_time), Timer.to_str(self.aws_time))\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Configuration Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration classes would normally be loaded from confection\n",
    "# For this notebook, we'll define minimal versions\n",
    "\n",
    "@dataclass\n",
    "class CSInput:\n",
    "    \"\"\"Input configuration\"\"\"\n",
    "    name: str\n",
    "    step: str\n",
    "    prev: str\n",
    "    env: UPath\n",
    "    folder: UPath\n",
    "    csv: UPath\n",
    "    json: UPath\n",
    "\n",
    "@dataclass\n",
    "class MetaCSV:\n",
    "    \"\"\"CSV configuration\"\"\"\n",
    "    file: UPath\n",
    "    columns: list\n",
    "\n",
    "@dataclass\n",
    "class CSOutput:\n",
    "    \"\"\"Output configuration\"\"\"\n",
    "    folder: UPath\n",
    "    pkl: UPath\n",
    "    csv: MetaCSV\n",
    "    xlsx: UPath\n",
    "    json: UPath\n",
    "\n",
    "@dataclass\n",
    "class ColumnConfig:\n",
    "    \"\"\"Column exclusion configuration\"\"\"\n",
    "    pii_cols: list[str]\n",
    "    token_cols: list[str]\n",
    "\n",
    "@dataclass\n",
    "class CSCompare:\n",
    "    \"\"\"Comparison configuration\"\"\"\n",
    "    n_process: int\n",
    "    drop_na: bool\n",
    "    exclude: ColumnConfig\n",
    "\n",
    "@dataclass\n",
    "class CSConfig:\n",
    "    \"\"\"Main configuration\"\"\"\n",
    "    input: CSInput\n",
    "    output: CSOutput\n",
    "    compare: CSCompare\n",
    "\n",
    "#--- Parse command line arguments and load configuration ---#\n",
    "def parse_config() -> CSConfig:\n",
    "    parser = argparse.ArgumentParser(description='Conduct Column Stats Analysis')\n",
    "    parser.add_argument(\n",
    "        '--name', type=str,\n",
    "        default='test_debug',\n",
    "        help='how to name this analysis (override)'\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Load configuration from file\n",
    "    # Implementation depends on your config file format\n",
    "    raise NotImplementedError(\"Implement config loading\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Start logging session ---#\n",
    "def start_run():\n",
    "    logger.info('\\n\\n' + '=' * WIDTH)\n",
    "\n",
    "#--- End logging session ---#\n",
    "def end_run():\n",
    "    logger.info('\\n\\n' + '=' * WIDTH)\n",
    "\n",
    "#--- Load environment variables ---#\n",
    "def load_env(file):\n",
    "    if inWindows:\n",
    "        from dotenv import load_dotenv\n",
    "        load_dotenv(file)\n",
    "\n",
    "#--- Renew AWS credentials ---#\n",
    "def aws_creds_renew(seconds=0):\n",
    "    # Implement AWS credential renewal\n",
    "    pass\n",
    "\n",
    "#--- Check if statistics DataFrame is empty ---#\n",
    "def is_stat_empty(df: pd.DataFrame):\n",
    "    return df['col_count'].sum() == 0\n",
    "\n",
    "class IO:\n",
    "    \"\"\"File I/O utilities\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def write_json(file, data, cls=None):\n",
    "        with open(file, 'w') as f:\n",
    "            json.dump(data, f, indent=2, cls=cls)\n",
    "\n",
    "    @staticmethod\n",
    "    def read_json(file):\n",
    "        with open(file, 'r') as fp:\n",
    "            return json.load(fp)\n",
    "\n",
    "    @staticmethod\n",
    "    def read_meta_json(json_file):\n",
    "        \"\"\"Read metadata JSON and convert to MetaJSON objects\"\"\"\n",
    "        data = IO.read_json(json_file)\n",
    "        return {k: MetaJSON(**v) for k, v in data.items()}\n",
    "\n",
    "    @staticmethod\n",
    "    def delete_file(file):\n",
    "        if (filepath := UPath(file)).exists():\n",
    "            filepath.unlink()\n",
    "\n",
    "class S3:\n",
    "    \"\"\"S3 utilities\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def upload(df: pd.DataFrame, s3_url: UPath):\n",
    "        \"\"\"Upload DataFrame to S3 as parquet\"\"\"\n",
    "        import io\n",
    "        aws_creds_renew()\n",
    "        out_buffer = io.BytesIO()\n",
    "        df.to_parquet(out_buffer, index=False)\n",
    "        out_buffer.seek(0)\n",
    "        UPath(s3_url).write_bytes(out_buffer.getvalue())\n",
    "        logger.info(f\"Uploading DataFrame to {s3_url} [finished]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Date Handling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Calculate ISO week date range ---#\n",
    "def get_iso_week_dates(year, week):\n",
    "    jan01, dec31 = dt.datetime(year, 1, 1), dt.datetime(year, 12, 31)\n",
    "    first_day = jan01 - dt.timedelta(days=jan01.weekday())\n",
    "    start = first_day + dt.timedelta(weeks=week - 1)\n",
    "    end = start + dt.timedelta(days=6)\n",
    "    start, end = max(start, jan01), min(end, dec31)\n",
    "    return start.strftime('%Y-%m-%d'), end.strftime('%Y-%m-%d')\n",
    "\n",
    "#--- Parse date format specification from variable name ---#\n",
    "def parse_format_date(str_w_format):\n",
    "    pattern = r'^(.+?)(?:\\s*\\(([^)]+)\\))?$'\n",
    "    return re.match(pattern, str_w_format)\n",
    "\n",
    "#--- Convert date exclusion clause to appropriate SQL format ---#\n",
    "def parse_exclude_date(exclude_clause):\n",
    "    \"\"\"\n",
    "    Convert date exclusions between Oracle and Athena formats:\n",
    "    1. Oracle: TO_CHAR(EFF_DT, 'YYYY-MM-DD') not in ('2025-01-22','2025-01-24')\n",
    "       to EFF_DT NOT IN (TO_DATE('2025-01-22', 'YYYY-MM-DD'), TO_DATE('2025-01-24', 'YYYY-MM-DD'))\n",
    "    2. Athena: DATE_FORMAT(DATE_PARSE(dw_bus_dt, '%Y%m%d'), '%Y-%m-%d') not in ('2025-01-22', '2025-01-24')\n",
    "       to dw_bus_dt NOT IN ('20250122', '20250124')\n",
    "    \"\"\"\n",
    "    #>>> Handle Oracle format <<<#\n",
    "    p1 = r\"TO_CHAR\\((?P<col>\\w+),\\s*'YYYY-MM-DD'\\)\\s+(?P<op>not in|in)\\s+\\((?P<dates>.*?)\\)\"\n",
    "    if m := re.match(p1, exclude_clause, flags=re.I):\n",
    "        col, op, dates = m.groups()\n",
    "        new_dates = ', '.join(f\"DATE {date.strip()}\" for date in dates.split(','))\n",
    "        return '%s %s (%s)' % (col, op, new_dates)\n",
    "    \n",
    "    #>>> Handle Athena format <<<#\n",
    "    p2 = r\"DATE_FORMAT\\(DATE_PARSE\\((?P<col>\\w+),\\s*'(?P<fmt>%Y%m%d)'\\),\\s*'%Y-%m-%d'\\)\\s+(?P<op>not in|in)\\s+\\((?P<dates>.*?)\\)\"\n",
    "    if m := re.match(p2, exclude_clause, flags=re.I):\n",
    "        col, fmt, op, dates = m.groups()\n",
    "        new_dates = ', '.join(\n",
    "            \"'%s'\" % dt.datetime.strptime(date.strip(\"'\"), '%Y-%m-%d').strftime(fmt)\n",
    "            for date in dates.split(',')\n",
    "        )\n",
    "        return '%s %s (%s)' % (col, op, new_dates)\n",
    "    return exclude_clause\n",
    "\n",
    "#--- Build PCDS WHERE clause with date filtering ---#\n",
    "def get_pcds_where(date_var, date_type, date_partition, date_range, date_format, snapshot=None, exclude_clauses=[]):\n",
    "    #>>> Handle character-based dates <<<#\n",
    "    if date_type and ('char' in date_type.lower() or 'varchar' in date_type.lower()):\n",
    "        date_var = f\"TO_DATE({date_var}, '{date_format}')\"\n",
    "    \n",
    "    if snapshot:\n",
    "        return ' AND '.join(parse_exclude_date(x) for x in exclude_clauses if x)\n",
    "    elif date_partition == 'whole':\n",
    "        base_clause = \"1=1\"\n",
    "    elif date_partition == 'year':\n",
    "        start_dt = f\"TO_DATE('{date_range}-01-01', 'YYYY-MM-DD')\"\n",
    "        end_dt = f\"TO_DATE('{date_range}-12-31', 'YYYY-MM-DD')\"\n",
    "        base_clause = f\"{date_var} >= {start_dt} AND {date_var} <= {end_dt}\"\n",
    "    elif date_partition == 'year_month':\n",
    "        start_dt = f\"TO_DATE('{date_range}', 'YYYY-MM')\"\n",
    "        end_dt = f\"LAST_DAY(TO_DATE('{date_range}', 'YYYY-MM'))\"\n",
    "        base_clause = f\"{date_var} >= {start_dt} AND {date_var} <= {end_dt}\"\n",
    "    elif date_partition in ('year_week', 'week'):\n",
    "        year, week = date_range.split('-W')\n",
    "        start_dt, end_dt = get_iso_week_dates(int(year), int(week))\n",
    "        base_clause = f\"{date_var} >= DATE '{start_dt}' AND {date_var} <= DATE '{end_dt}'\"\n",
    "    elif date_partition == 'daily':\n",
    "        target_dt = f\"TO_DATE('{date_range}', 'YYYY-MM-DD')\"\n",
    "        base_clause = f\"{date_var} = {target_dt}\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported partition type: {date_partition}\")\n",
    "    \n",
    "    #>>> Add exclusions if provided <<<#\n",
    "    if (exclude_clauses := [x for x in exclude_clauses if x]):\n",
    "        exclude_clause = ' AND '.join(parse_exclude_date(x) for x in exclude_clauses if x)\n",
    "        return f\"({base_clause}) AND ({exclude_clause})\"\n",
    "    else:\n",
    "        return base_clause\n",
    "\n",
    "#--- Build AWS WHERE clause with date filtering ---#\n",
    "def get_aws_where(date_var, date_type, date_partition, date_range, date_format, snapshot=None, exclude_clauses=[]):\n",
    "    #>>> Handle variable=value format <<<#\n",
    "    if '=' in date_range:\n",
    "        _date_var, date_range = date_range.split('=', 1)\n",
    "        assert date_var.split()[0] == _date_var, f\"Date Variable Should Match: {date_var} vs {_date_var}\"\n",
    "    \n",
    "    #>>> Extract format from date_var if present <<<#\n",
    "    if (m := parse_format_date(date_var)):\n",
    "        date_var, date_format = m.groups()\n",
    "    \n",
    "    #>>> Handle string/varchar dates that need parsing <<<#\n",
    "    if date_type and re.match(r'^(string|varchar)', date_type, re.IGNORECASE):\n",
    "        if date_format:\n",
    "            date_var = f\"DATE_PARSE({date_var}, '{date_format}')\"\n",
    "        else:\n",
    "            date_var = f\"DATE_PARSE({date_var}, '%Y%m%d')\"\n",
    "    \n",
    "    if snapshot:\n",
    "        return ' AND '.join('(%s)' % parse_exclude_date(x) for x in exclude_clauses if x)\n",
    "    elif date_partition == 'whole':\n",
    "        base_clause = \"1=1\"\n",
    "    elif date_partition == 'year':\n",
    "        base_clause = f\"DATE_FORMAT({date_var}, '%Y') = '{date_range}'\"\n",
    "    elif date_partition == 'year_month':\n",
    "        base_clause = f\"DATE_FORMAT({date_var}, '%Y-%m') = '{date_range}'\"\n",
    "    elif date_partition in ('year_week', 'week'):\n",
    "        if '-W' in date_range:\n",
    "            year, week = date_range.split('-W')\n",
    "        else:\n",
    "            year, week = map(int, date_range.split('-'))\n",
    "            week = f\"W{week:02d}\"\n",
    "        base_clause = f\"DATE_FORMAT({date_var}, '%Y-%v') = '{year}-{week}'\"\n",
    "    elif date_partition == 'daily':\n",
    "        base_clause = f\"DATE({date_var}) = DATE('{date_range}')\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported partition type: {date_partition}\")\n",
    "    \n",
    "    if (exclude_clauses := [x for x in exclude_clauses if x]):\n",
    "        exclude_clause = ' AND '.join('(%s)' % parse_exclude_date(x) for x in exclude_clauses if x)\n",
    "        return f\"({base_clause}) AND ({exclude_clause})\"\n",
    "    else:\n",
    "        return base_clause\n",
    "\n",
    "#--- Query database to get available vintages (time periods) ---#\n",
    "def get_vintages_from_data(info_str, date_var, date_type, date_format, partition_type, where_clause=\"1=1\"):\n",
    "    \"\"\"\n",
    "    Get available time partitions from database based on partition type.\n",
    "    Returns list of vintage strings (e.g., '2024', '2024-01', '2024-W01') in reverse chronological order.\n",
    "    \"\"\"\n",
    "    # Implementation depends on your SQL engines\n",
    "    # This would query the database to find available date ranges\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: SQL Engine and Database Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas.io.sql as psql\n",
    "\n",
    "class SQLengine:\n",
    "    \"\"\"SQL query engine for PCDS and AWS\"\"\"\n",
    "    \n",
    "    def __init__(self, platform: Literal['PCDS', 'AWS']):\n",
    "        self._platform = platform\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self._where = None\n",
    "        self._type = None\n",
    "        self._date = None\n",
    "        self._dateraw = None\n",
    "        self._table = None\n",
    "\n",
    "    def query(self, query, connection, **query_kwargs):\n",
    "        \"\"\"Execute SQL query and return DataFrame\"\"\"\n",
    "        df = psql.read_sql_query(query, connection, **query_kwargs)\n",
    "        if self._platform == 'PCDS':\n",
    "            df.columns = [x.upper() for x in df.columns]\n",
    "        else:\n",
    "            df.columns = [x.lower() for x in df.columns]\n",
    "        return df\n",
    "\n",
    "    def execute_PCDS(self, query, service_name):\n",
    "        \"\"\"Execute PCDS PL/SQL block and parse DBMS_OUTPUT\"\"\"\n",
    "        from oracledb import STRING, NUMBER\n",
    "        query_stmt = query  # Cleaned query\n",
    "        \n",
    "        #>>> Connect and execute <<<#\n",
    "        # with pcds_connect(service_name=service_name) as CONN:\n",
    "        #     cursor = CONN.cursor()\n",
    "        #     cursor.callproc(\"dbms_output.enable\", [None])\n",
    "        #     cursor.execute(query_stmt)\n",
    "        #     ...\n",
    "        # Parse output into dictionary\n",
    "        raise NotImplementedError(\"Implement PCDS connection\")\n",
    "\n",
    "    def query_PCDS(self, query_stmt: str, service_name: str, **query_kwargs):\n",
    "        \"\"\"Execute query on PCDS\"\"\"\n",
    "        # with pcds_connect(service_name=service_name) as CONN:\n",
    "        #     return self.query(query_stmt, CONN, **query_kwargs)\n",
    "        raise NotImplementedError(\"Implement PCDS connection\")\n",
    "\n",
    "    def query_AWS(self, query_stmt: str, **query_kwargs):\n",
    "        \"\"\"Execute query on AWS Athena\"\"\"\n",
    "        import pyathena as pa\n",
    "        aws_creds_renew()\n",
    "        CONN = pa.connect(\n",
    "            s3_staging_dir=\"s3://355538383407-us-east-1-athena-output/uscb-analytics/\",\n",
    "            region_name=\"us-east-1\",\n",
    "        )\n",
    "        return self.query(query_stmt, CONN, **query_kwargs)\n",
    "\n",
    "# Create global instances\n",
    "proc_pcds = SQLengine('PCDS')\n",
    "proc_aws = SQLengine('AWS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Column Comparator - Main Analysis Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Parse date value with special handling for PCDS format ---#\n",
    "def parse_date_value(x, in_pcds=False, window=20):\n",
    "    if in_pcds and re.match(r'^\\d{2}-\\d{2}-\\d{4} \\d{2}:\\d{2}:\\d{2}$', x):\n",
    "        date_part = pd.to_datetime(x.split(\" \")[0], format='%d-%m-%Y', dayfirst=True)\n",
    "        if date_part.year > TODAY.year + window:\n",
    "            date_part = date_part.replace(year=date_part.year - 100)\n",
    "        return date_part.strftime(AWS_DT_FORMAT)\n",
    "    try:\n",
    "        return pd.to_datetime(x).strftime(AWS_DT_FORMAT)\n",
    "    except (AttributeError, ValueError, TypeError):\n",
    "        return str(x) if not pd.isna(x) else x\n",
    "\n",
    "class PsuedoLock:\n",
    "    \"\"\"Dummy lock for single-threaded execution\"\"\"\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        pass\n",
    "\n",
    "class ColumnComparator:\n",
    "    \"\"\"Enhanced column comparison engine for PCDS and AWS data\"\"\"\n",
    "    \n",
    "    parallel = 'process'\n",
    "    statistics = {\n",
    "        'col_type': 'Type',\n",
    "        'col_count': 'N_Total',\n",
    "        'col_distinct': 'N_Unique',\n",
    "        'col_missing': 'N_Missing',\n",
    "        'col_max': 'Max',\n",
    "        'col_min': 'Min',\n",
    "        'col_avg': 'Mean',\n",
    "        'col_std': 'Std',\n",
    "        'col_sum': 'Sum',\n",
    "        'col_sum_sq': 'Sum_Square',\n",
    "        'col_freq': 'Frequency'\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "        self.comparison_results = {}\n",
    "\n",
    "    def download_pcds_snapshot(self, info_str, columns, where_clause) -> pd.DataFrame:\n",
    "        \"\"\"Download PCDS table snapshot for backup\"\"\"\n",
    "        try:\n",
    "            proc_pcds = SQLengine('PCDS')\n",
    "            service, table_name = info_str.split('.')\n",
    "            columns = ', '.join(columns)\n",
    "            sql_stmt = f\"SELECT {columns} FROM {table_name} WHERE {where_clause}\"\n",
    "            logger.info(f\"Downloading PCDS table {table_name}\")\n",
    "            return proc_pcds.query_PCDS(sql_stmt, service_name=service)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in PCDS pulling: {e}\")\n",
    "            raise\n",
    "\n",
    "    def run_pcds_column_analysis(self, info_str: str, where_clause: str = \"1=1\") -> pd.DataFrame:\n",
    "        \"\"\"Run comprehensive column statistics on PCDS table\"\"\"\n",
    "        try:\n",
    "            proc_pcds = SQLengine('PCDS')\n",
    "            service, table_name = info_str.split('.')\n",
    "            where_clause = where_clause.replace('\\'', '\\'\\'')\n",
    "            sql_stmt = PCDS_SQL_COLUMN.format(table=table_name, limit=where_clause)\n",
    "            logger.info(f\"Executing PCDS analysis for {table_name}\")\n",
    "            result = proc_pcds.execute_PCDS(sql_stmt, service_name=service)\n",
    "            return pd.DataFrame(result).T\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in PCDS analysis: {e}\")\n",
    "            raise\n",
    "\n",
    "    @staticmethod\n",
    "    def run_aws_single(data_type, column_name, db, table, limit='', lock=None):\n",
    "        \"\"\"Run statistics on single AWS column\"\"\"\n",
    "        continuous_types = ('tinyint', 'smallint', 'integer', 'bigint', 'float', 'double', 'decimal')\n",
    "        \n",
    "        #>>> Handle timestamp columns <<<#\n",
    "        if re.match('^time', data_type, flags=re.I):\n",
    "            column_name = 'CAST(%s AS DATE)' % column_name\n",
    "        \n",
    "        #>>> Choose appropriate SQL template <<<#\n",
    "        is_continuous = any(ct in data_type.lower() for ct in continuous_types)\n",
    "        if is_continuous:\n",
    "            sql_template = AWS_Cont_SQL\n",
    "        else:\n",
    "            sql_template = AWS_Catg_SQL\n",
    "        \n",
    "        sql_stmt = sql_template.format(\n",
    "            db=db, table=table, column_name=column_name, data_type=data_type, limit=limit\n",
    "        )\n",
    "        \n",
    "        with lock:\n",
    "            aws_creds_renew()\n",
    "        \n",
    "        proc_aws = SQLengine('AWS')\n",
    "        return proc_aws.query_AWS(sql_stmt)\n",
    "\n",
    "    def run_aws_column_analysis(self, info_str: str, columns_info: dict, where_clause: str = \"1=1\", n_jobs=None) -> pd.DataFrame:\n",
    "        \"\"\"Run column analysis on AWS table with optional parallelization\"\"\"\n",
    "        results = {}\n",
    "        db_name, table_name = info_str.split('.')\n",
    "        logger.info(f\"Executing AWS analysis for {table_name}\")\n",
    "        worker = ft.partial(self.run_aws_single, db=db_name, table=table_name, limit=where_clause)\n",
    "        \n",
    "        try:\n",
    "            if n_jobs is None:\n",
    "                #>>> Sequential execution <<<#\n",
    "                locker = PsuedoLock()\n",
    "                for col_name, data_type in tqdm(columns_info.items()):\n",
    "                    results[col_name] = worker(data_type, col_name, lock=locker)\n",
    "            else:\n",
    "                #>>> Parallel execution <<<#\n",
    "                if self.parallel == \"thread\":\n",
    "                    executor_class = ThreadPoolExecutor\n",
    "                    locker = td.Lock()\n",
    "                elif self.parallel == \"process\":\n",
    "                    executor_class = ft.partial(ProcessPoolExecutor, mp_context=mp.get_context('spawn'))\n",
    "                    locker = mp.Manager().Lock()\n",
    "                \n",
    "                with executor_class(max_workers=n_jobs) as executor:\n",
    "                    futures = {}\n",
    "                    for col_name, data_type in tqdm(columns_info.items()):\n",
    "                        futures[executor.submit(worker, data_type, col_name, lock=locker)] = col_name\n",
    "                    \n",
    "                    for future in tqdm(as_completed(futures), total=len(futures), desc='Processing ... '):\n",
    "                        try:\n",
    "                            col_name = futures[future]\n",
    "                            results[col_name] = future.result()\n",
    "                        except Exception as e:\n",
    "                            logger.error(f\"Task failed: {e}\")\n",
    "                    executor.shutdown()\n",
    "            \n",
    "            df = pd.concat(results.values(), keys=results).droplevel(1)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in AWS analysis: {e}\")\n",
    "            raise\n",
    "\n",
    "    @staticmethod\n",
    "    def get_value(row: pd.Series, column: str, is_pcds: bool = False):\n",
    "        \"\"\"Extract and parse value from statistics row\"\"\"\n",
    "        value = row.get(column, np.nan) or np.nan\n",
    "        is_date = bool(re.match(r'^date|time', row['col_type'], re.I))\n",
    "        \n",
    "        #>>> Parse frequency distribution <<<#\n",
    "        if column == 'col_freq' and isinstance(value, str):\n",
    "            value = sorted([\n",
    "                m.groups() for s in value.split('; ')\n",
    "                if (m := re.search(r'([^(]*)\\((\\d+)\\)', s.strip()))\n",
    "            ], key=lambda x: (-int(x[1]), parse_date_value(x[0], is_pcds)))\n",
    "        \n",
    "        #>>> Try numeric conversion <<<#\n",
    "        try:\n",
    "            return float(value)\n",
    "        except (ValueError, TypeError):\n",
    "            try:\n",
    "                return int(value)\n",
    "            except (ValueError, TypeError):\n",
    "                return value\n",
    "\n",
    "    @staticmethod\n",
    "    def contains_datelike(dtype1, dtype2):\n",
    "        \"\"\"Check if either type is date/timestamp\"\"\"\n",
    "        return bool({dtype1.lower(), dtype2.lower()} & {'date', 'timestamp'})\n",
    "\n",
    "    def compare_statistics(\n",
    "        self,\n",
    "        pcds_stats: pd.DataFrame,\n",
    "        aws_stats: pd.DataFrame,\n",
    "        column_mapping: dict[str, str],\n",
    "        tokenised_cols: list\n",
    "    ) -> dict[str, any]:\n",
    "        \"\"\"Compare statistics between PCDS and AWS\"\"\"\n",
    "        mismatched_columns = set()\n",
    "        mismatched_details = {}\n",
    "        \n",
    "        #>>> Align columns based on mapping <<<#\n",
    "        aligned_pcds = pcds_stats.copy()\n",
    "        aligned_aws = aws_stats.copy()\n",
    "        \n",
    "        aligned_pcds = (\n",
    "            aligned_pcds\n",
    "            .drop(index=tokenised_cols)\n",
    "            .rename(index=column_mapping)\n",
    "        )\n",
    "        \n",
    "        #>>> Compare common columns <<<#\n",
    "        common_columns = set(aligned_pcds.index) & set(aligned_aws.index)\n",
    "        for column in common_columns:\n",
    "            pcds_row = aligned_pcds.loc[column]\n",
    "            aws_row = aligned_aws.loc[column]\n",
    "            column_diffs = {}\n",
    "            has_mismatch = False\n",
    "            \n",
    "            for stat, name in self.statistics.items():\n",
    "                if stat in ('col_type',):\n",
    "                    continue\n",
    "                \n",
    "                #>>> Skip date frequency comparison <<<#\n",
    "                if stat == 'col_freq' and self.contains_datelike(\n",
    "                    pcds_row['col_type'], aws_row['col_type']\n",
    "                ):\n",
    "                    continue\n",
    "                \n",
    "                pcds_val = self.get_value(pcds_row, stat, True)\n",
    "                aws_val = self.get_value(aws_row, stat)\n",
    "                \n",
    "                if self._values_different(pcds_val, aws_val):\n",
    "                    column_diffs[name] = {'pcds': pcds_val, 'aws': aws_val}\n",
    "                    has_mismatch = True\n",
    "            \n",
    "            if has_mismatch:\n",
    "                mismatched_columns.add(column)\n",
    "                mismatched_details[column] = column_diffs\n",
    "                logger.warning(f\"Mismatch found in column {column}: {column_diffs}\")\n",
    "        \n",
    "        #>>> Format output <<<#\n",
    "        pcds_stats = (\n",
    "            pcds_stats.loc[list(column_mapping)]\n",
    "            [list(self.statistics)]\n",
    "            .rename(columns=self.statistics)\n",
    "        )\n",
    "        aws_stats = (\n",
    "            aws_stats.loc[[v for k, v in column_mapping.items()]]\n",
    "            [list(self.statistics)]\n",
    "            .rename(columns=self.statistics)\n",
    "        )\n",
    "        \n",
    "        results = {\n",
    "            'mismatched_columns': mismatched_columns,\n",
    "            'mismatched_details': mismatched_details,\n",
    "            'total_columns': len(common_columns),\n",
    "            'matched_columns': len(common_columns) - len(mismatched_columns),\n",
    "        }\n",
    "        return results, pcds_stats, aws_stats\n",
    "\n",
    "    @staticmethod\n",
    "    def _values_different(val1, val2) -> bool:\n",
    "        \"\"\"Check if two values are different with tolerance\"\"\"\n",
    "        #>>> Handle list comparisons (frequency distributions) <<<#\n",
    "        if isinstance(val1, list):\n",
    "            flag = any(\n",
    "                ColumnComparator._values_different(x1, x2)\n",
    "                for t1, t2 in zip(val1, val2)\n",
    "                for x1, x2 in zip(t1, t2)\n",
    "            )\n",
    "            return flag\n",
    "        \n",
    "        #>>> Handle NaN values <<<#\n",
    "        if pd.isna(val1) and pd.isna(val2):\n",
    "            return False\n",
    "        if val1 == 0 and pd.isna(val2):\n",
    "            return False\n",
    "        if pd.isna(val1) ^ pd.isna(val2):\n",
    "            return True\n",
    "        \n",
    "        #>>> Try date comparison <<<#\n",
    "        try:\n",
    "            dat1 = parse_date_value(val1, in_pcds=True)\n",
    "            dat2 = parse_date_value(val2)\n",
    "            return dat1 != dat2\n",
    "        except (ValueError, TypeError):\n",
    "            #>>> Try numeric comparison <<<#\n",
    "            try:\n",
    "                num1, num2 = float(val1), float(val2)\n",
    "                return not np.isclose(num1, num2, atol=1e-6, rtol=1e-6)\n",
    "            except (ValueError, TypeError):\n",
    "                #>>> Fall back to string comparison <<<#\n",
    "                return str(val1) != str(val2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Excel Report Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XS:\n",
    "    \"\"\"Excel styling helper\"\"\"\n",
    "    \n",
    "    def __init__(self, ws: xw.Range):\n",
    "        self.ws = ws\n",
    "\n",
    "    def get_color(self, color):\n",
    "        if isinstance(color, str):\n",
    "            return get_rgb(color)\n",
    "        return color\n",
    "\n",
    "    def apply_styles(self, pos='A1', value='', font={}, align='left', color='', border={}):\n",
    "        \"\"\"Apply styles to a cell\"\"\"\n",
    "        cell = self.make_cell(pos)\n",
    "        if value:\n",
    "            cell.value = value\n",
    "        if font:\n",
    "            if 'family' in font:\n",
    "                cell.font.name = font['family']\n",
    "            if 'size' in font:\n",
    "                cell.font.size = font['size']\n",
    "            if 'color' in font:\n",
    "                cell.font.color = self.get_color(font['color'])\n",
    "            if 'bold' in font:\n",
    "                cell.font.bold = font['bold']\n",
    "        if color:\n",
    "            cell.color = self.get_color(color)\n",
    "        if 'style' in border:\n",
    "            cell.api.Borders.LineStyle = border['style']\n",
    "        if align == 'right':\n",
    "            cell.api.HorizontalAlignment = HAlign.xlHAlignRight\n",
    "        elif align == 'left':\n",
    "            cell.api.HorizontalAlignment = HAlign.xlHAlignLeft\n",
    "        elif align == 'center':\n",
    "            cell.api.HorizontalAlignment = HAlign.xlHAlignCenter\n",
    "\n",
    "    def make_cell(self, pos='A1', value=None):\n",
    "        cell = self.ws.range(pos)\n",
    "        if value:\n",
    "            cell.value = value\n",
    "        return cell\n",
    "\n",
    "    def write_dataframe(self, df: pd.DataFrame, pos='A1', header=True, index=False):\n",
    "        cell = self.make_cell(pos)\n",
    "        cell.options(index=index, header=header).value = df\n",
    "        return cell\n",
    "\n",
    "class ExcelReporter:\n",
    "    \"\"\"Excel reporter for column comparison results\"\"\"\n",
    "    \n",
    "    def __init__(self, workbook_path: str):\n",
    "        self.workbook_path = UPath(workbook_path)\n",
    "        self.app = None\n",
    "        self.wb = None\n",
    "        self.ns = -1\n",
    "        self.cx, self.cy = None, None\n",
    "\n",
    "    def __enter__(self):\n",
    "        try:\n",
    "            self.workbook_path.unlink(True)\n",
    "        except PermissionError:\n",
    "            xw.Book(self.workbook_path).close()\n",
    "        self.app = xw.App(visible=True, add_book=False)\n",
    "        self.wb = self.app.books.add()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        if self.wb:\n",
    "            self.wb.save(str(self.workbook_path))\n",
    "\n",
    "    def create_comparison_report(self, comparison_results: dict[str, dict[str, CSResult]]):\n",
    "        \"\"\"Create Excel report with comparison results\"\"\"\n",
    "        self._create_summary_sheet(comparison_results)\n",
    "        for dataset_name, dataset_data in comparison_results.items():\n",
    "            self._create_dataset_sheet(dataset_name, dataset_data)\n",
    "\n",
    "    def _create_summary_sheet(self, comparison_results):\n",
    "        \"\"\"Create summary sheet with overview\"\"\"\n",
    "        ws = self.wb.sheets[0]\n",
    "        ws.name = 'SUMMARY'\n",
    "        ws.range('A1').value = 'Column Statistics Comparison'\n",
    "        ws.range('A1').font.bold = True\n",
    "        headers = ['Dataset', 'Vintage', 'Total Columns', 'Matched Columns', 'Mismatched Columns', 'Match Rate %']\n",
    "        ws.range('A3').value = headers\n",
    "        ws.range('A3:F3').font.bold = True\n",
    "        ws.range('A3:F3').color = (200, 200, 200)\n",
    "        \n",
    "        row = 4\n",
    "        for dataset_name, dataset_data in comparison_results.items():\n",
    "            for vintage, data in dataset_data.items():\n",
    "                pcds_stats = data.pcds_stats\n",
    "                matched = (total_cols := len(pcds_stats)) - (mismatched := len(data.miss_columns))\n",
    "                match_rate = (matched / total_cols * 100) if total_cols > 0 else 0\n",
    "                ws.range(f'A{row}').value = [dataset_name, vintage, total_cols, matched, mismatched, f'{match_rate:.1f}%']\n",
    "                if match_rate >= 95:\n",
    "                    ws.range(f'A{row}:F{row}').color = (200, 255, 200)\n",
    "                else:\n",
    "                    ws.range(f'A{row}:F{row}').color = (255, 200, 200)\n",
    "                row += 1\n",
    "        ws.autofit()\n",
    "        self.ns += 1\n",
    "\n",
    "    def _create_dataset_sheet(self, name: str, result_d: dict[str, CSResult]):\n",
    "        \"\"\"Create detailed sheet for a specific dataset\"\"\"\n",
    "        wb = self.wb\n",
    "        try:\n",
    "            ws = wb.sheets.add(name.upper(), after=wb.sheets[self.ns])\n",
    "        except ValueError:\n",
    "            ws = wb.sheets[name.upper()]\n",
    "        finally:\n",
    "            ws.clear()\n",
    "        xs = XS(ws)\n",
    "        \n",
    "        row = 1\n",
    "        for vintage, data in result_d.items():\n",
    "            #>>> Write vintage header <<<#\n",
    "            xs.make_cell(pos=f'A{row}', value='Vintage: ')\n",
    "            xs.apply_styles(pos=f'B{row}', value=vintage, align='right')\n",
    "            ws.range(f'B{row}:D{row}').merge()\n",
    "            xs.apply_styles(f'A{row}:D{row}', font={'bold': True}, color=(190, 190, 190))\n",
    "            row += 2\n",
    "            \n",
    "            #>>> Write PCDS statistics <<<#\n",
    "            pcds_tbl = data.meta_data.pcds_table.split('.')[-1]\n",
    "            xs.make_cell(pos=f'A{row}', value='PCDS: ')\n",
    "            xs.apply_styles(pos=f'B{row}', value=pcds_tbl, align='right')\n",
    "            ws.range(f'B{row}:D{row}').merge()\n",
    "            xs.apply_styles(f'A{row}:D{row}', font={'bold': True}, color=(240, 240, 240))\n",
    "            row += 1\n",
    "            \n",
    "            #>>> Reorder columns to show mismatches first <<<#\n",
    "            aws_view = data.aws_stats.T.map(self._format_cell_value)\n",
    "            indices = [i for i, x in enumerate(aws_view) if x in data.miss_columns]\n",
    "            the_rest = [i for i in range(len(aws_view.columns)) if i not in indices]\n",
    "            aws_view = aws_view[aws_view.columns[indices + the_rest]]\n",
    "            pcds_view = data.pcds_stats.T.map(self._format_cell_value)\n",
    "            pcds_view = pcds_view[pcds_view.columns[indices + the_rest]]\n",
    "            \n",
    "            self.cx, self.cy = 2, row + 1\n",
    "            xs.write_dataframe(pcds_view, f'B{row}', index=True)\n",
    "            \n",
    "            #>>> Write AWS statistics <<<#\n",
    "            row += len(pcds_view) + 2\n",
    "            xs.make_cell(pos=f'A{row}', value='AWS: ')\n",
    "            aws_tbl = data.meta_data.aws_table.lower()\n",
    "            xs.apply_styles(pos=f'B{row}', value=aws_tbl, align='right')\n",
    "            ws.range(f'B{row}:D{row}').merge()\n",
    "            xs.apply_styles(f'A{row}:D{row}', font={'bold': True}, color=(240, 240, 240))\n",
    "            row += 1\n",
    "            xs.write_dataframe(aws_view, f'B{row}', index=True)\n",
    "            row += len(aws_view) + 3\n",
    "            \n",
    "            self._highlight_differences(ws, nx=len(indices), ny=len(pcds_view) - 1)\n",
    "            row += 2\n",
    "        ws.autofit()\n",
    "\n",
    "    def _highlight_differences(self, ws: xw.Sheet, nx: int, ny: int):\n",
    "        \"\"\"Highlight differences between PCDS and AWS\"\"\"\n",
    "        ix, iy = self.cx, self.cy\n",
    "        for i in range(iy, iy + ny):\n",
    "            for j in range(ix, ix + nx):\n",
    "                pcds, aws = ws[i, j], ws[i + ny + 4, j]\n",
    "                pcds.number_format = '0.00'\n",
    "                aws.number_format = '0.00'\n",
    "                if pcds.value == aws.value:\n",
    "                    pcds.font.color = get_rgb('green')\n",
    "                    aws.font.color = get_rgb('green')\n",
    "                else:\n",
    "                    pcds.font.color = get_rgb('red')\n",
    "                    aws.font.color = get_rgb('red')\n",
    "\n",
    "    def _format_cell_value(self, value) -> str:\n",
    "        \"\"\"Format cell value for display\"\"\"\n",
    "        if pd.isna(value):\n",
    "            return ''\n",
    "        elif isinstance(value, (int, float)):\n",
    "            if isinstance(value, float) and value.is_integer():\n",
    "                return str(int(value))\n",
    "            return str(value)\n",
    "        else:\n",
    "            str_val = str(value)\n",
    "            return str_val[:50] + '...' if len(str_val) > 50 else str_val\n",
    "\n",
    "#--- Create comparison report from results dictionary ---#\n",
    "def create_comparison_report(comparison_results: dict[dict[str, CSResult]], output_path: UPath):\n",
    "    with ExcelReporter(output_path) as reporter:\n",
    "        reporter.create_comparison_report(comparison_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Main Execution Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main execution function for column statistics analysis\"\"\"\n",
    "    global record\n",
    "    config = parse_config()\n",
    "    C_out, C_in, C_cmp = config.output, config.input, config.compare\n",
    "    logger.info('Configuration:\\n' + str(config))\n",
    "    \n",
    "    #>>> Load environment and setup <<<#\n",
    "    load_env(C_in.env)\n",
    "    # start_setup(C_out)  # Implement based on needs\n",
    "    start_run()\n",
    "    IO.delete_file(C_out.csv.file)\n",
    "    aws_creds_renew(15 * 60)\n",
    "    \n",
    "    #>>> Load metadata from previous meta analysis <<<#\n",
    "    meta_json = IO.read_meta_json(C_in.json)\n",
    "    meta_csv = pd.read_csv(C_in.csv)\n",
    "    \n",
    "    HAS_HEADER = False\n",
    "    ALL_RESULT = {}\n",
    "    CC = ColumnComparator()\n",
    "    \n",
    "    for i, row in tqdm(meta_csv.iterrows(), desc='Processing ...', total=len(meta_csv)):\n",
    "        name = row.get('PCDS Table Details with DB Name')\n",
    "        logger.info(f\"Processing dataset: {name}\")\n",
    "        \n",
    "        #>>> Load metadata for this table <<<#\n",
    "        meta_info = meta_json.get(name)\n",
    "        meta_pcds, meta_aws = meta_info.pcds, meta_info.aws\n",
    "        \n",
    "        #>>> Remove PII and tokenized columns <<<#\n",
    "        avai_cols = [x for x in meta_pcds.col2COL if x not in C_cmp.exclude]\n",
    "        meta_pcds.update(\n",
    "            col2COL={k: meta_pcds.col2COL[k] for k in avai_cols},\n",
    "            col2type={k: meta_pcds.col2type[k] for k in avai_cols}\n",
    "        )\n",
    "        meta_aws.update(\n",
    "            col2COL={v: k for k, v in meta_pcds.col2COL.items()},\n",
    "            col2type={\n",
    "                k: v for k, v in meta_aws.col2type.items()\n",
    "                if k in meta_pcds.col2COL.values()\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        DATA_RESULT = {}\n",
    "        record = SQLRecord(name=name)\n",
    "        \n",
    "        #>>> Determine time partitions to analyze <<<#\n",
    "        if (partition := meta_info.partition) == 'empty':\n",
    "            continue\n",
    "        elif partition in get_args(TPartition):\n",
    "            vintages = get_vintages_from_data(\n",
    "                info_str=meta_aws.infostr,\n",
    "                date_var=meta_aws.rowvar,\n",
    "                date_type=meta_aws.rowtype,\n",
    "                date_format='%y-%m-%d',\n",
    "                partition_type=partition,\n",
    "                where_clause=meta_aws.where\n",
    "            )\n",
    "        else:\n",
    "            vintages = ['entire_dataset']\n",
    "        \n",
    "        #>>> Process each vintage/time period <<<#\n",
    "        for vintage in vintages:\n",
    "            logger.info(f\"Processing vintage: {vintage}\")\n",
    "            \n",
    "            #>>> Build WHERE clauses for this vintage <<<#\n",
    "            pcds_where = get_pcds_where(\n",
    "                date_var=meta_pcds.rowvar,\n",
    "                date_type=meta_pcds.rowtype,\n",
    "                date_partition=partition,\n",
    "                date_range=vintage,\n",
    "                date_format='YYYY-MM-DD',\n",
    "                snapshot=partition == 'snapshot',\n",
    "                exclude_clauses=[meta_pcds.where, meta_pcds.rowexclude]\n",
    "            )\n",
    "            \n",
    "            rowvar = meta_aws.rowvar\n",
    "            aws_where = get_aws_where(\n",
    "                date_var=rowvar,\n",
    "                date_type=meta_aws.rowtype,\n",
    "                date_partition=partition,\n",
    "                date_range='%s=%s' % (re.sub(r\"\\s*\\(.*?\\)$\", \"\", rowvar), vintage) if vintage != 'entire_dataset' else vintage,\n",
    "                date_format='%Y%m%d',\n",
    "                snapshot=partition == 'snapshot',\n",
    "                exclude_clauses=[meta_aws.where, meta_aws.rowexclude]\n",
    "            )\n",
    "            \n",
    "            os.environ['SKIP_SAVE'] = 'N'\n",
    "            if os.environ.get('SKIP_SNAPSHOT') and partition == 'snapshot':\n",
    "                os.environ['SKIP_SAVE'] = 'Y'\n",
    "            \n",
    "            #>>> Compute column statistics <<<#\n",
    "            with Timer() as timer:\n",
    "                pcds_stats = CC.run_pcds_column_analysis(\n",
    "                    meta_pcds.infostr, pcds_where\n",
    "                )\n",
    "                pcds_time = timer.pause()\n",
    "            \n",
    "            with Timer() as timer:\n",
    "                aws_stats = CC.run_aws_column_analysis(\n",
    "                    meta_aws.infostr, meta_aws.col2type, aws_where, C_cmp.n_process\n",
    "                )\n",
    "                aws_time = timer.pause()\n",
    "            \n",
    "            if is_stat_empty(pcds_stats) or is_stat_empty(aws_stats):\n",
    "                continue\n",
    "            \n",
    "            #>>> Compare statistics <<<#\n",
    "            comparison_result, pcds_stats, aws_stats = CC.compare_statistics(\n",
    "                pcds_stats, aws_stats, meta_pcds.col2COL, meta_info.tokenised_cols\n",
    "            )\n",
    "            \n",
    "            pcds_stats = pcds_stats.loc[list(meta_pcds.col2COL)]\n",
    "            aws_stats = aws_stats.loc[[v for _, v in meta_pcds.col2COL.items()]]\n",
    "            \n",
    "            DATA_RESULT[vintage] = CSResult(\n",
    "                pcds_stats=pcds_stats,\n",
    "                aws_stats=aws_stats,\n",
    "                miss_columns=comparison_result['mismatched_columns'],\n",
    "                miss_details=comparison_result['mismatched_details'],\n",
    "                meta_data=CSMeta(\n",
    "                    pcds_table=meta_pcds.infostr,\n",
    "                    aws_table=meta_aws.infostr,\n",
    "                    partition=partition,\n",
    "                    vintage=vintage,\n",
    "                    pcds_time=pcds_time,\n",
    "                    aws_time=aws_time,\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            #>>> Upload PCDS snapshot if needed <<<#\n",
    "            if partition == 'snapshot' and len(comparison_result['mismatched_columns']) > 0:\n",
    "                df_pcds = CC.download_pcds_snapshot(\n",
    "                    meta_pcds.infostr,\n",
    "                    columns=[meta_aws.col2COL[x] for x in comparison_result['mismatched_columns']],\n",
    "                    where_clause=pcds_where\n",
    "                )\n",
    "                S3.upload(df_pcds, 's3://355538383407-edpss/pcds_tables/PCDS_{tbl}_snap/{part}.pq'.format(\n",
    "                    tbl=name.upper(), part='today=%s' % TODAY.strftime(AWS_DT_FORMAT)\n",
    "                ))\n",
    "                logger.info(f\"Uploading dataset with issues to S3: {name}\")\n",
    "            \n",
    "            record.update(\n",
    "                unmatched=comparison_result['mismatched_columns'],\n",
    "                nrow=int(pcds_stats['N_Total'].max()),\n",
    "                ncol=comparison_result['total_columns'],\n",
    "                pcds_time=pcds_time,\n",
    "                aws_time=aws_time\n",
    "            )\n",
    "            logger.info(f\"Vintage {vintage}: {comparison_result['matched_columns']}/{comparison_result['total_columns']} columns matched\")\n",
    "        \n",
    "        ALL_RESULT[name] = DATA_RESULT\n",
    "        \n",
    "        #>>> Write results to CSV <<<#\n",
    "        with open(C_out.csv.file, 'a+', newline='') as fp:\n",
    "            writer = csv.DictWriter(fp, fieldnames=C_out.csv.columns)\n",
    "            if not HAS_HEADER:\n",
    "                writer.writeheader()\n",
    "                HAS_HEADER = True\n",
    "            row_dict = {k: v for k, v in row.items() if k in C_out.csv.columns}\n",
    "            writer.writerow({**record.toJSON(), **row_dict})\n",
    "        \n",
    "        #>>> Reset engines <<<#\n",
    "        proc_aws.reset()\n",
    "        proc_pcds.reset()\n",
    "    \n",
    "    #>>> Save results to JSON <<<#\n",
    "    IO.write_json(C_out.json, {\n",
    "        dataset: {\n",
    "            vintage: {\n",
    "                'mismatched_columns': list(data.miss_columns),\n",
    "                'mismatched_details': data.miss_details,\n",
    "                'metadata': data.meta_data.todict()\n",
    "            } for vintage, data in vintages.items()\n",
    "        } for dataset, vintages in ALL_RESULT.items()\n",
    "    })\n",
    "    \n",
    "    #>>> Create Excel report <<<#\n",
    "    create_comparison_report(ALL_RESULT, C_out.xlsx)\n",
    "    logger.info(f\"Excel report created: {C_out.xlsx}\")\n",
    "    \n",
    "    end_run()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Analysis\n",
    "\n",
    "Uncomment the cell below to run the full analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
