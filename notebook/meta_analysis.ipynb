{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Analysis - Table Comparison Between PCDS and AWS\n",
    "\n",
    "This notebook performs meta-level analysis comparing tables between PCDS (Oracle) and AWS (Athena) databases.\n",
    "It checks:\n",
    "- Column mappings and data types\n",
    "- Row counts and date ranges\n",
    "- Schema differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import shutil\n",
    "import pickle\n",
    "import argparse\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyathena as pa\n",
    "import pandas.io.sql as psql\n",
    "import awswrangler as aws\n",
    "import boto3\n",
    "\n",
    "from upath import UPath\n",
    "from loguru import logger\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from dataclasses import dataclass, field, fields, is_dataclass\n",
    "from configparser import ConfigParser\n",
    "from confection import Config\n",
    "from unittest import mock\n",
    "from enum import Enum\n",
    "from typing import Literal, Dict, List\n",
    "from collections import defaultdict, abc\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message='.*pandas only supports SQLAlchemy connectable.*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Constants and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Global Constants ---\n",
    "SEP = '; '\n",
    "PCDS_DT_FORMAT = 'YYYY-MM-DD'\n",
    "AWS_DT_FORMAT = '%Y-%m-%d'\n",
    "TODAY = datetime.now()\n",
    "ONEDAY = timedelta(days=1)\n",
    "WIDTH = 80\n",
    "NO_DATE = 'no_date_provided'\n",
    "inWindows = os.name == 'nt'\n",
    "SESSION = None\n",
    "\n",
    "class PullStatus(Enum):\n",
    "    \"\"\"Enumeration for data pull status codes\"\"\"\n",
    "    NONEXIST_PCDS = 'Nonexisting PCDS Table'\n",
    "    NONEXIST_AWS = 'Nonexisting AWS Table'\n",
    "    NONDATE_PCDS = 'Nonexisting Date Variable in PCDS'\n",
    "    NONDATE_AWS = 'Nonexisting Date Variable in AWS'\n",
    "    EMPTY_PCDS = 'Empty PCDS Table'\n",
    "    EMPTY_AWS = 'Empty AWS Table'\n",
    "    NO_MAPPING = 'Column Mapping Not Provided'\n",
    "    SUCCESS = 'Successful Data Access'\n",
    "\n",
    "# --- SQL Templates for PCDS (Oracle) ---\n",
    "PCDS_SQL_META = \"\"\"\n",
    "select\n",
    "    column_name,\n",
    "    data_type || case\n",
    "    when data_type = 'NUMBER' then \n",
    "        case when data_precision is NULL AND data_scale is NULL\n",
    "            then NULL\n",
    "        else\n",
    "            '(' || TO_CHAR(data_precision) || ',' || TO_CHAR(data_scale) || ')'\n",
    "        end\n",
    "    when data_type LIKE '%CHAR%'\n",
    "        then\n",
    "            '(' || TO_CHAR(data_length) || ')'\n",
    "        else NULL\n",
    "    end AS data_type\n",
    "from all_tab_cols\n",
    "where table_name = UPPER('{table}')\n",
    "order by column_id\n",
    "\"\"\"\n",
    "\n",
    "PCDS_SQL_NROW = \"\"\"\n",
    "SELECT COUNT(*) AS nrow FROM {table}\n",
    "where {limit}\n",
    "\"\"\"\n",
    "\n",
    "PCDS_SQL_DATE = \"\"\"\n",
    "SELECT {date}, count(*) AS NROWS\n",
    "FROM {table} \n",
    "WHERE {limit}\n",
    "GROUP BY {date}\n",
    "\"\"\"\n",
    "\n",
    "# --- SQL Templates for AWS (Athena) ---\n",
    "AWS_SQL_META = \"\"\"\n",
    "select column_name, data_type from information_schema.columns\n",
    "where table_schema = LOWER('{db}') and table_name = LOWER('{table}')\n",
    "\"\"\"\n",
    "\n",
    "AWS_SQL_NROW = \"\"\"\n",
    "SELECT COUNT(*) AS nrow FROM {db}.{table}\n",
    "where {limit}\n",
    "\"\"\"\n",
    "\n",
    "AWS_SQL_DATE = \"\"\"\n",
    "SELECT {date}, count(*) AS nrows\n",
    "FROM {db}.{table} \n",
    "WHERE {limit}\n",
    "GROUP BY {date}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Exception Classes and Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Custom Exceptions ---\n",
    "class NONEXIST_TABLE(Exception):\n",
    "    \"\"\"Exception raised when database view does not exist\"\"\"\n",
    "    pass\n",
    "\n",
    "class NONEXIST_DATEVAR(Exception):\n",
    "    \"\"\"Exception raised when no date-like variable exists\"\"\"\n",
    "    pass\n",
    "\n",
    "# --- Helper Functions for Configuration Reading ---\n",
    "def read_str_lst(lst_str, sep='\\n'):\n",
    "    \"\"\"Parse newline-separated string into list\"\"\"\n",
    "    return [x for x in lst_str.strip().split(sep) if x]\n",
    "\n",
    "def read_dstr_lst(dct_str, sep='='):\n",
    "    \"\"\"Parse key=value pairs into dictionary\"\"\"\n",
    "    d = dict(line.split(sep, 1) for line in read_str_lst(dct_str))\n",
    "    return {k.strip(): v.strip() for k, v in d.items()}\n",
    "\n",
    "# --- Base Type Class ---\n",
    "class BaseType:\n",
    "    \"\"\"Base class with logging and nested dataclass support\"\"\"\n",
    "    def __post_init__(self):\n",
    "        for _field in fields(self):\n",
    "            if is_dataclass(_field.type):\n",
    "                field_val = _field.type(**getattr(self, _field.name))\n",
    "                setattr(self, _field.name, field_val)\n",
    "\n",
    "    def tolog(self, indent=1, padding=''):\n",
    "        \"\"\"Convert dataclass to formatted string for logging\"\"\"\n",
    "        import pprint as pp\n",
    "        def get_val(x, pad):\n",
    "            if isinstance(x, BaseType):\n",
    "                return x.tolog(indent, pad)\n",
    "            elif isinstance(x, Dict):\n",
    "                return pp.pformat(x, indent)\n",
    "            else:\n",
    "                return repr(x)\n",
    "        cls_name = self.__class__.__name__\n",
    "        padding = padding + '\\t' * indent\n",
    "        fields_str = [f'{padding}{k}={get_val(v, padding)}' for k, v in vars(self).items()]\n",
    "        return f'{cls_name}(\\n' + ',\\n'.join(fields_str) + '\\n)'\n",
    "\n",
    "# --- Configuration Dataclasses ---\n",
    "@dataclass\n",
    "class MetaRange:\n",
    "    \"\"\"Range configuration for row selection\"\"\"\n",
    "    start_rows: int | None\n",
    "    end_rows: int | None\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield from [self.start_rows or 1, self.end_rows or float('inf')]\n",
    "\n",
    "@dataclass\n",
    "class MetaTable(BaseType):\n",
    "    \"\"\"Excel table configuration\"\"\"\n",
    "    file: UPath\n",
    "    sheet: str\n",
    "    skip_rows: int\n",
    "    select_cols: dict\n",
    "    select_rows: dict\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.select_cols = read_dstr_lst(self.select_cols)\n",
    "        self.select_rows = read_str_lst(str(self.select_rows))\n",
    "\n",
    "@dataclass\n",
    "class MetaInput(BaseType):\n",
    "    \"\"\"Input configuration\"\"\"\n",
    "    name: str\n",
    "    step: str\n",
    "    env: str\n",
    "    range: MetaRange\n",
    "    category: Literal['loan', 'dpst']\n",
    "    clear_cache: bool = True\n",
    "    table: MetaTable = None\n",
    "\n",
    "@dataclass\n",
    "class MetaCSV:\n",
    "    \"\"\"CSV output configuration\"\"\"\n",
    "    file: UPath\n",
    "    columns: str\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.columns = read_str_lst(self.columns)\n",
    "\n",
    "@dataclass\n",
    "class S3Config:\n",
    "    \"\"\"S3 path configuration\"\"\"\n",
    "    run: UPath\n",
    "    data: UPath\n",
    "\n",
    "@dataclass\n",
    "class LogConfig:\n",
    "    \"\"\"Logging configuration\"\"\"\n",
    "    level: Literal['info', 'warning', 'debug', 'error']\n",
    "    format: str\n",
    "    file: str\n",
    "    overwrite: bool\n",
    "\n",
    "    def todict(self):\n",
    "        return {\n",
    "            'level': self.level.upper(),\n",
    "            'format': self.format,\n",
    "            'sink': self.file,\n",
    "            'mode': 'w' if self.overwrite else 'a'\n",
    "        }\n",
    "\n",
    "@dataclass\n",
    "class NextConfig:\n",
    "    \"\"\"Next step configuration\"\"\"\n",
    "    file: UPath\n",
    "    fields: str\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.fields = read_dstr_lst(self.fields)\n",
    "\n",
    "@dataclass\n",
    "class CacheConfig:\n",
    "    \"\"\"Cache configuration (not used in this notebook)\"\"\"\n",
    "    enable: bool\n",
    "    directory: UPath\n",
    "    expire_hours: int = None\n",
    "    force_restart: bool = False\n",
    "    verbose: bool = False\n",
    "\n",
    "@dataclass\n",
    "class MetaOutput(BaseType):\n",
    "    \"\"\"Output configuration\"\"\"\n",
    "    folder: UPath\n",
    "    to_pkl: UPath\n",
    "    csv: MetaCSV\n",
    "    to_s3: S3Config\n",
    "    log: LogConfig\n",
    "    next: NextConfig\n",
    "    cache: CacheConfig\n",
    "\n",
    "@dataclass\n",
    "class MetaMatch:\n",
    "    \"\"\"Column matching configuration\"\"\"\n",
    "    candidates: str\n",
    "    drop_cols: dict\n",
    "    add_cols: dict\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.candidates = read_str_lst(self.candidates)\n",
    "        self.drop_cols = list(self.drop_cols)\n",
    "        self.add_cols = list(self.add_cols)\n",
    "\n",
    "@dataclass\n",
    "class ColumnMap(BaseType):\n",
    "    \"\"\"Column mapping configuration\"\"\"\n",
    "    output: UPath\n",
    "    input: UPath\n",
    "    na_str: str\n",
    "    overwrite: bool\n",
    "    excludes: list[str]\n",
    "    pcds_col: str\n",
    "    aws_col: str\n",
    "    pcds_view: str\n",
    "    aws_view: str\n",
    "\n",
    "    def __post_init__(self):\n",
    "        def transform(p):\n",
    "            if isinstance(p, str):\n",
    "                return ['_'.join(x for x in c.split()) for c in read_str_lst(p)]\n",
    "            return p\n",
    "        self.pcds_col = transform(self.pcds_col)\n",
    "        self.pcds_view = transform(self.pcds_view)\n",
    "        self.aws_col = transform(self.aws_col)\n",
    "        self.aws_view = transform(self.aws_view)\n",
    "        if '_+_' in self.aws_view[0]:\n",
    "            self.aws_view = '.'.join('{%s}' % x.lower() for x in self.aws_view[0].split('_+_'))\n",
    "        self.excludes = [x] if isinstance(self.excludes, str) else list(self.excludes) if self.excludes else []\n",
    "\n",
    "@dataclass\n",
    "class MetaConfig(BaseType):\n",
    "    \"\"\"Main configuration class\"\"\"\n",
    "    input: MetaInput\n",
    "    output: MetaOutput\n",
    "    match: MetaMatch\n",
    "    column_maps: ColumnMap\n",
    "\n",
    "@dataclass\n",
    "class MetaRecord:\n",
    "    \"\"\"Record tracking during processing\"\"\"\n",
    "    next_d: dict = field(default_factory=dict)\n",
    "    col_maps: dict = field(default_factory=dict)\n",
    "    pull_status: PullStatus = None\n",
    "\n",
    "@dataclass\n",
    "class MetaMerge:\n",
    "    \"\"\"Results from merging PCDS and AWS column metadata\"\"\"\n",
    "    unique_pcds: list\n",
    "    unique_aws: list\n",
    "    col_mapping: pd.DataFrame\n",
    "    mismatches: str\n",
    "    uncaptured: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Configuration Reading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Patch confection library to preserve case sensitivity ---#\n",
    "def patch_confection():\n",
    "    def get_configparser(interpolate: bool = True):\n",
    "        from confection import CustomInterpolation\n",
    "        config = ConfigParser(\n",
    "            interpolation=CustomInterpolation() if interpolate else None,\n",
    "            allow_no_value=True,\n",
    "        )\n",
    "        config.optionxform = str\n",
    "        return config\n",
    "    mock_obj = mock.patch('confection.get_configparser', wraps=get_configparser)\n",
    "    if not hasattr(mock_obj, 'is_local'):\n",
    "        mock_obj.start()\n",
    "\n",
    "#--- Read configuration file and create config object ---#\n",
    "def read_config(config_class: BaseType, config_path: None | UPath | str = None, overrides={}):\n",
    "    patch_confection()\n",
    "    if UPath(config_path).is_file():\n",
    "        config = Config().from_disk(config_path, overrides=overrides)\n",
    "    else:\n",
    "        config = Config().from_str(config_path, overrides=overrides)\n",
    "    return config_class(**{**config.pop('root', {}), **config})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Utility Classes and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Start logging session with separator ---#\n",
    "def start_run():\n",
    "    logger.info('\\n\\n' + '=' * WIDTH)\n",
    "\n",
    "#--- End logging session with separator ---#\n",
    "def end_run():\n",
    "    logger.info('\\n\\n' + '=' * WIDTH)\n",
    "\n",
    "#--- Load environment variables from file ---#\n",
    "def load_env(file):\n",
    "    inWindows and load_dotenv(file)\n",
    "\n",
    "#--- Renew AWS credentials from temporary session ---#\n",
    "def aws_creds_renew(seconds=0, delta=0, force=False, msg='AWS Credential Has Been Updated !'):\n",
    "    # Placeholder - implement based on your credential management\n",
    "    # This would typically fetch temporary credentials from AWS STS\n",
    "    pass\n",
    "\n",
    "class IO:\n",
    "    \"\"\"File I/O utility class\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def write_json(file, data, cls=None):\n",
    "        \"\"\"Write dictionary to JSON file\"\"\"\n",
    "        with open(file, 'w') as f:\n",
    "            json.dump(data, f, indent=2, cls=cls)\n",
    "\n",
    "    @staticmethod\n",
    "    def write_pickle(file, data):\n",
    "        \"\"\"Pickle data to file\"\"\"\n",
    "        with open(file, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "\n",
    "    @staticmethod\n",
    "    def read_json(file):\n",
    "        \"\"\"Read JSON file into dictionary\"\"\"\n",
    "        with open(file, 'r') as fp:\n",
    "            data = json.load(fp)\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def read_pickle(file):\n",
    "        \"\"\"Unpickle data from file\"\"\"\n",
    "        with open(file, 'rb') as fp:\n",
    "            data = pickle.load(fp)\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def delete_file(file):\n",
    "        \"\"\"Delete file if it exists\"\"\"\n",
    "        if (filepath := UPath(file)).exists():\n",
    "            filepath.unlink()\n",
    "\n",
    "class UDict(dict):\n",
    "    \"\"\"Case-insensitive dictionary for flexible key matching\"\"\"\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        return super().__getitem__(self._match(key))\n",
    "    \n",
    "    def __contains__(self, key):\n",
    "        try:\n",
    "            self._match(key)\n",
    "            return True\n",
    "        except KeyError:\n",
    "            return False\n",
    "\n",
    "    def _match(self, key):\n",
    "        \"\"\"Find matching key regardless of case\"\"\"\n",
    "        for k in self:\n",
    "            if k.lower() == key.lower():\n",
    "                return k\n",
    "        raise KeyError(key)\n",
    "\n",
    "    def update(self, other=None, **kwargs):\n",
    "        if other is not None:\n",
    "            for k, v in other.items() if isinstance(other, abc.Mapping) else other:\n",
    "                self[k] = v\n",
    "        for k, v in kwargs.items():\n",
    "            assert self._match(k)\n",
    "            self[k] = v\n",
    "\n",
    "    def get(self, key, default_value=None):\n",
    "        try:\n",
    "            return self[key]\n",
    "        except KeyError:\n",
    "            return default_value\n",
    "\n",
    "class Misc:\n",
    "    \"\"\"Miscellaneous utility functions\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_items(input_str, delete_lst):\n",
    "        \"\"\"Remove specific items from semicolon-separated string\"\"\"\n",
    "        pattern = '|'.join(r'\\b%s\\b;?\\s?' % x for x in delete_lst)\n",
    "        return re.sub(pattern, '', input_str).rstrip('; ')\n",
    "\n",
    "    @staticmethod\n",
    "    def prefix(a, b):\n",
    "        \"\"\"Check if either string is prefix of the other\"\"\"\n",
    "        return a.startswith(b) or b.startswith(a)\n",
    "\n",
    "    @staticmethod\n",
    "    def common(a, b, use_prefix=False):\n",
    "        \"\"\"Find common items between two lists with optional prefix matching\"\"\"\n",
    "        def prefix_cmp(a, b):\n",
    "            return a.startswith(b) or b.startswith(a)\n",
    "        \n",
    "        result, visited = {}, set()\n",
    "        prefix_d = defaultdict(list)\n",
    "        \n",
    "        #>>> Build prefix matching dictionary <<<#\n",
    "        for x, y in [(x, y) for x in a for y in b]:\n",
    "            if prefix_cmp(x, y):\n",
    "                prefix_d[x].append(y)\n",
    "        \n",
    "        #>>> Prioritize exact matches <<<#\n",
    "        for x in a:\n",
    "            if x in b and x not in visited:\n",
    "                result[x] = x\n",
    "                visited.add(x)\n",
    "        \n",
    "        #>>> Handle prefix matches for remaining items <<<#\n",
    "        for x in a:\n",
    "            if x in result and (not use_prefix):\n",
    "                continue\n",
    "            for y in prefix_d[x]:\n",
    "                if y not in visited:\n",
    "                    result[x] = y\n",
    "                    visited.add(y)\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def convert2int(a):\n",
    "        \"\"\"Safely convert value to integer\"\"\"\n",
    "        try:\n",
    "            return int(a)\n",
    "        except (TypeError, ValueError):\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def convert2datestr(a):\n",
    "        \"\"\"Convert datetime to string format\"\"\"\n",
    "        if isinstance(a, datetime):\n",
    "            return a.strftime('%Y-%m-%d')\n",
    "        return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Database Connection and SQL Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Connect to PCDS Oracle database ---#\n",
    "def pcds_connect(service_name, ldap_service='X'):\n",
    "    \"\"\"Establish connection to PCDS Oracle database\"\"\"\n",
    "    import oracledb\n",
    "    # Map service names to connection strings\n",
    "    svc2server = {\n",
    "        'A': 'PBCS21P',\n",
    "        'B': 'PBCS30P',\n",
    "        'C': 'PCDS',\n",
    "        'D': 'PBCS23P',\n",
    "    }\n",
    "    # Implement connection logic based on your environment\n",
    "    # return oracledb.connect(user=usr, password=pwd, dsn=dns_tns)\n",
    "    raise NotImplementedError(\"Please implement PCDS connection logic\")\n",
    "\n",
    "class SQLengine:\n",
    "    \"\"\"SQL query engine for PCDS and AWS databases\"\"\"\n",
    "    \n",
    "    def __init__(self, platform: Literal['PCDS', 'AWS']):\n",
    "        self._platform = platform\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset internal state\"\"\"\n",
    "        self._where = None\n",
    "        self._type = None\n",
    "        self._date = None\n",
    "        self._dateraw = None\n",
    "        self._table = None\n",
    "        self._format = AWS_DT_FORMAT if self._platform == 'AWS' else PCDS_DT_FORMAT\n",
    "\n",
    "    def extract_var(self, stmt):\n",
    "        \"\"\"Extract variable names from SQL date expression\"\"\"\n",
    "        def _extract_var():\n",
    "            word, time, tagt = r'\\w+_\\w+', r\"'[^']*'\", r'[^,]+'\n",
    "            pattern1 = fr\"{word}\\({word}\\(({tagt}),\\s*{time}\\),\\s*{time}\\)\"\n",
    "            pattern2 = fr\"{word}\\(({tagt}),\\s*{time}\\)\"\n",
    "            if (m := re.match(pattern1, stmt)):\n",
    "                return stmt, m.group(1)\n",
    "            elif (m := re.match(pattern2, stmt)):\n",
    "                return stmt, m.group(1)\n",
    "            return stmt, stmt\n",
    "        \n",
    "        date_var, date_raw = _extract_var()\n",
    "        if self._platform == 'PCDS':\n",
    "            return date_var, date_raw.upper()\n",
    "        else:\n",
    "            return date_var, date_raw.lower()\n",
    "\n",
    "    def query(self, query, connection, **query_kwargs):\n",
    "        \"\"\"Execute SQL query and return DataFrame\"\"\"\n",
    "        query = self.clean_query(query)\n",
    "        df = psql.read_sql_query(query, connection, **query_kwargs)\n",
    "        \n",
    "        #>>> Normalize column names based on platform <<<#\n",
    "        if self._platform == 'PCDS':\n",
    "            df.columns = [x.upper() for x in df.columns]\n",
    "        else:\n",
    "            df.columns = [x.lower() for x in df.columns]\n",
    "        return df\n",
    "\n",
    "    def clean_query(self, query: str):\n",
    "        \"\"\"Clean and prepare SQL query for execution\"\"\"\n",
    "        #>>> Extract table name from query <<<#\n",
    "        table_pattern = r'([\\w.]+)\\s+MORF\\b'\n",
    "        self._table = re.search(table_pattern, query[::-1], flags=re.I).group(1)[::-1]\n",
    "        \n",
    "        #>>> Add alias to date column if needed <<<#\n",
    "        date_pattern = r'(?!\\\\s+(?:AS\\s+)\\w+)'\n",
    "        if self._date and (match := re.search(\n",
    "            re.escape(self._date) + date_pattern,\n",
    "            re.split(r'\\b(?:FROM|WHERE)\\b', query, flags=re.I)[0],\n",
    "            flags=re.I\n",
    "        )):\n",
    "            st, ed = match.span()\n",
    "            query = query[:st] + f'{self._date} as {self._dateraw}' + query[ed:]\n",
    "        \n",
    "        #>>> Remove empty WHERE clauses <<<#\n",
    "        where_pattern = r'^\\s*where\\s*$'\n",
    "        return re.sub(where_pattern, '', query, flags=re.I | re.M)\n",
    "\n",
    "    def get_where_sql(self, date_var: str, date_type: str, start_dt=None, end_dt=None, where_cstr='') -> str:\n",
    "        \"\"\"Build WHERE clause for date filtering\"\"\"\n",
    "        self._type = date_type\n",
    "        \n",
    "        #>>> Handle subquery in where constraint <<<#\n",
    "        if not pd.isna(where_cstr) and (m := re.search(r'(?<=\\()select.*(?=\\))', where_cstr)):\n",
    "            call_func = getattr(self, 'query_%s' % self._platform)\n",
    "            rhs = call_func(m.group()).iloc[0, 0]\n",
    "            if isinstance(rhs, str):\n",
    "                where_cstr = \"%s '%s'\" % (where_cstr[:m.start() - 1], rhs)\n",
    "            else:\n",
    "                where_cstr = \"%s '%s'\" % (where_cstr[:m.start() - 1], rhs.strftime('%Y-%m-%d'))\n",
    "        \n",
    "        where_sql = [where_cstr]\n",
    "        self.get_date_sql(date_var, date_type)\n",
    "        \n",
    "        #>>> Add date range filters <<<#\n",
    "        if not pd.isna(start_dt):\n",
    "            start_dt = Misc.convert2datestr(start_dt)\n",
    "            where_sql.append(f\"{self._date} >= '{start_dt}'\")\n",
    "        if not pd.isna(end_dt):\n",
    "            end_dt = Misc.convert2datestr(end_dt)\n",
    "            where_sql.append(f\"{self._date} <= '{end_dt}'\")\n",
    "        \n",
    "        #>>> Convert TO_CHAR comparisons to TO_DATE for PCDS <<<#\n",
    "        for i, sql_stmt in enumerate(where_sql):\n",
    "            if self._platform == 'AWS' or pd.isna(sql_stmt):\n",
    "                continue\n",
    "            if (match := re.match(r\"^TO_CHAR\\(([^,]+),\\s*'(.*)'\\)\\s*([><=!]+)\\s*'([^']+)'\", sql_stmt)):\n",
    "                a, b, c, d = match.groups()\n",
    "                where_sql[i] = f\"{a} {c} TO_DATE('{d}', '{b}')\"\n",
    "        \n",
    "        self._where = ' AND '.join(x for x in where_sql if not pd.isna(x))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_date_format(date_var):\n",
    "        \"\"\"Extract date format from variable specification\"\"\"\n",
    "        pattern = r'^(.+?)(?:\\s*\\(([^)]+)\\))?$'\n",
    "        date_var, date_format = re.match(pattern, date_var).groups()\n",
    "        return date_var, date_format\n",
    "\n",
    "    def get_date_sql(self, date_var: str, date_type: str):\n",
    "        \"\"\"Convert date column to standard format in SQL\"\"\"\n",
    "        date_var, date_format = self.get_date_format(date_var)\n",
    "        is_date = re.search(r'time|date', date_type, re.IGNORECASE)\n",
    "        \n",
    "        #>>> Parse string dates if format provided <<<#\n",
    "        if date_format and (not is_date):\n",
    "            if self._platform == 'PCDS':\n",
    "                date_var = f\"TO_DATE({date_var}, '{date_format}')\"\n",
    "            else:\n",
    "                date_var = f\"DATE_PARSE({date_var}, '{date_format}')\"\n",
    "            is_date = True\n",
    "        \n",
    "        #>>> Convert to standard string format <<<#\n",
    "        if is_date:\n",
    "            if self._platform == 'PCDS':\n",
    "                date_var = f\"TO_CHAR({date_var}, 'YYYY-MM-DD')\"\n",
    "            else:\n",
    "                date_var = f\"DATE_FORMAT({date_var}, '%Y-%m-%d')\"\n",
    "        \n",
    "        self._date, self._dateraw = self.extract_var(date_var)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'SQL({self._platform})\\n' \\\n",
    "               f'   table: {self._table}\\n' \\\n",
    "               f'   where: {self._where}\\n' \\\n",
    "               f'   date : {self._date} ({self._dateraw})'\n",
    "\n",
    "    def query_PCDS(self, query_stmt: str, service_name: str, **query_kwargs):\n",
    "        \"\"\"Execute query on PCDS database\"\"\"\n",
    "        with pcds_connect(service_name=service_name) as CONN:\n",
    "            return self.query(query_stmt, CONN, **query_kwargs)\n",
    "\n",
    "    def query_AWS(self, query_stmt: str, **query_kwargs):\n",
    "        \"\"\"Execute query on AWS Athena\"\"\"\n",
    "        aws_creds_renew()\n",
    "        CONN = pa.connect(\n",
    "            s3_staging_dir=\"s3://355538383407-us-east-1-athena-output/uscb-analytics/\",\n",
    "            region_name=\"us-east-1\",\n",
    "        )\n",
    "        return self.query(query_stmt, CONN, **query_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Excel Input Processing and Column Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Read and process Excel input file with table configurations ---#\n",
    "def read_excel_input(config: MetaTable) -> pd.DataFrame:\n",
    "    def trim_me(x):\n",
    "        \"\"\"Trim whitespace from strings\"\"\"\n",
    "        return x.strip() if isinstance(x, str) else x\n",
    "\n",
    "    def extract_name(name):\n",
    "        \"\"\"Remove parenthetical notes from names\"\"\"\n",
    "        if pd.isna(name):\n",
    "            return pd.NA\n",
    "        if not isinstance(name, str):\n",
    "            return name\n",
    "        remove_extra = r'\\(.*\\)'\n",
    "        return re.sub(remove_extra, '', name).strip()\n",
    "\n",
    "    def merge_pcds_svc_tbl(df):\n",
    "        \"\"\"Combine service and table names into qualified names\"\"\"\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore', pd.errors.SettingWithCopyWarning)\n",
    "            cols = [x for x in df.columns if x not in (\n",
    "                'group', 'pcds_dt', 'aws_dt', 'pcds_where', 'aws_where'\n",
    "            )]\n",
    "            df[cols] = df[cols].map(extract_name)\n",
    "            tbl = df.pop('pcds_tbl')\n",
    "            df['col_map'] = df['col_map'].fillna(tbl).copy()\n",
    "            svc = df.pop('pcds_svc').fillna('no_server_provided')\n",
    "            df.loc[:, ['pcds_tbl']] = svc + '.' + tbl.str.lower()\n",
    "            df['pcds_dt'] = df['pcds_dt'].copy().fillna(NO_DATE)\n",
    "            df['aws_dt'] = df['aws_dt'].copy().fillna(NO_DATE)\n",
    "            df[['pcds_where', 'aws_where']] = df[['pcds_where', 'aws_where']].replace(np.nan, None)\n",
    "\n",
    "    file_path = config.file\n",
    "    try:\n",
    "        df = pd.read_excel(\n",
    "            file_path, sheet_name=config.sheet,\n",
    "            skiprows=config.skip_rows, usecols=list(config.select_cols)\n",
    "        )\n",
    "        df = df.rename(columns=config.select_cols).map(trim_me)\n",
    "        if len(config.select_rows) > 0:\n",
    "            df = df.query(' & '.join(config.select_rows))\n",
    "        merge_pcds_svc_tbl(df)\n",
    "        logger.info(f\"Read {len(df)} rows from {file_path}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to read Excel file {file_path}: {e}\")\n",
    "        raise\n",
    "\n",
    "class ColmapUtils:\n",
    "    \"\"\"Utility class for processing column mapping files\"\"\"\n",
    "    \n",
    "    def __init__(self, category: Literal['loan', 'dpst']):\n",
    "        self.category = category\n",
    "        if category == 'dpst':\n",
    "            self._obtain_table = self.process_dpst\n",
    "        elif category == 'loan':\n",
    "            self._obtain_table = self.process_loan\n",
    "\n",
    "    def is_column_tokenized(self, row):\n",
    "        \"\"\"Check if column contains PII that should be tokenized\"\"\"\n",
    "        if row.get('pii_encryption', None) == 'Y':\n",
    "            return True\n",
    "        try:\n",
    "            return 'tokenise' in row.get('note', \"\").lower()\n",
    "        except (TypeError, AttributeError):\n",
    "            return False\n",
    "\n",
    "    def process_loan(self, config: ColumnMap):\n",
    "        \"\"\"Process loan category mapping file\"\"\"\n",
    "        all_sheets = pd.read_excel(config.input, sheet_name=None)\n",
    "        yield from all_sheets.items()\n",
    "\n",
    "    def process_dpst(self, config: ColumnMap):\n",
    "        \"\"\"Process deposit category mapping file\"\"\"\n",
    "        all_sheets = pd.read_excel(config.input, sheet_name='Column_Details')\n",
    "        return all_sheets.groupby(config.pcds_view[0])\n",
    "\n",
    "    def process(self, config: ColumnMap) -> UDict:\n",
    "        \"\"\"Process column mapping configuration file\"\"\"\n",
    "        if os.path.exists(config.output) and (not config.overwrite):\n",
    "            return IO.read_json(config.output)\n",
    "        \n",
    "        table_excludes = config.excludes or []\n",
    "        mappings = {}\n",
    "        \n",
    "        #>>> Process each table's column mappings <<<#\n",
    "        for pcds_name, df in self._obtain_table(config):\n",
    "            if pcds_name in table_excludes:\n",
    "                continue\n",
    "            \n",
    "            if self.category == 'loan' and 'Source' in df.columns:\n",
    "                df = pd.DataFrame(df.iloc[1:].values, columns=df.iloc[0])\n",
    "\n",
    "            df = df.rename(columns=self.colfunc)\n",
    "            pcds2aws, aws2pcds = {}, {}\n",
    "            s = {\n",
    "                'aws_unique': [],\n",
    "                'pcds_unique': [],\n",
    "                'duplicated_pcds': set(),\n",
    "                'duplicated_aws': set(),\n",
    "                'pii_cols': set()\n",
    "            }\n",
    "            \n",
    "            #>>> Build bidirectional column mappings <<<#\n",
    "            for i, row in enumerate(df.itertuples()):\n",
    "                row = UDict(**row._asdict())\n",
    "                if i == 0:\n",
    "                    if self.category == 'loan':\n",
    "                        pcds_name = self.fetchcol(row, config.pcds_view, config.na_str)\n",
    "                        aws_name = self.fetchcol(row, config.aws_view, config.na_str)\n",
    "                    elif self.category == 'dpst':\n",
    "                        aws_name = config.aws_view.format(**row)\n",
    "                \n",
    "                pcds_col = self.fetchcol(row, config.pcds_col, config.na_str)\n",
    "                aws_col = self.fetchcol(row, config.aws_col, config.na_str)\n",
    "                pcds_na, aws_na = pd.isna(pcds_col), pd.isna(aws_col)\n",
    "                \n",
    "                if pcds_na and aws_na:\n",
    "                    continue\n",
    "                elif pcds_na:\n",
    "                    s['aws_unique'].append(aws_col.lower())\n",
    "                elif aws_na:\n",
    "                    s['pcds_unique'].append(pcds_col.upper())\n",
    "                else:\n",
    "                    pcds_col, aws_col = pcds_col.upper(), aws_col.lower()\n",
    "                    \n",
    "                    #>>> Check for duplicate AWS columns <<<#\n",
    "                    has_dupl, aws_dup = self.get_duplicates(aws_col, pcds_col, aws2pcds, s['aws_unique'])\n",
    "                    if has_dupl:\n",
    "                        logger.warning(f'Table {pcds_name} has duplicated AWS column {aws_col}')\n",
    "                        s['duplicated_aws'] |= aws_dup\n",
    "                    else:\n",
    "                        aws2pcds[aws_col] = pcds_col\n",
    "\n",
    "                    #>>> Check for duplicate PCDS columns <<<#\n",
    "                    has_dupl, pcds_dup = self.get_duplicates(pcds_col, aws_col, pcds2aws, s['pcds_unique'])\n",
    "                    if has_dupl:\n",
    "                        logger.warning(f'Table {pcds_name} has duplicated PCDS column {pcds_col}')\n",
    "                        s['duplicated_pcds'] |= pcds_dup\n",
    "                    else:\n",
    "                        pcds2aws[pcds_col] = aws_col\n",
    "                \n",
    "                if self.is_column_tokenized(row):\n",
    "                    s['pii_cols'].add(pcds_col)\n",
    "            \n",
    "            if len(pcds2aws) == 0:\n",
    "                logger.info(f\"No match key is found in {pcds_name}\")\n",
    "            \n",
    "            s['duplicated_pcds'] = list(s['duplicated_pcds'])\n",
    "            s['duplicated_aws'] = list(s['duplicated_aws'])\n",
    "            s['pii_cols'] = list(x for x in s['pii_cols'] if pd.notna(x))\n",
    "            mappings[pcds_name] = {\n",
    "                'pcds_table': pcds_name,\n",
    "                'aws_table': aws_name,\n",
    "                'pcds2aws': pcds2aws,\n",
    "                **s\n",
    "            }\n",
    "        \n",
    "        IO.write_json(config.output, mappings)\n",
    "        return UDict(mappings)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_duplicates(col_a, col_b, a2b: dict, unique_a: list):\n",
    "        \"\"\"Check for duplicate column mappings\"\"\"\n",
    "        exist_key = set(list(a2b) + unique_a)\n",
    "        exist_val = a2b.get(col_a, pd.NA)\n",
    "        if col_a in exist_key and col_b != exist_val:\n",
    "            return True, {f'{col_a}:{exist_val}', f'{col_a}:{col_b}'}\n",
    "        return False, None\n",
    "\n",
    "    @staticmethod\n",
    "    def colfunc(col):\n",
    "        \"\"\"Normalize column names\"\"\"\n",
    "        if pd.isna(col):\n",
    "            return 'comment'\n",
    "        col = col.split('\\n')[-1]\n",
    "        return '_'.join(x.lower() for x in col.split())\n",
    "\n",
    "    @staticmethod\n",
    "    def fetchcol(row, names, na_str):\n",
    "        \"\"\"Fetch first non-null column value from list of column names\"\"\"\n",
    "        for name in names:\n",
    "            name = name.lower()\n",
    "            if (name in row) and (not pd.isna(row[name])) and (row[name] != na_str):\n",
    "                return row[name].strip()\n",
    "        return pd.NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: S3 Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S3:\n",
    "    \"\"\"AWS S3 utility functions\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def upload_multiple(s3_url, folder, prefix=''):\n",
    "        \"\"\"Upload multiple files from folder to S3\"\"\"\n",
    "        folder, s3_url = UPath(folder), UPath(s3_url)\n",
    "        for file in folder.glob('%s.*' % prefix):\n",
    "            aws.s3.upload(\n",
    "                local_file=file.as_posix(),\n",
    "                path=s3_url.joinpath(file.name).as_posix(),\n",
    "                boto3_session=SESSION\n",
    "            )\n",
    "            logger.info(f\"Uploading {file.name} to {s3_url} [finished]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Main Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize global objects\n",
    "proc_pcds = SQLengine('PCDS')\n",
    "proc_aws = SQLengine('AWS')\n",
    "record = MetaRecord()\n",
    "\n",
    "#--- Parse command line arguments and load configuration ---#\n",
    "def parse_config():\n",
    "    parser = argparse.ArgumentParser(description='Conduct Meta Info Analysis')\n",
    "    parser.add_argument(\n",
    "        '--category',\n",
    "        choices=['loan', 'dpst'],\n",
    "        default='dpst',\n",
    "        help='which meta template to use',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--name', type=str,\n",
    "        default='test_0827',\n",
    "        help='how to name this analysis (override)'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--query', type=str,\n",
    "        default='group == \"test_0827\"',\n",
    "        help='how to name this analysis (override)'\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.category == 'dpst':\n",
    "        config_path = r'files/inputs/config_meta_dpst.cfg'\n",
    "    elif args.category == 'loan':\n",
    "        config_path = r'files/inputs/config_meta_loan.cfg'\n",
    "    \n",
    "    config = read_config(\n",
    "        MetaConfig,\n",
    "        config_path=config_path,\n",
    "        overrides={\n",
    "            'input.table.select_rows': args.query,\n",
    "            'input.name': args.name\n",
    "        }\n",
    "    )\n",
    "    (out_folder := UPath(config.output.folder)).mkdir(exist_ok=True)\n",
    "    shutil.copy(config_path, out_folder.joinpath(f'{config.input.step}.cfg'))\n",
    "    return config\n",
    "\n",
    "#--- Normalize date format specifications ---#\n",
    "def parse_date_format(row):\n",
    "    return row._replace(\n",
    "        pcds_dt=row.pcds_dt.upper(),\n",
    "        aws_dt=row.aws_dt.lower(),\n",
    "    )\n",
    "\n",
    "#--- Process PCDS table metadata (columns and row count) ---#\n",
    "def process_pcds_meta(row, rename_columns={}):\n",
    "    service, table = (info_str := row.pcds_tbl).split('.', maxsplit=1)\n",
    "    logger.info(f\"\\tStart processing {info_str}\")\n",
    "    \n",
    "    #>>> Query column metadata and row counts <<<#\n",
    "    try:\n",
    "        with pcds_connect(service) as CONN:\n",
    "            df_type = proc_pcds.query(PCDS_SQL_META.format(table=table), CONN)\n",
    "            if hasattr(df_type, 'last_modified'):\n",
    "                record.next_d.update(last_modified=df_type.last_modified)\n",
    "            \n",
    "            #>>> Extract date variable and build WHERE clause <<<#\n",
    "            date_var = re.match(r'(\\w+)(?=\\s*\\()?', row.pcds_dt).group(1)\n",
    "            if date_var == NO_DATE:\n",
    "                proc_pcds._where = row.pcds_where\n",
    "            else:\n",
    "                proc_pcds.get_where_sql(\n",
    "                    date_var=row.pcds_dt,\n",
    "                    date_type=df_type.query(f\"COLUMN_NAME == '{date_var.upper()}'\")['DATA_TYPE'].item(),\n",
    "                    start_dt=row.start_dt,\n",
    "                    end_dt=row.end_dt,\n",
    "                    where_cstr=row.pcds_where\n",
    "                )\n",
    "            \n",
    "            nrow_sql = PCDS_SQL_NROW.format(table=table, limit=proc_pcds._where)\n",
    "            df_nrow = proc_pcds.query(nrow_sql, CONN)\n",
    "    except (pd.errors.DatabaseError, ValueError):\n",
    "        logger.warning(f\"Couldn't find {table.upper()} in {service.upper()}\")\n",
    "        raise NONEXIST_TABLE(\"PCDS View Not Existing\")\n",
    "    \n",
    "    df_type.columns = [x.lower() for x in df_type.columns]\n",
    "    df_type['aws_colname'] = df_type['column_name'].map(rename_columns)\n",
    "    return {'column': df_type, 'row': df_nrow}, len(rename_columns) > 0\n",
    "\n",
    "#--- Query PCDS table for date-wise row counts ---#\n",
    "def process_pcds_date(row):\n",
    "    service, table = (info_str := row.pcds_tbl).split('.', maxsplit=1)\n",
    "    \n",
    "    try:\n",
    "        with pcds_connect(service) as CONN:\n",
    "            date_sql = PCDS_SQL_DATE.format(\n",
    "                table=table, limit=proc_pcds._where, date=proc_pcds._date\n",
    "            )\n",
    "            df_meta = proc_pcds.query(date_sql, CONN)\n",
    "        logger.info(f\"\\tFinish Processing {info_str}\")\n",
    "    except pd.errors.DatabaseError:\n",
    "        if proc_pcds._dateraw:\n",
    "            logger.warning(f\"Column {proc_pcds._dateraw.upper()} not found in {table.upper()}\")\n",
    "        raise NONEXIST_DATEVAR(\"Date-like Variable Not In PCDS\")\n",
    "    return df_meta\n",
    "\n",
    "#--- Process AWS table metadata (columns and row count) ---#\n",
    "def process_aws_meta(row):\n",
    "    database, table = (info_str := row.aws_tbl).split('.', maxsplit=1)\n",
    "    CONN = pa.connect(\n",
    "        s3_staging_dir=\"s3://355538383407-us-east-1-athena-output/uscb-analytics/\",\n",
    "        region_name=\"us-east-1\",\n",
    "    )\n",
    "    logger.info(f\"\\tStart processing {info_str}\")\n",
    "    \n",
    "    #>>> Query column metadata and row counts <<<#\n",
    "    try:\n",
    "        df_type = proc_aws.query(AWS_SQL_META.format(table=table, db=database), CONN)\n",
    "        date_var = re.match(r'(\\w+)(?=\\s*\\()?', row.aws_dt).group(1)\n",
    "        if date_var == NO_DATE:\n",
    "            proc_aws._where = row.aws_where\n",
    "        else:\n",
    "            proc_aws.get_where_sql(\n",
    "                date_var=row.aws_dt,\n",
    "                date_type=df_type.query(f\"column_name == '{date_var.lower()}'\")['data_type'].item(),\n",
    "                start_dt=row.start_dt,\n",
    "                end_dt=row.end_dt,\n",
    "                where_cstr=row.aws_where\n",
    "            )\n",
    "        nrow_sql = AWS_SQL_NROW.format(table=table, db=database, limit=proc_aws._where)\n",
    "        df_nrow = proc_aws.query(nrow_sql, CONN)\n",
    "    except pd.errors.DatabaseError:\n",
    "        logger.warning(f\"Couldn't find {table.lower()} in {database.lower()}\")\n",
    "        raise NONEXIST_TABLE(\"AWS View Not Existing\")\n",
    "    \n",
    "    df_type.columns = [x.lower() for x in df_type.columns]\n",
    "    return {'column': df_type, 'row': df_nrow}\n",
    "\n",
    "#--- Query AWS table for date-wise row counts ---#\n",
    "def process_aws_date(row):\n",
    "    database, table = (info_str := row.aws_tbl).split('.', maxsplit=1)\n",
    "    CONN = pa.connect(\n",
    "        s3_staging_dir=\"s3://355538383407-us-east-1-athena-output/uscb-analytics/\",\n",
    "        region_name=\"us-east-1\",\n",
    "    )\n",
    "    try:\n",
    "        date_sql = AWS_SQL_DATE.format(\n",
    "            table=table, limit=proc_aws._where, date=proc_aws._date, db=database\n",
    "        )\n",
    "        df_meta = proc_aws.query(date_sql, CONN)\n",
    "        logger.info(f\"\\tFinish Processing {info_str}\")\n",
    "    except pd.errors.DatabaseError:\n",
    "        if proc_aws._dateraw:\n",
    "            logger.warning(f\"Column {proc_aws._dateraw.upper()} not found in {table.upper()}\")\n",
    "        raise NONEXIST_DATEVAR(\"Date-like Variable Not In AWS\")\n",
    "    \n",
    "    df_meta.columns = [x.lower() for x in df_meta.columns]\n",
    "    return df_meta\n",
    "\n",
    "#--- Initialize output folders and logging ---#\n",
    "def start_setup(start_row, C_out):\n",
    "    try:\n",
    "        assert start_row <= 1\n",
    "        os.remove(C_out.csv.file)\n",
    "    except (TypeError, AssertionError, FileNotFoundError):\n",
    "        pass\n",
    "    os.makedirs(C_out.folder, exist_ok=True)\n",
    "    logger.add(**C_out.log.todict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Data Type Mapping and Column Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Check if PCDS and AWS data types are compatible ---#\n",
    "def map_pcds_aws(row):\n",
    "    aws_dtype = row.data_type_aws\n",
    "    match (pcds_dtype := row.data_type_pcds):\n",
    "        case 'NUMBER':\n",
    "            ok_1 = aws_dtype == 'double'\n",
    "            return ok_1\n",
    "        case _ if pcds_dtype.startswith('NUMBER'):\n",
    "            y1 = re.match(r'NUMBER\\(\\d*,(\\d+)\\)', pcds_dtype).group(1)\n",
    "            match = re.match(r'decimal\\(\\d*,(\\d+)\\)', aws_dtype)\n",
    "            return bool(match and match.group(1) == y1)\n",
    "        case _ if pcds_dtype.startswith('VARCHAR2'):\n",
    "            return pcds_dtype.replace('VARCHAR2', 'varchar') == aws_dtype\n",
    "        case _ if pcds_dtype.startswith('CHAR'):\n",
    "            n = re.match(r'CHAR\\((\\d+)\\)', pcds_dtype).group(1)\n",
    "            return not (aws_dtype.startswith('VARCHAR') and n != 1)\n",
    "        case 'DATE':\n",
    "            ok_1 = aws_dtype == 'date'\n",
    "            ok_2 = aws_dtype.startswith('timestamp')\n",
    "            return ok_1 | ok_2\n",
    "        case _ if pcds_dtype.startswith('TIMESTAMP'):\n",
    "            return aws_dtype.startswith('timestamp')\n",
    "        case _:\n",
    "            s = \">>> Mismatched type on {}\\n\\tPCDS ({}) ==> AWS ({})\"\n",
    "            logger.info(s.format(row.column_name_aws, pcds_dtype, aws_dtype))\n",
    "            return False\n",
    "\n",
    "#--- Merge and compare PCDS and AWS column metadata ---#\n",
    "def process_merge(pcds: pd.DataFrame, aws: pd.DataFrame) -> MetaMerge:\n",
    "    \"\"\"\n",
    "    Check column mapping and variable typing differences\n",
    "    Returns unique columns, type mismatches, and uncaptured mappings\n",
    "    \"\"\"\n",
    "    #>>> Find columns without documented mappings <<<#\n",
    "    unmapped_pcds = (\n",
    "        pcds.query('aws_colname != aws_colname')\n",
    "        ['column_name'].str.lower().to_list()\n",
    "    )\n",
    "    unmapped_aws = (\n",
    "        aws.query('~column_name.isin(@pcds.aws_colname)')\n",
    "        ['column_name'].to_list()\n",
    "    )\n",
    "    \n",
    "    #>>> Use substring matching to find undocumented pairs <<<#\n",
    "    map_uncaptured = Misc.common(unmapped_pcds, unmapped_aws)\n",
    "    map_uncaptured = {\n",
    "        k.upper(): v for k, v in map_uncaptured.items()\n",
    "        if k not in record.next_d['tokenised_cols']\n",
    "    }\n",
    "    uncaptured = SEP.join('{}->{}'.format(k, v) for k, v in map_uncaptured.items())\n",
    "\n",
    "    #>>> Update column mappings with discovered pairs <<<#\n",
    "    pcds['aws_colname'] = (\n",
    "        pcds['aws_colname']\n",
    "        .combine_first(pcds['column_name'].map(map_uncaptured))\n",
    "    )\n",
    "    \n",
    "    #>>> Merge PCDS and AWS metadata <<<#\n",
    "    df_match = pd.merge(\n",
    "        pcds, aws,\n",
    "        left_on='aws_colname', right_on='column_name',\n",
    "        suffixes=['_pcds', '_aws'],\n",
    "        how='outer', indicator=True\n",
    "    )\n",
    "    \n",
    "    #>>> Separate unique columns from each platform <<<#\n",
    "    pcds_cols = ['column_name_pcds', 'data_type_pcds']\n",
    "    pcds_unique = df_match.query('_merge == \"left_only\"')[pcds_cols]\n",
    "    aws_cols = ['column_name_aws', 'data_type_aws']\n",
    "    aws_unique = df_match.query('_merge == \"right_only\"')[aws_cols]\n",
    "\n",
    "    #>>> Check data type compatibility for matched columns <<<#\n",
    "    if record.pull_status == PullStatus.NO_MAPPING:\n",
    "        mismatched, merged = '', None\n",
    "    else:\n",
    "        merged = (\n",
    "            df_match.query('_merge == \"both\"')\n",
    "            .drop(columns=['aws_colname', '_merge'])\n",
    "        )\n",
    "        merged['type_match'] = merged.apply(map_pcds_aws, axis=1)\n",
    "        mismatch_d = (\n",
    "            merged.query('~type_match')\n",
    "            [['data_type_pcds', 'data_type_aws']]\n",
    "            .drop_duplicates()\n",
    "        )\n",
    "        mismatched = SEP.join('{}->{}'.format(*x[1:]) for x in mismatch_d.itertuples())\n",
    "\n",
    "    #>>> Filter out previously known unique columns <<<#\n",
    "    unmapped_pcds = pcds_unique['column_name_pcds'].str.upper().to_list()\n",
    "    unmapped_aws = aws_unique['column_name_aws'].str.lower().to_list()\n",
    "    if (col_map := record.col_maps.get(proc_pcds._table)):\n",
    "        unmapped_aws = [x for x in unmapped_aws if x not in col_map['aws_unique']]\n",
    "        unmapped_pcds = [x for x in unmapped_pcds if x not in col_map['pcds_unique']]\n",
    "\n",
    "    return MetaMerge(\n",
    "        unique_pcds=unmapped_pcds,\n",
    "        unique_aws=unmapped_aws,\n",
    "        col_mapping=merged,\n",
    "        mismatches=mismatched,\n",
    "        uncaptured=uncaptured\n",
    "    )\n",
    "\n",
    "#--- Compare column mappings and total records between PCDS and AWS ---#\n",
    "def process_meta(pcds_t: dict, aws_t: dict) -> dict:\n",
    "    uncaptured = \"\"\n",
    "    next_d = record.next_d\n",
    "    pcds_c, aws_c = pcds_t['column'], aws_t['column']\n",
    "    \n",
    "    #>>> Handle missing column mapping <<<#\n",
    "    if pcds_c['aws_colname'].isna().all():\n",
    "        pcds_c['aws_colname'] = pcds_c['column_name'].str.lower()\n",
    "        uncaptured = \"Column Mapping Not Provided\"\n",
    "    \n",
    "    profile = process_merge(pcds_c, aws_c)\n",
    "    logger.info(\">>> Finish Merging Type Data\")\n",
    "\n",
    "    #>>> Prepare data for next processing step <<<#\n",
    "    d = (\n",
    "        profile.col_mapping\n",
    "        .drop(columns='type_match')\n",
    "        .apply(lambda x: SEP.join(x.tolist()), axis=0)\n",
    "        .to_dict()\n",
    "    )\n",
    "    next_d.update(\n",
    "        pcds_cols=d['column_name_pcds'],\n",
    "        pcds_types=d['data_type_pcds'],\n",
    "        pcds_nrows=int(pcds_t['row'].iloc[0, 0]),\n",
    "        pcds_where=proc_pcds._where,\n",
    "        aws_cols=d['column_name_aws'],\n",
    "        aws_types=d['data_type_aws'],\n",
    "        aws_nrows=int(aws_t['row'].iloc[0, 0]),\n",
    "        aws_where=proc_aws._where,\n",
    "        pcds_dt_type=proc_pcds._type,\n",
    "        aws_dt_type=proc_aws._type\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'Row UnMatch': next_d['pcds_nrows'] != next_d['aws_nrows'],\n",
    "        'Row UnMatch Details': f\"PCDS({next_d['pcds_nrows']}) : AWS({next_d['aws_nrows']})\",\n",
    "        'Type UnMatch Details': profile.mismatches,\n",
    "        'Column Type UnMatch': len(profile.mismatches) > 0,\n",
    "        'PCDS Extra Columns': len(profile.unique_pcds) > 0,\n",
    "        'PCDS Unique Columns': SEP.join(profile.unique_pcds),\n",
    "        'AWS Extra Columns': len(profile.unique_aws) > 0,\n",
    "        'AWS Unique Columns': SEP.join(profile.unique_aws),\n",
    "        'Uncaptured Column Mappings': uncaptured or profile.uncaptured,\n",
    "    }\n",
    "\n",
    "#--- Identify specific dates with row count discrepancies ---#\n",
    "def process_date(cnt_pcds: pd.DataFrame, cnt_aws: pd.DataFrame):\n",
    "    def get_date(a, b):\n",
    "        \"\"\"Return first non-null date\"\"\"\n",
    "        return b if pd.isna(a) else a\n",
    "\n",
    "    def get_detailed_mismatch():\n",
    "        \"\"\"Format mismatch details\"\"\"\n",
    "        a, _, b, _ = time_mismatch.columns\n",
    "        return '; '.join(\n",
    "            f\"{get_date(r[a], r[b])} ({r['NROWS']} : {Misc.convert2int(r['nrows'])})\"\n",
    "            for _, r in time_mismatch.iterrows()\n",
    "        )\n",
    "\n",
    "    def get_time_excludes_sql():\n",
    "        \"\"\"Build SQL to exclude problematic dates\"\"\"\n",
    "        exclude = ','.join(\"'%s'\" % x for x in time_mismatch[pcds_dt].fillna(time_mismatch[aws_dt]) if x)\n",
    "        return {\n",
    "            'pcds_exclude': f'{proc_pcds._date} not in ({exclude})',\n",
    "            'aws_exclude': f'{proc_aws._date} not in ({exclude})',\n",
    "        }\n",
    "\n",
    "    pcds_dt, aws_dt = proc_pcds._dateraw, proc_aws._dateraw\n",
    "    \n",
    "    #>>> Merge date-wise row counts <<<#\n",
    "    df_all = pd.merge(\n",
    "        left=cnt_pcds,\n",
    "        right=cnt_aws,\n",
    "        left_on=pcds_dt,\n",
    "        right_on=aws_dt,\n",
    "        suffixes=['_pcds', '_aws'],\n",
    "        how='outer'\n",
    "    )\n",
    "    \n",
    "    time_mismatch = df_all.query('NROWS != nrows')\n",
    "    logger.warning(\"Out of {} days to compare, issues are found on {} days\".format(\n",
    "        len(cnt_aws), len(time_mismatch)\n",
    "    ))\n",
    "    record.next_d.update(**get_time_excludes_sql())\n",
    "    \n",
    "    return {\n",
    "        'Time Span UnMatch': len(time_mismatch) > 0,\n",
    "        'Time Span Variable': f'{pcds_dt} : {aws_dt}',\n",
    "        'Time UnMatch Details (PCDS : AWS)': get_detailed_mismatch()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 11: Main Execution Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main execution function for meta analysis\"\"\"\n",
    "    config = parse_config()\n",
    "    df_dict, df_next = {}, {}\n",
    "    C_out, C_in = config.output, config.input\n",
    "    start_row, end_row = C_in.range\n",
    "    start_setup(start_row, C_out)\n",
    "    logger.info('Configuration:\\n' + config.tolog())\n",
    "    \n",
    "    #>>> Load environment and AWS credentials <<<#\n",
    "    load_env(C_in.env)\n",
    "    start_run()\n",
    "    aws_creds_renew(15 * 60)\n",
    "    \n",
    "    HAS_HEADER = False\n",
    "    \n",
    "    #>>> Load input table list and column mappings <<<#\n",
    "    tbl_list = (\n",
    "        read_excel_input(C_in.table)\n",
    "        .groupby('pcds_tbl')\n",
    "        .first()\n",
    "        .reset_index()\n",
    "    )\n",
    "    record.col_maps = (\n",
    "        ColmapUtils(C_in.category)\n",
    "        .process(config.column_maps)\n",
    "    )\n",
    "    \n",
    "    #>>> Process each table <<<#\n",
    "    total = len(tbl_list)\n",
    "    for i, row in enumerate(tqdm(\n",
    "        tbl_list.itertuples(), desc='Processing ...', total=total\n",
    "    ), start=1):\n",
    "        if (i < start_row or i > end_row):\n",
    "            HAS_HEADER = False\n",
    "            continue\n",
    "        \n",
    "        pcds_m, pcds_d, aws_m, aws_d = {}, None, {}, None\n",
    "        record.next_d = UDict(C_out.next.fields)\n",
    "        name: str = row.pcds_tbl.split('.')[1].lower()\n",
    "        \n",
    "        logger.info(f\">>> Start {name}\")\n",
    "        \n",
    "        #>>> Initialize record for this table <<<#\n",
    "        record.next_d.update(\n",
    "            pcds_tbl=row.pcds_tbl,\n",
    "            aws_tbl=row.aws_tbl,\n",
    "            pcds_dt=row.pcds_dt,\n",
    "            aws_dt=row.aws_dt,\n",
    "            partition=row.partition,\n",
    "            last_modified=datetime.now().strftime('%Y-%m-%d'),\n",
    "            tokenised_cols=[]\n",
    "        )\n",
    "        \n",
    "        os.environ['SKIP_SAVE'] = 'N'\n",
    "        if os.environ.get('SKIP_SNAPSHOT') and row.partition == 'snapshot':\n",
    "            os.environ['SKIP_SAVE'] = 'Y'\n",
    "        \n",
    "        row_result_d = {\n",
    "            'Consumer Loans Data Product': row.group,\n",
    "            'PCDS Table Details with DB Name': name,\n",
    "            'Tables delivered in AWS with DB Name': row.aws_tbl\n",
    "        }\n",
    "        pull_status = PullStatus.SUCCESS\n",
    "        \n",
    "        #>>> Try to pull PCDS table metadata <<<#\n",
    "        try:\n",
    "            rename_columns = {}\n",
    "            if (c := row.col_map) and (c in record.col_maps):\n",
    "                meta_info = record.col_maps[c]\n",
    "                rename_columns = {\n",
    "                    k: v for k, v in meta_info['pcds2aws'].items()\n",
    "                    if k not in meta_info['pii_cols']\n",
    "                }\n",
    "                record.next_d.update(tokenised_cols=meta_info['pii_cols'].copy())\n",
    "            pcds_m, exist_mapping = process_pcds_meta(row, rename_columns)\n",
    "        except NONEXIST_TABLE:\n",
    "            pull_status = PullStatus.NONEXIST_PCDS\n",
    "        \n",
    "        #>>> Check if column mapping was provided <<<#\n",
    "        if pull_status == PullStatus.SUCCESS and (not exist_mapping):\n",
    "            pull_status = PullStatus.NO_MAPPING\n",
    "        \n",
    "        #>>> Try to get date-wise counts from PCDS <<<#\n",
    "        try:\n",
    "            pcds_d = process_pcds_date(row)\n",
    "            if len(pcds_m) == 0:\n",
    "                pull_status = PullStatus.EMPTY_PCDS\n",
    "        except NONEXIST_DATEVAR:\n",
    "            if pull_status == PullStatus.SUCCESS:\n",
    "                pull_status = PullStatus.NONDATE_PCDS\n",
    "        \n",
    "        #>>> Try to pull AWS table metadata <<<#\n",
    "        try:\n",
    "            aws_m = process_aws_meta(row)\n",
    "        except NONEXIST_TABLE:\n",
    "            if pull_status == PullStatus.SUCCESS:\n",
    "                pull_status = PullStatus.NONEXIST_AWS\n",
    "        \n",
    "        #>>> Try to get date-wise counts from AWS <<<#\n",
    "        try:\n",
    "            aws_d = process_aws_date(row)\n",
    "            if len(aws_m) == 0:\n",
    "                pull_status = PullStatus.EMPTY_AWS\n",
    "        except NONEXIST_DATEVAR:\n",
    "            if pull_status == PullStatus.SUCCESS:\n",
    "                pull_status = PullStatus.NONDATE_AWS\n",
    "        \n",
    "        #>>> Initialize result dictionary <<<#\n",
    "        ROW_D = {\n",
    "            'Status': pull_status.value,\n",
    "            'Row UnMatch': False,\n",
    "            'Row UnMatch Details': '',\n",
    "            'Time Span UnMatch': False,\n",
    "            'Time Span Variable': f'{row.pcds_dt} : {row.aws_dt}',\n",
    "            'Time UnMatch Details (PCDS : AWS)': '',\n",
    "            'Column Type UnMatch': False,\n",
    "            'Type UnMatch Details': '',\n",
    "            'PCDS Extra Columns': False,\n",
    "            'PCDS Unique Columns': '',\n",
    "            'AWS Extra Columns': False,\n",
    "            'AWS Unique Columns': '',\n",
    "            'Uncaptured Column Mappings': '',\n",
    "        }\n",
    "        \n",
    "        #>>> Handle different status scenarios <<<#\n",
    "        if pull_status in (PullStatus.NONEXIST_PCDS, PullStatus.NONDATE_AWS):\n",
    "            pass\n",
    "        elif pull_status == PullStatus.NO_MAPPING:\n",
    "            #>>> Basic comparison without column mapping <<<#\n",
    "            nrow_pcds = int(pcds_m['row'].iloc[0].item())\n",
    "            nrow_aws = int(aws_m['row'].iloc[0].item())\n",
    "            \n",
    "            unmapped_pcds = [x.lower() for x in pcds_m['column'].column_name]\n",
    "            unmapped_aws = aws_m['column'].column_name.to_list()\n",
    "            \n",
    "            pcds2aws = {k.upper(): v for k, v in Misc.common(unmapped_pcds, unmapped_aws).items()}\n",
    "            aws2pcds = {v: k for k, v in pcds2aws.items()}\n",
    "            \n",
    "            pcds_unique = [x.upper() for x in unmapped_pcds if x.upper() not in pcds2aws]\n",
    "            aws_unique = [x.lower() for x in unmapped_aws if x.lower() not in aws2pcds]\n",
    "            \n",
    "            pcds2type = pcds_m['column'].set_index('column_name').to_dict()['data_type']\n",
    "            aws2type = aws_m['column'].set_index('column_name').to_dict()['data_type']\n",
    "            \n",
    "            ROW_D.update(**{\n",
    "                'Row UnMatch': nrow_pcds != nrow_aws,\n",
    "                'Row UnMatch Details': f\"PCDS({nrow_pcds}) : AWS({nrow_aws})\",\n",
    "                'PCDS Extra Columns': len(pcds_unique) > 0,\n",
    "                'PCDS Unique Columns': SEP.join(pcds_unique),\n",
    "                'AWS Extra Columns': len(aws_unique) > 0,\n",
    "                'AWS Unique Columns': SEP.join(aws_unique),\n",
    "                'Uncaptured Column Mappings': SEP.join('{}->{}'.format(k, v) for k, v in pcds2aws.items()),\n",
    "            })\n",
    "            \n",
    "            record.next_d.update(\n",
    "                pcds_cols=SEP.join(pcds2aws),\n",
    "                pcds_types=SEP.join([pcds2type[x] for x in pcds2aws]),\n",
    "                pcds_nrows=nrow_pcds,\n",
    "                pcds_dt_type=proc_pcds._type,\n",
    "                pcds_where=proc_pcds._where,\n",
    "                aws_cols=SEP.join(aws2pcds),\n",
    "                aws_types=SEP.join([aws2type[x] for x in aws2pcds]),\n",
    "                aws_nrows=nrow_aws,\n",
    "                aws_dt_type=proc_aws._type,\n",
    "                aws_where=proc_aws._where,\n",
    "            )\n",
    "        else:\n",
    "            #>>> Full comparison with column mapping <<<#\n",
    "            meta_result_d = process_meta(pcds_m, aws_m)\n",
    "            ROW_D.update(**meta_result_d)\n",
    "        \n",
    "        #>>> Check for date-wise discrepancies if row counts don't match <<<#\n",
    "        if (ROW_D['Row UnMatch']) and \\\n",
    "                pull_status not in (PullStatus.NONDATE_AWS, PullStatus.NONDATE_PCDS):\n",
    "            try:\n",
    "                date_result_d = process_date(pcds_d, aws_d)\n",
    "                ROW_D.update(**date_result_d)\n",
    "            except TypeError as e:\n",
    "                logger.error(e)\n",
    "                continue\n",
    "        \n",
    "        #>>> Clean up column lists <<<#\n",
    "        ROW_D.update(**{\n",
    "            'PCDS Unique Columns': Misc.remove_items(\n",
    "                ROW_D['PCDS Unique Columns'], config.match.drop_cols\n",
    "            ),\n",
    "            'AWS Unique Columns': Misc.remove_items(\n",
    "                ROW_D['AWS Unique Columns'], config.match.add_cols\n",
    "            )\n",
    "        })\n",
    "        \n",
    "        #>>> Write results to CSV <<<#\n",
    "        with open(C_out.csv.file, 'a+', newline='') as fp:\n",
    "            writer = csv.DictWriter(fp, fieldnames=C_out.csv.columns)\n",
    "            if not HAS_HEADER:\n",
    "                writer.writeheader()\n",
    "                HAS_HEADER = True\n",
    "            writer.writerow({\n",
    "                **row_result_d,\n",
    "                **ROW_D,\n",
    "            })\n",
    "        \n",
    "        df_dict[name] = {\n",
    "            'pcds_meta': pcds_m,\n",
    "            'pcds_date': pcds_d,\n",
    "            'aws_meta': aws_m,\n",
    "            'aws_date': aws_d,\n",
    "        }\n",
    "        df_next[name] = record.next_d.copy()\n",
    "        IO.write_json(C_out.next.file, df_next)\n",
    "        \n",
    "        #>>> Reset engines for next iteration <<<#\n",
    "        proc_aws.reset()\n",
    "        proc_pcds.reset()\n",
    "    \n",
    "    #>>> Upload results to S3 <<<#\n",
    "    S3.upload_multiple(\n",
    "        s3_url=UPath(C_out.to_s3.run, C_in.name),\n",
    "        folder=C_out.folder,\n",
    "        prefix=C_in.step\n",
    "    )\n",
    "    end_run()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Analysis\n",
    "\n",
    "Uncomment the cell below to run the full analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
