{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Validation System - Main Notebook\n",
    "\n",
    "This notebook orchestrates the multi-step data validation workflow.\n",
    "\n",
    "## Workflow Steps:\n",
    "1. **Load Compare List** - Load table pairs from XLSX\n",
    "2. **Row Meta Check** - Validate row counts by date partition\n",
    "3. **Column Meta Check** - Validate column mappings and types\n",
    "4. **Column Statistics** - Analyze column-level statistics\n",
    "5. **Row Hash Check** - Compare row-level hashes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports successful\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from loguru import logger\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import modules\n",
    "import db  # noqa: E402\n",
    "import services as svc  # noqa: E402\n",
    "run_name = 'demo'\n",
    "\n",
    "# Configure logger\n",
    "logger.add(f\"files/outputs/{run_name}/events.log\", rotation=\"1 day\")\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-19 06:49:00.479\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdb.database\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mDatabase URL: sqlite:///files/outputs/demo/data.db\u001b[0m\n",
      "\u001b[32m2025-11-19 06:49:00.491\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdb.database\u001b[0m:\u001b[36minit_db\u001b[0m:\u001b[36m47\u001b[0m - \u001b[1mInitializing database schema...\u001b[0m\n",
      "\u001b[32m2025-11-19 06:49:00.499\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdb.database\u001b[0m:\u001b[36minit_db\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mDatabase initialized successfully\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Database Table Stats:\n",
      "  validation_runs_tbl: 0 rows\n",
      "  compare_list_tbl: 0 rows\n",
      "  row_meta_tbl: 0 rows\n",
      "  cross_walk_tbl: 0 rows\n",
      "  col_stat_tbl: 0 rows\n",
      "  row_hash_tbl: 0 rows\n",
      "  step_logs_tbl: 0 rows\n"
     ]
    }
   ],
   "source": [
    "# Initialize database (creates schema if not exists)\n",
    "database = db.init_db(db_path=f'files/outputs/{run_name}/data.db', reset=False)\n",
    "\n",
    "# Check database stats\n",
    "stats = database.get_table_stats()\n",
    "print(\"\\nDatabase Table Stats:\")\n",
    "for table, count in stats.items():\n",
    "    print(f\"  {table}: {count} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create New Validation Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new run: validation (ID: 1)\n"
     ]
    }
   ],
   "source": [
    "# Create a new validation run\n",
    "session = db.get_session()\n",
    "\n",
    "run_name = \"validation\"  # Change this for each run\n",
    "category = \"dpst\"  # or \"loan\"\n",
    "\n",
    "# Check if run already exists\n",
    "existing_run = session.query(db.ValidationRun).filter_by(run_name=run_name).first()\n",
    "\n",
    "if existing_run:\n",
    "    print(f\"Using existing run: {run_name} (ID: {existing_run.run_id})\")\n",
    "    run = existing_run\n",
    "else:\n",
    "    run = db.ValidationRun(\n",
    "        run_name=run_name,\n",
    "        category=category,\n",
    "        status='pending',\n",
    "        notes='Test validation run'\n",
    "    )\n",
    "    session.add(run)\n",
    "    session.commit()\n",
    "    print(f\"Created new run: {run_name} (ID: {run.run_id})\")\n",
    "\n",
    "run_id = run.run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-19 06:49:45.237\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mservices.orchestrator\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mOrchestrator initialized for run: validation\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orchestrator initialized for: validation\n",
      "Status: {'run_id': 1, 'run_name': 'validation', 'status': 'pending', 'current_step': 0, 'updated_at': datetime.datetime(2025, 11, 19, 6, 49, 3, 701800)}\n"
     ]
    }
   ],
   "source": [
    "orchestrator = svc.ValidationOrchestrator(session, run_id)\n",
    "print(f\"Orchestrator initialized for: {orchestrator.run.run_name}\")\n",
    "print(f\"Status: {orchestrator.get_status()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Compare List from XLSX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to input XLSX file\n",
    "# TODO: Update this path to your actual input file\n",
    "xlsx_path = \"files/inputs/compare_list.xlsx\"\n",
    "\n",
    "# Execute Step 1\n",
    "result = orchestrator.execute_step(1, xlsx_path=xlsx_path)\n",
    "\n",
    "print(\"\\nStep 1 Results:\")\n",
    "print(f\"  Total rows: {result['total_rows']}\")\n",
    "print(f\"  Loaded: {result['loaded']}\")\n",
    "print(f\"  Skipped: {result['skipped']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View loaded compare list\n",
    "import pandas as pd\n",
    "\n",
    "compares = session.query(db.CompareList).filter_by(run_id=run_id).all()\n",
    "\n",
    "df_compares = pd.DataFrame([\n",
    "    {\n",
    "        'id': c.id,\n",
    "        'group': c.group_name,\n",
    "        'pcds_tbl': c.pcds_tbl,\n",
    "        'aws_tbl': c.aws_tbl,\n",
    "        'partition': c.partition,\n",
    "        'enabled': c.enabled\n",
    "    }\n",
    "    for c in compares\n",
    "])\n",
    "\n",
    "print(f\"\\nLoaded {len(df_compares)} table pairs:\")\n",
    "df_compares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Row Meta Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Step 2 for all tables (or specify compare_ids)\n",
    "compare_ids = None  # None = all tables, or [1, 2, 3] for specific tables\n",
    "\n",
    "result = orchestrator.execute_step(2, compare_ids=compare_ids)\n",
    "\n",
    "print(\"\\nStep 2 Results:\")\n",
    "print(f\"  Tables processed: {result['total_tables']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View row meta comparison using SQL view\n",
    "query = \"SELECT * FROM v_row_meta_comparison WHERE run_name = :run_name\"\n",
    "df_row_meta = pd.read_sql(query, db.engine, params={'run_name': run.run_name})\n",
    "\n",
    "print(f\"\\nRow Meta Comparison ({len(df_row_meta)} partitions):\")\n",
    "df_row_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Column Meta Check (CrossWalk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Step 3\n",
    "compare_ids = None  # None = all tables\n",
    "\n",
    "result = orchestrator.execute_step(3, compare_ids=compare_ids)\n",
    "\n",
    "print(\"\\nStep 3 Results:\")\n",
    "print(f\"  Tables processed: {result['total_tables']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View crosswalk summary using SQL view\n",
    "query = \"SELECT * FROM v_crosswalk_summary WHERE run_name = :run_name\"\n",
    "df_crosswalk = pd.read_sql(query, db.engine, params={'run_name': run.run_name})\n",
    "\n",
    "print(f\"\\nCrossWalk Summary:\")\n",
    "df_crosswalk.pivot_table(\n",
    "    index=['pcds_tbl', 'aws_tbl'],\n",
    "    columns='mapping_status',\n",
    "    values='column_count',\n",
    "    fill_value=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Column Statistics Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4A: Generate SAS Code for PCDS (Manual Execution Required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate SAS code for PCDS column statistics\n",
    "# This will create .sas files that you need to run in SAS\n",
    "\n",
    "print(\"SAS code generation not yet implemented\")\n",
    "print(\"After generating SAS code:\")\n",
    "print(\"  1. Run .sas files in SAS\")\n",
    "print(\"  2. Upload resulting CSV files using Step 4C below\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4B: Execute AWS Column Statistics (Automated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Step 4 for AWS (PCDS requires manual SAS execution)\n",
    "compare_ids = None  # None = all tables\n",
    "vintages = None  # None = all vintages from row_meta\n",
    "columns = None  # None = all comparable columns\n",
    "\n",
    "result = orchestrator.execute_step(4, \n",
    "                                   compare_ids=compare_ids,\n",
    "                                   vintages=vintages,\n",
    "                                   columns=columns)\n",
    "\n",
    "print(\"\\nStep 4 Results:\")\n",
    "print(f\"  Tables processed: {result['total_tables']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4C: Upload PCDS Statistics from SAS CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After running SAS, upload the CSV results\n",
    "from services.step4_col_stat import ColStatAnalyzer\n",
    "\n",
    "analyzer = ColStatAnalyzer(session, run_id)\n",
    "\n",
    "# TODO: Update these paths to your actual CSV files from SAS\n",
    "pcds_csv_uploads = [\n",
    "    {'compare_id': 1, 'vintage': '2024', 'csv_path': 'output/pcds_stats_compare1_vintage2024.csv'},\n",
    "    # Add more as needed\n",
    "]\n",
    "\n",
    "for upload in pcds_csv_uploads:\n",
    "    result = analyzer.upload_pcds_stats_csv(**upload)\n",
    "    print(f\"Uploaded {result['rows_inserted']} stats for compare_id={upload['compare_id']}, vintage={upload['vintage']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4D: View Column Statistics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View column statistics mismatches using SQL view\n",
    "query = \"SELECT * FROM v_col_stat_mismatch WHERE run_name = :run_name\"\n",
    "df_stat_mismatch = pd.read_sql(query, db.engine, params={'run_name': run.run_name})\n",
    "\n",
    "print(f\"\\nColumn Statistics Mismatches ({len(df_stat_mismatch)} found):\")\n",
    "df_stat_mismatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Row Hash Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Step 5\n",
    "compare_ids = None  # None = all tables\n",
    "vintages = None  # None = all vintages\n",
    "sample_size = 10000  # Limit rows to check (None = all rows)\n",
    "\n",
    "result = orchestrator.execute_step(5,\n",
    "                                   compare_ids=compare_ids,\n",
    "                                   vintages=vintages,\n",
    "                                   sample_size=sample_size)\n",
    "\n",
    "print(\"\\nStep 5 Results:\")\n",
    "print(f\"  Tables processed: {result['total_tables']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View hash mismatches\n",
    "from db.models import RowHash\n",
    "\n",
    "mismatches = session.query(RowHash).filter_by(\n",
    "    run_id=run_id,\n",
    "    has_match=False\n",
    ").limit(100).all()\n",
    "\n",
    "df_mismatches = pd.DataFrame([\n",
    "    {\n",
    "        'compare_id': m.compare_id,\n",
    "        'vintage': m.vintage,\n",
    "        'platform': m.platform,\n",
    "        'unique_ids': m.unique_id_json,\n",
    "        'row_hash': m.row_hash\n",
    "    }\n",
    "    for m in mismatches\n",
    "])\n",
    "\n",
    "print(f\"\\nRow Hash Mismatches (first 100):\")\n",
    "df_mismatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all results to Excel\n",
    "output_path = f\"output/validation_results_{run.run_name}.xlsx\"\n",
    "db.export_to_excel(output_path, run_id=run_id)\n",
    "\n",
    "print(f\"Results exported to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backup database\n",
    "backup_path = db.backup_db()\n",
    "print(f\"Database backed up to: {backup_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get final status\n",
    "status = orchestrator.get_status()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VALIDATION RUN SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Run Name: {status['run_name']}\")\n",
    "print(f\"Run ID: {status['run_id']}\")\n",
    "print(f\"Status: {status['status']}\")\n",
    "print(f\"Current Step: {status['current_step']}\")\n",
    "print(f\"Last Updated: {status['updated_at']}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Table counts\n",
    "stats = db.get_table_stats()\n",
    "print(\"\\nDatabase Stats:\")\n",
    "for table, count in stats.items():\n",
    "    print(f\"  {table}: {count} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close session\n",
    "session.close()\n",
    "print(\"Session closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jobs-pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
